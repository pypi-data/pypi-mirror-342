{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructLab Skills Synthetic Data Generation\n",
    "\n",
    "![InstructLab Banner](../../../assets/imgs/instructlab-banner.png)\n",
    "\n",
    "This notebook demonstrates how to customize language models by generating training data for specific skills, following the methodology outlined in the LAB (Large-scale Alignment for Chatbots) framework [[paper link](https://arxiv.org/pdf/2403.01081)].\n",
    "\n",
    "### Customizing Model Behavior\n",
    "\n",
    "The LAB framework enables us to shape how a model responds to various tasks by training it on carefully crafted examples. Want your model to write emails in your company's tone? Need it to follow specific formatting guidelines? This customization is achieved through what the paper defines as compositional skills.\n",
    "\n",
    "Compositional skills are tasks that combine different abilities to handle complex queries. For example, if you want your model to write company emails about quarterly performance, it needs to:\n",
    "- Understand financial concepts\n",
    "- Perform basic arithmetic\n",
    "- Write in your preferred communication style\n",
    "- Follow your organization's email format\n",
    "\n",
    "### Demo Overview\n",
    "\n",
    "This notebook will show you how to:\n",
    "1. Set up a teacher model for generating training data\n",
    "2. Create examples that reflect your preferred style and approach\n",
    "3. Generate Synthetic Data\n",
    "4. Validate that the generated data matches your requirements\n",
    "\n",
    "The end goal is to create training data that will help align the model with your specific needs, whether that's matching your company's communication style, following particular protocols, or handling specialized tasks in your preferred way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install sdg-hub\n",
    "\n",
    "```bash \n",
    "pip install sdg-hub==0.1.0a2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ Step 1: Set Up the Teacher Model\n",
    "\n",
    "This demo uses **Mixtral-8x7B-Instruct-v0.1** as the teacher model. We'll serve it using **vLLM**, and use **Llama Stack** to expose an OpenAI-compatible API.\n",
    "\n",
    "\n",
    "### Serve the Model with vLLM\n",
    "\n",
    "Launch the vLLM server with:\n",
    "\n",
    "```bash\n",
    "vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 2\n",
    "```\n",
    "\n",
    "This will start the model server at: `http://localhost:8000`\n",
    "\n",
    "> ‚ö†Ô∏è Make sure your system has sufficient GPU memory.  \n",
    "> üîß Adjust `--tensor-parallel-size` based on available GPUs.  \n",
    "> ‚è±Ô∏è First-time model loading may take several minutes.\n",
    "\n",
    "\n",
    "### Set Up Llama Stack (OpenAI-Compatible Interface)\n",
    "\n",
    "1. Clone and install Llama Stack (OpenAI-compatible branch)\n",
    "```bash\n",
    "git clone https://github.com/bbrowning/llama-stack.git\n",
    "cd llama-stack\n",
    "git checkout openai_server_compat\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "2. Install the Python client\n",
    "```bash\n",
    "pip install llama-stack-client\n",
    "```\n",
    "\n",
    "3. Launch the Llama Stack Server (connected to vLLM)\n",
    "```bash\n",
    "export INFERENCE_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "llama stack build --template remote-vllm\n",
    "```\n",
    "\n",
    "The server will start at: `http://localhost:8321`\n",
    "\n",
    "You can use the CLI to verify the setup:\n",
    "\n",
    "```bash\n",
    "llama-stack-client   --endpoint http://localhost:8321   inference chat-completion   --model-id $INFERENCE_MODEL   --message \"write a haiku about language models\"\n",
    "```\n",
    "\n",
    "\n",
    "Let‚Äôs setup a client and test the connection with python and move on! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful! mistralai/Mixtral-8x7B-Instruct-v0.1:  Hello! It's nice to meet you.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8321/v1/openai/v1\"\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "teacher_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Test the connection with a simple completion\n",
    "response = client.chat.completions.create(\n",
    "    model=teacher_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    temperature=0.0,\n",
    "    max_tokens=10\n",
    ")\n",
    "completion = response.choices[0].message.content\n",
    "\n",
    "print(f\"Connection successful! {teacher_model}: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Step 2: Provide Custom Examples\n",
    "\n",
    "\n",
    "#### Usecase: Teaching a Language Model the Skill: Unstructured Text ‚Üí Markdown Table\n",
    "\n",
    "Company X receives large volumes of user feedback through support emails, in-app surveys, and app store reviews. These messages often contain valuable product insights, but the content is unstructured and difficult to analyze at scale.\n",
    "\n",
    "To streamline internal workflows, an AI team at Company X wants to teach a language model how to convert raw user feedback into structured markdown tables. These tables summarize key topics, user sentiment, and issues in a format that‚Äôs easy to scan, report, or push into dashboards and tracking systems.\n",
    "\n",
    "We can do this using InstructLab!\n",
    "\n",
    "#### üßæ Example Input and Output\n",
    "\n",
    "üì• Input (Unstructured Feedback)\n",
    "```\n",
    "Hey team ‚Äî I‚Äôve been using the new update for about a week now.\n",
    "\n",
    "Couple of things:\n",
    "- The dark mode is awesome, great job!\n",
    "- But the loading time after login feels slower than before. Not a deal breaker but noticeable.\n",
    "- I also noticed that the calendar widget doesn‚Äôt update properly if I change time zones.\n",
    "\n",
    "Overall, I love where this is going. Just needs a few tweaks.\n",
    "```\n",
    "üì§ Output (Markdown Table)\n",
    "\n",
    "| Feature           | Feedback                                                               | Sentiment |\n",
    "|------------------|------------------------------------------------------------------------|-----------|\n",
    "| Dark Mode        | Works well, user is satisfied.                                          | Positive  |\n",
    "| Login Performance| Loading time after login is slower than previous version.               | Negative  |\n",
    "| Calendar Widget  | Doesn't update correctly when time zones change.                        | Negative  |\n",
    "| Overall          | User is happy with the direction of the product, but suggests tweaks.   | Positive  |\n",
    "\n",
    "#### Instructlab Grounded Skills Generation Pipeline \n",
    "\n",
    "Now that we have laid out our usecase, lets dive into the skills generation pipeline proposed by LAB \n",
    "You can refer to the flow details and block config from this yaml (src/instructlab/sdg/flows/generation/skills/simple_grounded_skill.yaml)\n",
    "\n",
    "InstructLab uses a multi-step process of generation and evaluation to generate synthetic data. For grounded skills it looks like this: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "    <img src=\"../../../assets/imgs/IL_skills_pipeline.png\" alt=\"Skills Pipeline\" width=\"250\">\n",
    "  </td>\n",
    "  <td>\n",
    "    <ul>\n",
    "      <li>\n",
    "        <strong>Context Generation (<code>gen_contexts</code>)</strong><br>\n",
    "        Generates diverse, relevant contexts for the skill<br>\n",
    "        Produces 10 unique contexts per run<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Question Generation & Validation</strong><br>\n",
    "        <code>gen_grounded_questions</code>: Creates 3 questions per context<br>\n",
    "        <code>eval_grounded_questions</code>: Evaluates question quality<br>\n",
    "        <code>filter_grounded_questions</code>: Keeps only perfect scores (1.0)<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Response Generation & Quality Control</strong><br>\n",
    "        <code>gen_grounded_responses</code>: Generates appropriate responses<br>\n",
    "        <code>evaluate_grounded_qa_pair</code>: Scores Q&A pair quality<br>\n",
    "        <code>filter_grounded_qa_pair</code>: Retains high-quality pairs (score ‚â• 2.0)<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Final Processing</strong><br>\n",
    "        <code>combine_question_and_context</code>: Merges context with questions for complete examples<br><br>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Data with Examples\n",
    "Now that we've seen how LAB generates skill-specific data, let's walk through how to use it for our own use case.\n",
    "\n",
    "As outlined in the LAB paper, the first step is to provide a small number of **seed examples** (typically 5) to bootstrap the skill. These examples are passed into the generation pipeline as input and are stored in a `.jsonl` file.\n",
    "\n",
    "For this demo, we‚Äôll use the pre-populated seed file located at: [mdtable_seeds.jsonl](examples/instructlab/skills/sample_data/mdtable_seeds.jsonl)\n",
    "\n",
    "Lets open the file and explore a row: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.conda/envs/lls/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task_description': 'Convert the following unstructured user feedback into a structured markdown table.',\n",
       " 'seed_context': \"Been using the new dashboard for a few days. It's way faster than the previous one, really appreciate the snappy filters. But export to CSV seems broken ‚Äî nothing happens when I click it. Also, dark mode resets every time I log in.\",\n",
       " 'seed_question': 'I would like to convert the above feedback into a markdown table with columns for Feature, Feedback and Sentiment.',\n",
       " 'seed_response': \"| Feature           | Feedback                                                           | Sentiment |\\n|------------------|--------------------------------------------------------------------|-----------|\\n| Dashboard        | Much faster than previous version, filters are responsive.         | Positive  |\\n| Export to CSV    | Clicking the export button doesn't trigger a download.             | Negative  |\\n| Dark Mode        | Resets to light mode on login.                                     | Negative  |\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the seed dataset\n",
    "seed_data = load_dataset(\"json\", data_files=\"sample_data/mdtable_seeds.jsonl\", split=\"train\")\n",
    "\n",
    "# Display the first example\n",
    "seed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Generate Synthetic Data\n",
    "\n",
    "Now that we have our seed data ready, we can use LAB‚Äôs Skill Data Generator to create **high-quality synthetic training examples** for our custom skill.\n",
    "\n",
    "This step leverages a predefined **flow configuration** that encodes how seed examples are expanded ‚Äî by generating new contexts, questions, and responses, and filtering them for quality.\n",
    "\n",
    "In this demo, we'll use the `synth_grounded_skills.yaml` flow, which follows LAB's grounded generation pattern (context ‚Üí question ‚Üí response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.pipeline import Pipeline\n",
    "from sdg_hub.sdg import SDG\n",
    "\n",
    "# Path to the skill generation flow configuration\n",
    "flow_path = \"../../../src/sdg_hub/flows/generation/skills/synth_grounded_skills.yaml\"\n",
    "\n",
    "# Load the flow\n",
    "flow = Flow(client).get_flow_from_file(flow_path)\n",
    "\n",
    "# Initialize the synthetic data generator\n",
    "generator = SDG(\n",
    "    [Pipeline(flow)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the generator is ready to run the full pipeline ‚Äî including context generation, question/response generation, evaluation, and filtering ‚Äî to produce a synthetic dataset that can be used for fine-tuning or skill bootstrapping.\n",
    "\n",
    "In the next step, we‚Äôll run this pipeline and inspect the generated outputs. (This should take about a minute or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:39:46] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8321/v1/openai/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200</span> <a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:39:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8321/v1/openai/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200\u001b[0m \u001b]8;id=775542;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=679720;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> LLM server supports batched inputs: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>                                         <a href=\"file:///workspace/home/lab/shiv/sdg_hub/src/sdg_hub/blocks/llmblock.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">llmblock.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///workspace/home/lab/shiv/sdg_hub/src/sdg_hub/blocks/llmblock.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LLM server supports batched inputs: \u001b[3;92mTrue\u001b[0m                                         \u001b]8;id=342937;file:///workspace/home/lab/shiv/sdg_hub/src/sdg_hub/blocks/llmblock.py\u001b\\\u001b[2mllmblock.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=378854;file:///workspace/home/lab/shiv/sdg_hub/src/sdg_hub/blocks/llmblock.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:39:51] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8321/v1/openai/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200</span> <a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:39:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8321/v1/openai/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200\u001b[0m \u001b]8;id=35813;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=993744;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:04] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8321/v1/openai/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200</span> <a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8321/v1/openai/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200\u001b[0m \u001b]8;id=895128;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=883624;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:45] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8321/v1/openai/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200</span> <a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8321/v1/openai/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200\u001b[0m \u001b]8;id=936136;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=908156;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211/211 [00:00<00:00, 15834.36 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211/211 [00:00<00:00, 56887.46 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211/211 [00:00<00:00, 41071.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:53] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8321/v1/openai/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200</span> <a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8321/v1/openai/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200\u001b[0m \u001b]8;id=154848;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=574633;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:58] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8321/v1/openai/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200</span> <a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8321/v1/openai/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200\u001b[0m \u001b]8;id=700548;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=602196;file:///home/lab/.conda/envs/lls/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 13887.81 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 56008.69 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 36493.36 examples/s]\n",
      "Map (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 969.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "generated_data = generator.generate(seed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Explore and Validate the Synthetically Generated Data\n",
    "\n",
    "Once the skill generation pipeline has been executed, the output is a set of **synthetically generated examples** ‚Äî new context-question-response triples that follow the same structure as the seed data but are expanded and refined by the teacher model.\n",
    "\n",
    "Below is an example of one generated entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_description': 'Convert the following unstructured user feedback into a structured markdown table.',\n",
       " 'seed_context': 'The analytics view is very informative. Would love to see breakdown by team as well. Charts sometimes take a few seconds to load though. Mobile layout is clean.',\n",
       " 'seed_question': 'Please convert the above feedback into a markdown table with columns for Feature, Feedback and Sentiment.',\n",
       " 'seed_response': '| Feature           | Feedback                                                             | Sentiment |\\n|------------------|----------------------------------------------------------------------|-----------|\\n| Analytics View    | Provides useful insights.                                           | Positive  |\\n| Team Breakdown    | Requested feature not currently available.                         | Neutral   |\\n| Charts            | Load slowly on occasion.                                            | Negative  |\\n| Mobile Layout     | Clean and well-designed.                                            | Positive  |',\n",
       " 'context': 'The user provided feedback on the e-commerce website design. They mentioned that the product search function is efficient and easy to use, but suggested improving the search results by adding images. The user also likes the layout of the shopping cart and checkout process. However, they pointed out that the site could benefit from a more prominent display of shipping costs. The user appreciates the guest checkout option but feels that creating an account should offer some benefits, such as discounts or quicker checkout in the future.',\n",
       " 'question': \"The user provided feedback on the e-commerce website design. They mentioned that the product search function is efficient and easy to use, but suggested improving the search results by adding images. The user also likes the layout of the shopping cart and checkout process. However, they pointed out that the site could benefit from a more prominent display of shipping costs. The user appreciates the guest checkout option but feels that creating an account should offer some benefits, such as discounts or quicker checkout in the future.\\n\\nRegarding the e-commerce website design, how would you express the user's thoughts on the guest checkout option and the benefits of creating an account in a markdown table with columns for 'Feature', 'Feedback', and 'Sentiment'?\",\n",
       " 'response': '| Feature                  | Feedback                                                                                   | Sentiment |\\n|-------------------------|--------------------------------------------------------------------------------------------|-----------|\\n| Guest Checkout Option   | Appreciated for its convenience, but could be improved with more incentives.              | Neutral   |\\n| Creating an Account      | User suggested offering benefits like discounts or quicker checkout in the future.        | Positive  |\\n| Benefits of Creating an Account | Not explicitly stated, but user implied they should offer value for account creation. | Positive  |',\n",
       " 'evaluation': \"The answer is a good example of how an AI Assistant should respond to the user's instruction. The answer is presented in a well-structured markdown table, directly addressing the user's feedback on the guest checkout option and the benefits of creating an account. The table includes all the required columns, and the feedback is accurately represented. However, the answer could have been more insightful by elaborating on the user's sentiment towards the guest checkout option.\",\n",
       " 'score': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "rand_idx = random.choice(range(len(generated_data)))\n",
    "generated_data[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user provided feedback on the e-commerce website design. They mentioned that the product search function is efficient and easy to use, but suggested improving the search results by adding images. The user also likes the layout of the shopping cart and checkout process. However, they pointed out that the site could benefit from a more prominent display of shipping costs. The user appreciates the guest checkout option but feels that creating an account should offer some benefits, such as discounts or quicker checkout in the future.\n",
      "\n",
      "Regarding the e-commerce website design, how would you express the user's thoughts on the guest checkout option and the benefits of creating an account in a markdown table with columns for 'Feature', 'Feedback', and 'Sentiment'?\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[rand_idx]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Feature                  | Feedback                                                                                   | Sentiment |\n",
      "|-------------------------|--------------------------------------------------------------------------------------------|-----------|\n",
      "| Guest Checkout Option   | Appreciated for its convenience, but could be improved with more incentives.              | Neutral   |\n",
      "| Creating an Account      | User suggested offering benefits like discounts or quicker checkout in the future.        | Positive  |\n",
      "| Benefits of Creating an Account | Not explicitly stated, but user implied they should offer value for account creation. | Positive  |\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[rand_idx]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to teach a custom skill to a language model using the InstructLab Skill Data Generator (SDG). Starting from a small set of seed examples, we walked through the full synthetic data generation pipeline ‚Äî including context creation, question generation, response synthesis, evaluation, and filtering.\n",
    "\n",
    "We explored a real-world use case: **transforming unstructured user feedback into structured markdown tables**, and showed how the LAB framework can automate the generation of high-quality, instructional training data at scale.\n",
    "\n",
    "This approach is especially powerful for procedural or domain-specific tasks where labeled data is scarce but consistent task logic can be modeled. With just a few carefully curated seed examples, you can unlock scalable skill creation and push new capabilities into LLMs with minimal manual effort.\n",
    "\n",
    "You‚Äôre now ready to use these synthetic examples for Fine-tuning small models! \n",
    "\n",
    "Next steps? Try adapting this pipeline to your own task, domain, or format ‚Äî whether it‚Äôs triaging support tickets, extracting structured data, or following domain-specific workflows. The skills are yours to create."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
