# src/ai_ensemble_suite/aggregation/multidimensional_voting.py

"""Multi-dimensional Voting aggregation strategy."""

import math
from typing import Dict, Any, Optional, List, Set, Tuple, TYPE_CHECKING
import time
import re

from ai_ensemble_suite.aggregation.base import BaseAggregator
from ai_ensemble_suite.exceptions import AggregationError, ConfigurationError, ModelError, ValidationError
from ai_ensemble_suite.utils.logging import logger
from ai_ensemble_suite.utils.tracing import TraceCollector
import copy

# Type hint imports
if TYPE_CHECKING:
    from ai_ensemble_suite.config import ConfigManager
    from ai_ensemble_suite.models import ModelManager



class MultidimensionalVoting(BaseAggregator):
    """Multi-dimensional Voting aggregation strategy.

    Evaluates phase outputs along multiple configured dimensions (e.g., accuracy,
    clarity) using scores potentially generated by an evaluator model or extracted
    from context, then selects the output with the highest weighted score.
    """

    # Override __init__ to accept model_manager
    def __init__(
        self,
        config_manager: "ConfigManager",
        strategy_name: str,
        model_manager: Optional["ModelManager"] = None, # Required for evaluation
        strategy_config_override: Optional[Dict[str, Any]] = None
    ) -> None:
        """Initialize the MultidimensionalVoting aggregator."""
        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)
        # No specific initialization needed here currently


    async def aggregate(
        self,
        outputs: Dict[str, Dict[str, Any]],
        context: Dict[str, Any], # Context received here
        trace_collector: Optional[TraceCollector] = None
    ) -> Dict[str, Any]:
        """Aggregate outputs by evaluating multiple dimensions and voting.

        Args:
            outputs: Dictionary mapping phase names to their outputs.
            context: Context information from collaboration phases (includes 'query').
            trace_collector: Optional trace collector for gathering execution details.

        Returns:
            Dictionary containing:
                response: The aggregated output (from the best phase) as a string.
                confidence: The weighted score of the best phase as confidence.
                best_phase: The name of the phase selected as best.
                dimension_scores: Scores for each phase across dimensions (0.0-1.0).
                weighted_scores: Final weighted score for each phase (0.0-1.0).

        Raises:
            AggregationError: If aggregation fails (e.g., no outputs, evaluation fails).
        """
        start_time = time.time()
        logger.debug(f"Starting Multidimensional Voting aggregation for strategy '{self._strategy_name}'")

        if not outputs:
             raise AggregationError("No phase outputs provided for Multidimensional Voting aggregation.")

        try:
            # Get dimensions from strategy configuration
            dimensions = self._config.get("dimensions")
            # Default dimensions if not specified or invalid
            if not isinstance(dimensions, list) or not dimensions:
                default_dims = ["accuracy", "clarity", "completeness", "reasoning"]
                logger.warning(f"Invalid or empty 'dimensions' in config, using defaults: {default_dims}")
                dimensions = default_dims

            # Get dimension weights from strategy configuration
            dimension_weights = self._config.get("dimension_weights", {})
            if not isinstance(dimension_weights, dict):
                 logger.warning("Invalid 'dimension_weights' in config (must be dict), using equal weights.")
                 dimension_weights = {}

            # Ensure all specified dimensions have a weight, defaulting to equal if needed
            normalized_weights: Dict[str, float] = {}
            dimensions_with_weight: List[str] = []
            total_weight_specified = 0.0
            for dim in dimensions:
                weight = dimension_weights.get(dim, -1.0) # Use -1 to detect unspecified
                if isinstance(weight, (int, float)) and weight >= 0:
                     normalized_weights[dim] = float(weight)
                     dimensions_with_weight.append(dim)
                     total_weight_specified += float(weight)
                # Else: Skip dimensions with invalid or negative weights defined

            # If no valid weights specified or sum is zero, use equal weights for *all* original dimensions
            if not dimensions_with_weight or total_weight_specified <= 0:
                logger.debug("Using default equal weights for all dimensions.")
                equal_weight = 1.0 / len(dimensions) if dimensions else 0
                normalized_weights = {dim: equal_weight for dim in dimensions}
                dimensions_with_weight = dimensions
                total_weight_after_default = sum(normalized_weights.values()) # Should be close to 1.0
                logger.debug(f"Default equal weights applied: {normalized_weights} (Sum: {total_weight_after_default})")
            # If weights were specified and sum > 0, normalize them if they don't sum to 1
            elif not math.isclose(total_weight_specified, 1.0):
                 logger.debug(f"Normalizing specified dimension weights from sum {total_weight_specified}")
                 normalized_weights = {dim: w / total_weight_specified for dim, w in normalized_weights.items()}
                 total_weight_after_normalize = sum(normalized_weights.values())
                 logger.debug(f"Normalized weights: {normalized_weights} (Sum: {total_weight_after_normalize})")


            # --- Get scores for each phase and dimension ---
            dimension_scores: Dict[str, Dict[str, float]] = {} # { phase_name: { dimension: score } }

            # 1. Look for pre-evaluated scores within phase outputs
            for phase_name, phase_output_data in outputs.items():
                 if not isinstance(phase_output_data, dict):
                      logger.debug(f"Skipping phase '{phase_name}' output as it's not a dictionary.")
                      continue # Skip non-dict outputs

                 phase_scores: Dict[str, float] = {}

                 # Check common locations for scores (evaluations dict, specific keys, text parsing)
                 # Prioritize 'evaluations' dict if present
                 evals_dict = phase_output_data.get("evaluations")
                 if isinstance(evals_dict, dict):
                      for dim in dimensions_with_weight: # Only look for relevant dimensions
                           if dim in evals_dict and isinstance(evals_dict[dim], (int, float)):
                                phase_scores[dim] = max(0.0, min(1.0, float(evals_dict[dim]))) # Normalize/clamp 0-1

                 # Look for direct keys like 'accuracy_score' if not found above
                 for dim in dimensions_with_weight:
                      if dim not in phase_scores: # Only if not already found in 'evaluations'
                           dim_key_score = f"{dim}_score"
                           dim_key_rating = f"{dim}_rating" # Another common pattern
                           score_val = None
                           if dim_key_score in phase_output_data and isinstance(phase_output_data[dim_key_score], (int, float)):
                                score_val = phase_output_data[dim_key_score]
                           elif dim_key_rating in phase_output_data and isinstance(phase_output_data[dim_key_rating], (int, float)):
                                score_val = phase_output_data[dim_key_rating]

                           if score_val is not None:
                                # Assume scores/ratings are 0-1 or 0-10, 1-10 etc. Normalize best guess.
                                if score_val > 1.0: score_val = score_val / 10.0 # Assume out of 10
                                phase_scores[dim] = max(0.0, min(1.0, float(score_val)))


                 # Attempt regex extraction from output text if scores still missing for some dims
                 output_text_content = self._extract_output(phase_output_data)
                 if output_text_content:
                      for dim in dimensions_with_weight:
                           if dim not in phase_scores: # Only parse if score not found yet
                                # Regex looks for "dimension: score/10" patterns, more robustly
                                score_pattern = rf'{re.escape(dim)}\s*[:\-]?\s*score\s*[:\-]?\s*(\d+(?:\.\d+)?)\s*/\s*10'
                                match = re.search(score_pattern, output_text_content, re.IGNORECASE)
                                if not match: # Try alternate pattern like "dimension: 8" (assume out of 10)
                                     score_pattern_alt = rf'{re.escape(dim)}\s*[:\-]?\s*(\d+(?:\.\d+)?)(?!\s*/)' # Number not followed by /
                                     match = re.search(score_pattern_alt, output_text_content, re.IGNORECASE)

                                if match:
                                    try:
                                         score_val = float(match.group(1))
                                         # Normalize if it seems to be out of 10
                                         if score_val > 1.0 or '/10' in match.group(0).lower():
                                              score_val = score_val / 10.0
                                         phase_scores[dim] = max(0.0, min(1.0, score_val)) # Normalize 0-1
                                    except ValueError:
                                         logger.warning(f"Could not parse score for dim '{dim}' in phase '{phase_name}' output via regex.")

                 # Store scores found for this phase if any relevant ones were found
                 if any(dim in phase_scores for dim in dimensions_with_weight):
                      dimension_scores[phase_name] = phase_scores

            logger.debug(f"Initial dimension scores extracted: {dimension_scores}")


            # 2. If scores incomplete or missing, use evaluator model if configured
            evaluator_model_id = self._config.get("evaluator_model")
            evaluator_model_id = context.get("evaluator_model", evaluator_model_id) # Allow context override

            # Determine if evaluation is needed
            phases_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())
            phases_with_incomplete_scores = {
                 name for name, scores in dimension_scores.items()
                 if not all(dim in scores for dim in dimensions_with_weight)
            }
            needs_evaluation = bool(phases_needing_scores or phases_with_incomplete_scores)

            if needs_evaluation and evaluator_model_id:
                 if self._model_manager is None:
                      logger.warning("Evaluator model specified, but ModelManager not available to aggregator. Cannot perform model-based evaluation.")
                 else:
                      logger.info(f"Performing dimension evaluation using model: {evaluator_model_id}")
                      try:
                           # *** FIX: Pass context to _evaluate_phases ***
                           # Pass only the dimensions that have weights assigned
                           evaluated_scores = await self._evaluate_phases(
                               outputs, dimensions_with_weight, evaluator_model_id, context, trace_collector
                           )
                           # Merge evaluated scores, filling gaps. Evaluated scores take precedence? Let's say yes.
                           for phase_name, scores in evaluated_scores.items():
                               if phase_name not in dimension_scores:
                                    dimension_scores[phase_name] = {}
                               # Update existing dict, overwriting with newly evaluated scores
                               dimension_scores[phase_name].update(scores)
                               logger.debug(f"Updated/Added evaluated scores for phase '{phase_name}': {scores}")

                      except ModelError as e:
                            logger.error(f"Evaluation model '{evaluator_model_id}' failed: {e}. Continuing without its evaluation.")
                            # Continue without evaluated scores if evaluation fails
                      except Exception as e:
                           logger.error(f"Unexpected error during phase evaluation using model '{evaluator_model_id}': {str(e)}", exc_info=True)
                           # Continue without evaluated scores


            # 3. Fallback: Use confidence scores if dimension scores are STILL missing/incomplete
            phases_still_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())
            phases_still_incomplete = {
                 name for name, scores in dimension_scores.items()
                 if not all(dim in scores for dim in dimensions_with_weight)
            }

            if phases_still_needing_scores or phases_still_incomplete:
                 logger.debug("Dimension scores still incomplete after potential evaluation, using phase confidence scores as fallback.")
                 for phase_name, phase_output_data in outputs.items():
                      # Check if scores are needed for this phase
                      needs_score_fill = phase_name in phases_still_needing_scores or \
                                         (phase_name in phases_still_incomplete and isinstance(dimension_scores.get(phase_name), dict))

                      if needs_score_fill:
                           confidence = 0.0
                           if isinstance(phase_output_data, dict) and "confidence" in phase_output_data:
                                conf_val = phase_output_data["confidence"]
                                if isinstance(conf_val, (int, float)):
                                     confidence = max(0.0, min(1.0, float(conf_val)))
                                elif isinstance(conf_val, dict):
                                     combined_conf = conf_val.get("combined")
                                     if isinstance(combined_conf, (int, float)):
                                         confidence = max(0.0, min(1.0, float(combined_conf)))

                           if confidence > 0:
                                if phase_name not in dimension_scores:
                                     dimension_scores[phase_name] = {}
                                # Fill in missing scores for dimensions_with_weight using confidence
                                for dim in dimensions_with_weight:
                                     if dim not in dimension_scores[phase_name]:
                                          dimension_scores[phase_name][dim] = confidence
                                logger.debug(f"Using confidence {confidence:.3f} to fill missing dimension scores for phase '{phase_name}'.")


            # 4. Final Fallback: Assign default scores (e.g., 0.5) if still no scores for some phases
            phases_finally_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())
            phases_finally_incomplete = {
                 name for name, scores in dimension_scores.items()
                 if not all(dim in scores for dim in dimensions_with_weight)
            }

            if phases_finally_needing_scores or phases_finally_incomplete:
                 default_fallback_score = 0.5 # Use a neutral default
                 logger.warning(f"No scores or confidence available for some phases/dimensions, assigning default score ({default_fallback_score}).")
                 for phase_name in outputs:
                      if phase_name not in dimension_scores:
                          dimension_scores[phase_name] = {dim: default_fallback_score for dim in dimensions_with_weight}
                      elif phase_name in phases_finally_incomplete: # Already has a dict, fill missing
                           for dim in dimensions_with_weight:
                                dimension_scores[phase_name].setdefault(dim, default_fallback_score)


            # --- Calculate weighted scores for each phase ---
            weighted_scores: Dict[str, float] = {}
            if not dimension_scores: # Check if somehow still empty
                 raise AggregationError("Could not determine dimension scores for any phase.")

            for phase_name, scores in dimension_scores.items():
                 if phase_name not in outputs: continue # Skip if phase wasn't in original outputs

                 current_weighted_score = 0.0
                 current_total_weight = 0.0 # Track weight used *for this phase*

                 # Calculate weighted sum using the normalized weights
                 for dim, score in scores.items():
                     if dim in normalized_weights: # Use only dimensions with defined, normalized weights
                          weight = normalized_weights[dim]
                          if weight > 0: # Skip dimensions with zero or negative weight
                               current_weighted_score += score * weight
                               current_total_weight += weight
                          # Else: Ignore dimensions without weight or with zero weight

                 # Normalize score by total weight actually used for this phase's scoring
                 # This handles cases where some dimensions might be missing scores or weights
                 if current_total_weight > 0:
                       weighted_scores[phase_name] = current_weighted_score / current_total_weight
                 elif scores: # If weights were zero/missing but scores existed, use simple average of available scores
                       logger.warning(f"Total applicable weight for phase '{phase_name}' was zero, using average score.")
                       valid_scores = [s for s in scores.values() if isinstance(s, (int, float))]
                       weighted_scores[phase_name] = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
                 else: # No scores and no applicable weights
                      logger.warning(f"No scores or applicable weights for phase '{phase_name}', assigning default weighted score 0.5.")
                      weighted_scores[phase_name] = 0.5

            logger.debug(f"Calculated final weighted scores: {weighted_scores}")


            # --- Select the best phase ---
            if not weighted_scores:
                raise AggregationError("No phases remaining after scoring for aggregation.")

            # Select the phase with the highest weighted score
            # Use max(), handling potential ties by taking the first one found (dict order dependent in <3.7)
            # For more robust tie-breaking, could use confidence or other factors.
            best_phase_name = max(weighted_scores, key=weighted_scores.get)
            best_score = weighted_scores[best_phase_name] # This is the final normalized weighted score

            # Extract the output content from the best phase
            best_output_content = self._extract_output(outputs[best_phase_name])


            execution_time = time.time() - start_time
            logger.info(
                f"Multidimensional Voting aggregation completed in {execution_time:.2f}s. "
                f"Best phase: '{best_phase_name}' (Weighted Score: {best_score:.3f})",
                extra={ "dimensions_used": list(normalized_weights.keys()) }
            )

            # Prepare final result dictionary
            aggregation_result = {
                "response": best_output_content,
                "confidence": best_score, # Use the final weighted score as confidence
                "best_phase": best_phase_name,
                 # Include detailed scores for dimensions that had weights
                "dimension_scores": dimension_scores,
                "weighted_scores": weighted_scores # Include final weighted scores per phase
            }

            # Add trace if collector is provided
            if trace_collector:
                trace_collector.add_aggregation_trace(
                    strategy_name=self._strategy_name,
                    inputs={
                        "phase_output_keys": list(outputs.keys()),
                        "context_keys": list(context.keys()),
                        "configured_dimensions": dimensions, # Original list from config
                        "configured_weights": dimension_weights, # Original weights from config
                    },
                    output=aggregation_result,
                    execution_time=execution_time,
                    parameters={ # Parameters influencing the outcome
                        "effective_dimensions": dimensions_with_weight, # Dimensions actually used
                        "normalized_weights": normalized_weights, # Weights used in calculation
                        "evaluator_model": evaluator_model_id
                    }
                )

            return aggregation_result

        except AggregationError as e:
             logger.error(f"Multidimensional voting aggregation failed: {str(e)}")
             raise # Re-raise known aggregation errors
        except Exception as e:
            logger.error(f"Unexpected error during Multidimensional Voting aggregation: {str(e)}", exc_info=True)
            # Wrap unexpected errors in AggregationError
            raise AggregationError(f"Multidimensional Voting aggregation failed unexpectedly: {str(e)}")

    async def _evaluate_phases(
            self,
            outputs: Dict[str, Dict[str, Any]],
            dimensions: List[str],
            evaluator_model_id: str,
            context: Dict[str, Any],  # Added missing context parameter
            trace_collector: Optional[TraceCollector] = None
    ) -> Dict[str, Dict[str, float]]:
        """Evaluate phase outputs using a specified model along multiple dimensions.

        Args:
            outputs: Dictionary mapping phase names to their output data.
            dimensions: List of dimension names (strings) to evaluate.
            evaluator_model_id: ID of the model to use for evaluation.
            context: Context dictionary (needed for the query).
            trace_collector: Optional trace collector.

        Returns:
            Dictionary mapping phase names to their evaluated dimension scores (0.0-1.0).
            Returns empty dict if evaluation fails significantly.

        Raises:
            ModelError: If the evaluation model fails.
            AggregationError: If required components like ModelManager are missing.
        """
        if self._model_manager is None:
             raise AggregationError("ModelManager is required for evaluation but not available.")

        logger.debug(f"Evaluating {len(outputs)} phases using model '{evaluator_model_id}' on dimensions: {dimensions}")

        # Format outputs for the evaluation prompt
        formatted_outputs_for_prompt = ""
        valid_outputs_to_eval = {} # Keep track of which outputs we could format
        for phase_name, phase_output_data in outputs.items():
            output_text = self._extract_output(phase_output_data)
            if output_text: # Only include phases with extractable text
                formatted_outputs_for_prompt += f"\n\n## Output from Phase: {phase_name}\n\n{output_text}\n"
                valid_outputs_to_eval[phase_name] = output_text
            else:
                 logger.warning(f"Could not extract text from output of phase '{phase_name}' for evaluation.")

        if not formatted_outputs_for_prompt:
             logger.warning("No valid outputs found to format for evaluation prompt.")
             return {}

        # Create dimensions string for the prompt
        dimensions_str = ", ".join(dimensions)

        # Get evaluation prompt template from config (or use a robust default)
        evaluation_template_name = self._config.get("evaluation_template", "multidimensional_evaluation")

        # Default template asking for scores 0-10
        default_eval_template = f"""Please evaluate the following outputs based on the user query and the specified dimensions.

USER QUERY:
{{query}}

DIMENSIONS TO EVALUATE: {dimensions_str}

For each output below, provide a score from 0 to 10 for each dimension, where 0 is worst and 10 is best.

OUTPUTS TO EVALUATE:
{formatted_outputs_for_prompt}

Provide your evaluation clearly for each phase, like this example:

## Evaluation for Phase: [phase_name_1]
- {dimensions[0]}: [score]/10
- {dimensions[1]}: [score]/10
...

## Evaluation for Phase: [phase_name_2]
- {dimensions[0]}: [score]/10
...
(End your response after evaluating all phases)
"""
        # Try formatting using ConfigManager, fallback to default if template missing/fails
        evaluation_prompt = ""
        try:
             # *** FIX: Use context parameter to get query ***
             query_context = context.get("query", "N/A") # Default if query missing in context
             context_dict = {
                 "query": query_context,
                 "outputs_section": formatted_outputs_for_prompt, # Keep the variable name simple
                 "dimensions_list": dimensions_str
            }
             evaluation_prompt = self._config_manager.render_template(evaluation_template_name, context_dict)

        except ConfigurationError:
             logger.warning(f"Evaluation template '{evaluation_template_name}' not found, using default.")
             # *** FIX: Use context parameter to get query for default template ***
             query_context = context.get("query", "N/A")
             # Format the default template string directly
             evaluation_prompt = default_eval_template.format(query=query_context)
        except (KeyError, ValidationError) as e: # Catch missing keys or other format errors
             logger.warning(f"Error formatting template '{evaluation_template_name}': {e}. Using default.")
             # *** FIX: Use context parameter to get query for default template ***
             query_context = context.get("query", "N/A")
             evaluation_prompt = default_eval_template.format(query=query_context)
        except Exception as e:
             logger.error(f"Unexpected error formatting evaluation prompt: {e}", exc_info=True)
             # *** FIX: Use context parameter to get query for default template ***
             query_context = context.get("query", "N/A")
             evaluation_prompt = default_eval_template.format(query=query_context)


        # Run evaluator model
        try:
            logger.debug(f"Sending evaluation prompt to model '{evaluator_model_id}'.")
            # Use model manager's run_inference
            evaluation_result_raw = await self._model_manager.run_inference(
                model_id=evaluator_model_id,
                prompt=evaluation_prompt,
                # Add params suitable for evaluation (e.g., lower temp)
                temperature=self._config.get("evaluation_temperature", 0.2), # More deterministic eval
                max_tokens=self._config.get("evaluation_max_tokens", 1024) # Allow enough space
            )
            logger.debug("Received evaluation result from model.")

            # Add trace for the evaluation model call
            if trace_collector:
                trace_collector.add_model_trace(
                    model_id=evaluator_model_id,
                    input_prompt=evaluation_prompt, # Can be large
                    output=evaluation_result_raw, # Store raw eval output
                    execution_time=evaluation_result_raw.get("total_time", 0),
                    parameters={"role": "dimension_evaluator", "evaluated_dimensions": dimensions}
                )

            evaluation_text = evaluation_result_raw.get("text", "")
            if not evaluation_text:
                 logger.warning(f"Evaluation model '{evaluator_model_id}' returned empty text.")
                 return {}


            # --- Parse scores from the evaluation text ---
            parsed_dimension_scores: Dict[str, Dict[str, float]] = {}
            # Iterate through the phases we expected to evaluate
            for phase_name in valid_outputs_to_eval.keys():
                phase_scores: Dict[str, float] = {}
                # Look for the section corresponding to this phase's evaluation
                # Make regex more robust: allows for variations in header, non-greedy match
                phase_pattern = rf"^\s*(?:#+)?\s*(?:Evaluation for Phase|Output from Phase|Phase):\s*{re.escape(phase_name)}\s*$(.*?)(?:^\s*(?:#+)?\s*(?:Evaluation for Phase|Output from Phase|Phase):|\Z)"
                phase_match = re.search(phase_pattern, evaluation_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)

                if phase_match:
                    phase_section_text = phase_match.group(1)
                    logger.debug(f"Found evaluation section for phase '{phase_name}'.")

                    # Extract scores for each dimension within this section
                    for dim in dimensions:
                         # Regex: dimension name, optional colon/hyphen, space, score / 10
                         score_pattern = rf'^\s*[\*\-]?\s*{re.escape(dim)}\s*[:\-]?\s*(\d+(?:\.\d+)?)\s*/\s*10'
                         score_match = re.search(score_pattern, phase_section_text, re.IGNORECASE | re.MULTILINE)
                         if not score_match: # Try alt pattern without /10
                             score_pattern_alt = rf'^\s*[\*\-]?\s*{re.escape(dim)}\s*[:\-]?\s*(\d+(?:\.\d+)?)(?![\d\.]|\s*/)'
                             score_match = re.search(score_pattern_alt, phase_section_text, re.IGNORECASE | re.MULTILINE)

                         if score_match:
                             try:
                                  score_val = float(score_match.group(1))
                                   # Normalize if likely out of 10
                                  if score_val > 1.0 or '/10' in score_match.group(0).lower():
                                       score_val /= 10.0
                                  phase_scores[dim] = max(0.0, min(1.0, score_val)) # Clamp score [0, 1]
                                  logger.debug(f"Parsed score for '{phase_name}' -> '{dim}': {phase_scores[dim]:.2f}")
                             except ValueError:
                                 logger.warning(f"Could not parse score '{score_match.group(1)}' for dim '{dim}' in phase '{phase_name}'.")
                         # else:
                         #     logger.debug(f"Score pattern not found for dim '{dim}' in phase '{phase_name}' section.")

                # else:
                #     logger.debug(f"Evaluation section pattern not found for phase '{phase_name}'.")


                # Add scores found for this phase to the result if any were parsed
                if phase_scores:
                    parsed_dimension_scores[phase_name] = phase_scores

            if not parsed_dimension_scores:
                 logger.warning("Failed to parse any scores from the evaluation model's output.")

            return parsed_dimension_scores

        except ModelError as e:
             # Specific error from the model run
             logger.error(f"Evaluation model '{evaluator_model_id}' failed during inference: {str(e)}")
             raise # Re-raise ModelError to be caught by the main aggregate method
        except Exception as e:
            # Unexpected errors during the evaluation process
            logger.error(f"Unexpected error during phase evaluation process: {str(e)}", exc_info=True)
            # Raise as AggregationError to indicate evaluation step failed
            raise AggregationError(f"Unexpected error during evaluation: {str(e)}")


    # Inherit _extract_output from BaseAggregator - No changes needed here
    # def _extract_output(self, phase_output_data: Any) -> str: ...
