<!-- metrics_details_table.html - Detailed metrics for uncertainty evaluation -->
<div class="metrics-details-table-container">
    <div class="table-info">
        <p>This table provides comprehensive metrics for evaluating different aspects of uncertainty quantification quality.</p>
    </div>

    <div class="table-wrapper">
        <table class="data-table metrics-details-table">
            <thead>
                <tr>
                    <th class="metric-col">Metric</th>
                    <th class="value-col">Value</th>
                    <th class="range-col">Acceptable Range</th>
                    <th class="status-col">Status</th>
                    <th class="description-col">Description</th>
                </tr>
            </thead>
            <tbody>
                <!-- Coverage Metrics -->
                <tr class="metric-category">
                    <td colspan="5" class="category-header">Coverage Metrics</td>
                </tr>
                <tr>
                    <td>Average Coverage</td>
                    <td id="avg-coverage-value">-</td>
                    <td>Depends on target</td>
                    <td id="avg-coverage-status">-</td>
                    <td>Average empirical coverage across all alpha levels</td>
                </tr>
                <tr>
                    <td>Average Coverage Gap</td>
                    <td id="avg-coverage-gap-value">-</td>
                    <td>|Gap| < 0.05</td>
                    <td id="avg-coverage-gap-status">-</td>
                    <td>Average difference between expected and empirical coverage</td>
                </tr>
                <tr>
                    <td>Coverage Consistency</td>
                    <td id="coverage-consistency-value">-</td>
                    <td>≥ 0.8</td>
                    <td id="coverage-consistency-status">-</td>
                    <td>Consistency of coverage performance across alpha levels</td>
                </tr>
                
                <!-- Calibration Metrics -->
                <tr class="metric-category">
                    <td colspan="5" class="category-header">Calibration Metrics</td>
                </tr>
                <tr>
                    <td>Expected Calibration Error</td>
                    <td id="ece-value-detail">-</td>
                    <td>< 0.05</td>
                    <td id="ece-status">-</td>
                    <td>Weighted average of calibration errors across all bins</td>
                </tr>
                <tr>
                    <td>Maximum Calibration Error</td>
                    <td id="mce-value-detail">-</td>
                    <td>< 0.15</td>
                    <td id="mce-status">-</td>
                    <td>Maximum calibration error observed in any bin</td>
                </tr>
                <tr>
                    <td>Brier Score</td>
                    <td id="brier-score-value-detail">-</td>
                    <td>< 0.1</td>
                    <td id="brier-score-status">-</td>
                    <td>Mean squared error between predicted probabilities and outcomes</td>
                </tr>
                
                <!-- Sharpness Metrics -->
                <tr class="metric-category">
                    <td colspan="5" class="category-header">Sharpness Metrics</td>
                </tr>
                <tr>
                    <td>Average Interval Width</td>
                    <td id="avg-width-value">-</td>
                    <td>Domain dependent</td>
                    <td id="avg-width-status">-</td>
                    <td>Average width of prediction intervals (lower is sharper)</td>
                </tr>
                <tr>
                    <td>Width Variation</td>
                    <td id="width-variation-value">-</td>
                    <td>< 0.5</td>
                    <td id="width-variation-status">-</td>
                    <td>Coefficient of variation in interval widths</td>
                </tr>
                <tr>
                    <td>Normalized Sharpness</td>
                    <td id="norm-sharpness-value">-</td>
                    <td>≥ 0.7</td>
                    <td id="norm-sharpness-status">-</td>
                    <td>Sharpness score normalized against baseline</td>
                </tr>
                
                <!-- Composite Scores -->
                <tr class="metric-category">
                    <td colspan="5" class="category-header">Composite Scores</td>
                </tr>
                <tr>
                    <td>Uncertainty Score</td>
                    <td id="uncertainty-score-value-detail">-</td>
                    <td>≥ 0.8</td>
                    <td id="uncertainty-score-status">-</td>
                    <td>Overall score for uncertainty quantification quality</td>
                </tr>
                <tr>
                    <td>Reliability-Sharpness Balance</td>
                    <td id="reliability-sharpness-value">-</td>
                    <td>≥ 0.7</td>
                    <td id="reliability-sharpness-status">-</td>
                    <td>Balance between reliable coverage and sharp intervals</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="metrics-explanation">
        <h4>Understanding the Metrics</h4>
        <p>
            Good uncertainty quantification requires balancing multiple objectives:
        </p>
        <ul>
            <li><strong>Reliability</strong>: Prediction intervals should contain the true value at the expected rate (measured by coverage metrics)</li>
            <li><strong>Calibration</strong>: Predicted probabilities should match observed frequencies (measured by calibration metrics)</li>
            <li><strong>Sharpness</strong>: Intervals should be as narrow as possible while maintaining reliability (measured by sharpness metrics)</li>
        </ul>
        <p>
            The composite scores combine these aspects to provide an overall assessment of uncertainty quality.
        </p>
    </div>
</div>