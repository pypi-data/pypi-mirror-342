<!-- uncertainty_score.html - Uncertainty score display component -->
<div class="uncertainty-score-indicator">
    <div class="score-circle" data-score="{{ uncertainty_score }}">
        <div class="score-inner">
            <div class="score-value">{{ (uncertainty_score * 100)|int }}%</div>
            <div class="score-label">Uncertainty Score</div>
        </div>
    </div>
    
    <div class="score-description">
        <p class="score-text">
            {% if uncertainty_score >= 0.9 %}
                Excellent uncertainty quantification with reliable prediction intervals.
            {% elif uncertainty_score >= 0.8 %}
                Good uncertainty quantification with mostly reliable intervals.
            {% elif uncertainty_score >= 0.7 %}
                Moderate uncertainty quantification with acceptable reliability.
            {% elif uncertainty_score >= 0.6 %}
                Fair uncertainty quantification with some reliability issues.
            {% else %}
                Limited uncertainty quantification with significant reliability concerns.
            {% endif %}
        </p>
    </div>
    
    <div class="score-interpretation">
        <div class="interpretation-scale">
            <div class="scale-segment poor" data-range="0-0.6">Poor</div>
            <div class="scale-segment fair" data-range="0.6-0.7">Fair</div>
            <div class="scale-segment moderate" data-range="0.7-0.8">Moderate</div>
            <div class="scale-segment good" data-range="0.8-0.9">Good</div>
            <div class="scale-segment excellent" data-range="0.9-1.0">Excellent</div>
            <div class="scale-indicator" style="left: {{ uncertainty_score * 100 }}%;"></div>
        </div>
        <div class="scale-description">
            <span class="scale-info">Higher scores indicate better uncertainty quantification</span>
        </div>
    </div>
</div>