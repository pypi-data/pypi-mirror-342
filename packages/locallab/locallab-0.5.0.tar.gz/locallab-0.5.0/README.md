# ğŸš€ LocalLab: Your Personal AI Lab

Run powerful AI language models on your own computer or Google Colab - no cloud services needed! Think of it as having ChatGPT-like capabilities right on your machine.

## ğŸ¤” What is LocalLab?

LocalLab brings AI to your fingertips with two key components:

```mermaid
graph TD
    A[Your Code] -->|Uses| B[LocalLab Client]
    B -->|Talks to| C[LocalLab Server]
    C -->|Runs| D[AI Models]
    C -->|Manages| E[Memory & Resources]
    C -->|Optimizes| F[Performance]
```

### ğŸ¯ Key Features

```
ğŸ“¦ Easy Setup         ğŸ”’ Privacy First       ğŸ® Free GPU Access
ğŸ¤– Multiple Models    ğŸ’¾ Memory Efficient    ğŸ”„ Auto-Optimization
ğŸŒ Local or Colab    âš¡ Fast Response       ğŸ”§ Simple API
```

### ğŸŒŸ Two Ways to Run

1. **On Your Computer (Local Mode)**
   ```
   ğŸ’» Your Computer
   â””â”€â”€ ğŸš€ LocalLab Server
       â””â”€â”€ ğŸ¤– AI Model
           â””â”€â”€ ğŸ”§ Auto-optimization
   ```

2. **On Google Colab (Free GPU Mode)**
   ```
   â˜ï¸ Google Colab
   â””â”€â”€ ğŸ® Free GPU
       â””â”€â”€ ğŸš€ LocalLab Server
           â””â”€â”€ ğŸ¤– AI Model
               â””â”€â”€ âš¡ GPU Acceleration
   ```

## ğŸ“¦ Installation & Setup

### 1. Install Required Packages

```bash
# Install both server and client packages
pip install locallab locallab-client
```

### 2. Configure the Server (Recommended)

```bash
# Run interactive configuration
locallab config

# This will help you set up:
# - Model selection
# - Memory optimizations
# - GPU settings
# - System resources
```

### 3. Start the Server

```bash
# Start with saved configuration
locallab start

# Or start with specific options
locallab start --model microsoft/phi-2 --quantize --quantize-type int8
```


## ğŸ’¡ Basic Usage

### Synchronous Usage (Easier for Beginners)

```python
from locallab_client import SyncLocalLabClient

# Connect to server
client = SyncLocalLabClient("http://localhost:8000")

try:
    # Generate text
    response = client.generate("Write a story")
    print(response)

    # Chat with AI
    response = client.chat([
        {"role": "system", "content": "You are helpful."},
        {"role": "user", "content": "Hello!"}
    ])
    print(response.choices[0]["message"]["content"])

finally:
    # Always close the client
    client.close()
```

### Asynchronous Usage (For Advanced Users)

```python
import asyncio
from locallab_client import LocalLabClient

async def main():
    # Connect to server
    client = LocalLabClient("http://localhost:8000")
    
    try:
        # Generate text
        response = await client.generate("Write a story")
        print(response)

        # Stream responses
        async for token in client.stream_generate("Tell me a story"):
            print(token, end="", flush=True)

        # Chat with AI
        response = await client.chat([
            {"role": "system", "content": "You are helpful."},
            {"role": "user", "content": "Hello!"}
        ])
        print(response.choices[0]["message"]["content"])

    finally:
        # Always close the client
        await client.close()

# Run the async function
asyncio.run(main())
```

## ğŸŒ Google Colab Usage

Run LocalLab on Google's free GPUs:

```python
# 1. Install packages
!pip install locallab locallab-client

# 2. Configure with CLI (notice the ! prefix)
!locallab config

# 3. Start server with CLI
!locallab start --use-ngrok

# 4. Connect client (Locally)
from locallab_client import LocalLabClient
client = LocalLabClient("https://your-server-ngrok-url.app")
response = await client.generate("Hello!")
```

## ğŸ’» Requirements

### Local Computer
- Python 3.8+
- 4GB RAM minimum (8GB+ recommended)
- GPU optional but recommended
- Internet connection for downloading models

### Google Colab
- Just a Google account!
- Free tier works fine

## ğŸŒŸ Features

- **Easy Setup**: Just pip install and run
- **Multiple Models**: Use any Hugging Face model
- **Resource Efficient**: Automatic optimization
- **Privacy First**: All local, no data sent to cloud
- **Free GPU**: Google Colab integration
- **Flexible Client API**: Both async and sync clients available
- **Automatic Resource Management**: Sessions close automatically

[â¡ï¸ See All Features](./docs/features/README.md)

## ğŸ“š Documentation

### Getting Started
1. [Installation Guide](./docs/guides/getting-started.md)
2. [Basic Examples](./docs/guides/examples.md)
3. [CLI Usage](./docs/guides/cli.md)

### Advanced Topics
1. [API Reference](./docs/guides/api.md)
2. [Client Libraries](./docs/clients/README.md)
3. [Advanced Features](./docs/guides/advanced.md)
4. [Performance Guide](./docs/features/performance.md)

### Deployment
1. [Local Setup](./docs/deployment/local.md)
2. [Google Colab Guide](./docs/colab/README.md)

## ğŸ” Need Help?

- Check [FAQ](./docs/guides/faq.md)
- Visit [Troubleshooting](./docs/guides/troubleshooting.md)
- Ask in [Discussions](https://github.com/UtkarshTheDev/LocalLab/discussions)

## ğŸ“– Additional Resources

- [Contributing Guide](./docs/guides/contributing.md)
- [Changelog](./CHANGELOG.md)
- [License](./LICENSE)

## ğŸŒŸ Star Us!
If you find LocalLab helpful, please star our repository! It helps others discover the project.

---

Made with â¤ï¸ by Utkarsh Tiwari
[GitHub](https://github.com/UtkarshTheDev) â€¢ [Twitter](https://twitter.com/UtkarshTheDev) â€¢ [LinkedIn](https://linkedin.com/in/utkarshthedev)
