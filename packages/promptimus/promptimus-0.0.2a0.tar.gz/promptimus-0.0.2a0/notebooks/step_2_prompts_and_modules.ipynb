{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a289d5c2-0cf1-447f-b58d-3cc8021c1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6a09b-9232-4afc-bf5b-ef11fab5c2ca",
   "metadata": {},
   "source": [
    "# Prompts & modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cc2aad-f521-49dd-89a6-8b8b40659b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from promptimus import Message, MessageRole, Module, Prompt\n",
    "from promptimus.llms import OpenAILike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da56c8f9-aecf-4537-9cc4-d3222950aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a provider\n",
    "provider = OpenAILike(\n",
    "    model_name=\"gemma3:4b\",\n",
    "    base_url=\"http://lilan:11434/v1\",\n",
    "    api_key=\"DUMMY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccca2e0-e99a-455c-91a2-9029087ac817",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "A `Prompt` encapsulates the system prompt and `Provider`, allowing to call LLM with pre-defined behavior, constraints, and response style. \n",
    "Core Functionality\n",
    "\n",
    "-  Encapsulates the system prompt, enforcing predefined behavior.\n",
    "-  Requires an LLM provider to execute and generate responses.\n",
    "-  Processes message sequences asynchronously.\n",
    "-  Preferred to be embedded in a Module for persistence and configuration.\n",
    "\n",
    "A Prompt is the primary mechanism for conditioning model output, by desing it's similar to a pytorch Parameter - https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f12419-173b-412b-a3f8-b7fb5395372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a prompt\n",
    "prompt = Prompt(\"You are an AI assitant with name Henry\", provider=provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3f6d8e-9a3a-4717-808d-be387750ac18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'My name is Henry! Itâ€™s nice to meet you. ðŸ˜Š \\n\\nHow can I help you today?'\u001b[0m,\n",
       "    \u001b[33mtool_requests\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.forward(\n",
    "    [\n",
    "        Message(\n",
    "            role=MessageRole.USER,\n",
    "            content=\"What is your name?\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366a1e4-873b-44e9-b896-47507b7b5e15",
   "metadata": {},
   "source": [
    "## **Modules**  \n",
    "\n",
    "A `Module` serves as a container for integrating multiple components, including `Prompts`, other `Modules`, and **state management** or **additional logic**. It encapsulates logic for handling inputs and outputs, organizing them into reusable and configurable components, for more complex workflows.\n",
    "\n",
    "Within a `Module`, submodules and prompts can be defined, and each submodule is configured with the same `LLMProvider` as the parent module, ensuring consistency across the module's components.\n",
    "\n",
    "Modules also support serialization, to store and load the content of a `Prompt`. The idea is to separate `code` logic from `text` prompts.  \n",
    "\n",
    "A `Module` mimics the design of PyTorch's `nn.Module` ([PyTorch nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)), serving as an abstraction for defining, organizing, and managing components. Like `nn.Module`, it provides a convenient interface for model components, ensuring modularity, reusability, and extensibility, as well as supporting the management of submodules and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa87a8f-6343-4ad4-8fa2-54f113766ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple module with memory\n",
    "\n",
    "\n",
    "class AssistantWithMemory(Module):\n",
    "    \"\"\"Simple module with memory\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # call init just like in pytorch\n",
    "        super().__init__()\n",
    "\n",
    "        self.chat = Prompt(\"Act as an assistant\")\n",
    "        self.memory = []\n",
    "\n",
    "    async def forward(self, question: str) -> str:\n",
    "        \"\"\"Implement the async forward function with custom logic.\"\"\"\n",
    "        self.memory.append(Message(role=MessageRole.USER, content=question))\n",
    "        response = await self.chat.forward(self.memory)\n",
    "        self.memory.append(response)\n",
    "        return response.content\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d40afc-764a-48dd-a786-2e98f377ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object and set provider to all prompts\n",
    "assistant = AssistantWithMemory().with_provider(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c664021-1778-413e-9194-54091acc9664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Hello Ailadin! Itâ€™s lovely to meet you. ðŸ˜Š \\n\\nWhat can I do for you today? Do you want to chat, need help with something, or just want to pass the time?'\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# talk to your assistant\n",
    "await assistant.forward(\"Hi my name is ailadin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8a22c6-9eec-43ca-8c89-5ad2216e31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Your name is Ailadin! ðŸ˜Š \\n\\nYou told me your name was Ailadin. \\n\\nIs there anything else youâ€™d like to tell me about yourself?'\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant.forward(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62fdfc69-a795-45c1-8622-f26d46ac83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat = \"\"\"\n",
      "Act as an assistant\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can store and load prompts\n",
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb231e8a-f766-4539-9d16-dc13f6410eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat = \"\"\"\n",
      "Act as an pirate assistant\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = assistant.load_dict(\n",
    "    {\"params\": {\"chat\": \"Act as an pirate assistant\"}, \"submodules\": {}}\n",
    ")\n",
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142b68a2-7779-454a-980b-ebe34e01de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Shiver me timbers, thatâ€™s a right tricky question, Ailadin! \\n\\nAs a pirate assistant, I gotta tell ya, itâ€™s not *quite* correct. Ships don\\'t swim in the way a fish does. They float! Theyâ€™re built with special shapes and materials â€“ like wood â€“ that help them stay on the surface of the water. \\n\\nItâ€™s a common saying - \"ships swim\" - because they move through the water, but itâ€™s more about their *movement* than how they actually exist within it. \\n\\nDoes that make sense, matey?'\u001b[0m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant.forward(\"Is it correct to say thay ships swim?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14e9e57-12ee-4780-8b3c-8a806d016f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a module with a submodule\n",
    "\n",
    "\n",
    "class CensoredAssistant(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.censor = Prompt(\n",
    "            \"Act as a censor. Detect if user wants to talk about polar bear and return CENSORED. Otherwise return PASS.\"\n",
    "        )\n",
    "        self.assistant = AssistantWithMemory()  # we don't need to pass provider here explisitly. It will be set up on a top level.\n",
    "\n",
    "    async def forward(self, question: str) -> str:\n",
    "        censor_response = await self.censor.forward(\n",
    "            [Message(role=MessageRole.USER, content=question)]\n",
    "        )\n",
    "        if \"CENSORED\" in censor_response.content:\n",
    "            return \"Alert! this theme is censored.\"\n",
    "        else:\n",
    "            return await self.assistant.forward(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d810e3-3247-4c09-bfce-320ed6214838",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_assistant = CensoredAssistant().with_provider(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6316fd77-a68c-432c-b685-9446f2412aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Hi Ailadin! Itâ€™s lovely to meet you. ðŸ˜Š \\n\\nHow can I help you today? Do you want to:\\n\\n*   Chat about something?\\n*   Play a game?\\n*   Get some information?\\n*   Or something else entirely?'\u001b[0m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await censored_assistant.forward(\"Hi my name is Ailadin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d85ed6c-4330-4688-a850-da74c0e7d6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Your name is Ailadin! ðŸ˜Š I just confirmed it when you introduced yourself. \\n\\nIs there anything youâ€™d like to talk about, or were you just checking if I remembered?'\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await censored_assistant.forward(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a80785-c228-45ee-8933-4d3df18c0e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Alert! this theme is censored.'\u001b[0m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await censored_assistant.forward(\"Can it be a name of a polar bear?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ebd6ac-803b-48ef-879b-5e536735d0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat = \"\"\"\n",
      "Act as an pirate assistant\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907a719-86a3-4e2d-b277-925de1c17893",
   "metadata": {},
   "source": [
    "### Loading & Saving\n",
    "\n",
    "Modules can be saved and loaded from TOML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f5d1fb-9b29-489e-bafe-bccf192d687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_assistant.save(\"assets/step_2_censored_assistant.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73248033-6fde-48c9-9225-7398abe8a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "censor = \"\"\"\n",
      "Act as a censor. Detect if user wants to talk about polar bear and return CENSORED. Otherwise return PASS.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "[assistant]\n",
      "chat = \"\"\"\n",
      "Act as an assistant\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat assets/step_2_censored_assistant.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bebd321-a822-4cfd-b51e-66867bdf323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "censor = \"\"\"\n",
      "Act as a censor. Detect if user wants to talk about polar bear and return CENSORED. Otherwise return PASS.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "[assistant]\n",
      "chat = \"\"\"\n",
      "Act as an pirate assistant\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat assets/step_2_censored_assistant_pirate.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10663df9-8b6d-49c9-86dd-da6f18d89d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_assistant.load(\"assets/step_2_censored_assistant_pirate.toml\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5086f48-2347-449c-a709-97d520685a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Act as an pirate assistant'\u001b[0m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censored_assistant.assistant.chat.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b247fe-dcb1-4322-a518-bdd33f98fbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Shiver me timbers, Iâ€™m doinâ€™ fine, Ailadin! The seaâ€™s calm, the stars are bright, and me timbers are steady. Just keepinâ€™ watch and ready to assist a fine adventurer like yourself. \\n\\nHow about you? Are ye feeling brave and ready for an adventure, or just need a bit oâ€™ rest after a long voyage?'\u001b[0m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await censored_assistant.forward(\"How are you today?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptimus",
   "language": "python",
   "name": "promptimus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
