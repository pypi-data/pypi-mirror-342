Metadata-Version: 2.4
Name: llamashell
Version: 0.0.5
Summary: A powerful shell that's powered by a locally running LLM
Home-page: https://github.com/hathibelagal-dev/llamashell
Author: Ashraff Hathibelagal
Project-URL: Source, https://github.com/hathibelagal-dev/llamashell
Project-URL: Tracker, https://github.com/hathibelagal-dev/llamashell/issues
Keywords: ai shell llm llama qwen localllama terminal linux macos
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: transformers>=4.49.0
Requires-Dist: torch
Requires-Dist: huggingface_hub
Requires-Dist: requests
Requires-Dist: prompt_toolkit
Requires-Dist: dnspython
Dynamic: author
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license-file
Dynamic: project-url
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# llamashell
 **llamashell** is a powerful shell that's powered by a locally running LLM. We have tested it with Llama 3.2, Qwen 2.5, and Gemma 3.

## Features

- **Interactive Shell**: Execute standard shell commands like `cd`, `ls`, and more, with support for pipes (`|`), input redirection (`<`), and output redirection (`>` or `>>`).
- **LLM Integration**: Interact with an LLM (default: `meta-llama/Llama-3.2-1B-Instruct`) for assistance using the `--` prefix (e.g., `-- write me an inspirational quote`).
- **Command History**: Persistent command history stored in `~/.llamashell_history`.
- **Chat Log Management**: Save and view LLM conversation logs with commands like `--save-chat-logs` and `--view-chat-logs`.
- **File Operations**: Read files into the LLM context with `--read <filename>` and save individual LLM responses with `--save <filename>`.
- **Auto-Completion**: Basic command and file auto-completion for a smoother user experience.
- **Cross-Platform**: Supports GPU acceleration (CUDA/MPS) and CPU fallback for broad compatibility.

## Installation

```bash
pip3 install llamashell
```

### Prerequisites
- Python 3.11+
- Linux or MacOS

## Usage

```bash
llamashell
```

You can provide any instruct LLM you can find on hugging face. For example:

```bash
llamashell --model "Qwen/Qwen2.5-0.5B-Instruct"
```

or

```bash
llamashell --model "google/gemma-3-1b-it"
```

### Special Commands

- `-- <message>`: Send a message to the LLM.
- `--save-chat-logs`: Save the entire LLM conversation to a file.
- `--save [filename]`: Save the last LLM response to a file.
- `--view-chat-logs`: Display the LLM conversation history.
- `--read <filename>`: Read a file and add its contents to the LLM context.
- `--clear`: Reset the LLM chat session.
- `history`: Show the shell command history.
- `exit`, `quit`, `bye`: Exit the shell.
