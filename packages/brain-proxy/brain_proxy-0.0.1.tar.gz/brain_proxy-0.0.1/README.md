# ğŸ§  brain-proxy

**Turn any FastAPI backend into a fully featured OpenAI-compatible LLM proxy â€” with memory, RAG, streaming, and file uploads.**

> Like the OpenAI `/chat/completions` endpoint â€” but with context, memory, and smart file ingestion.

---

## âœ¨ Features

- âœ… OpenAI-compatible `/chat/completions` (drop-in SDK support)
- âœ… Multi-tenant routing (`/v1/<tenant>/chat/completions`)
- âœ… File ingestion via `file_data` messages
- âœ… RAG with Chroma + LangChain
- âœ… LangMem-powered long & short-term memory
- âœ… Streaming via Server-Sent Events
- âœ… Custom text extractor support for PDFs, CSVs, etc.
- âœ… No frontend changes required

---

## ğŸš€ Installation

```bash
pip install brain-proxy
```

---

## âš¡ Quickstart

```python
from fastapi import FastAPI
from brain_proxy import BrainProxy

proxy = BrainProxy(
    openai_api_key="sk-...",  # used for both LLM + embeddings
)

app = FastAPI()
app.include_router(proxy.router, prefix="/v1")
```

Now any OpenAI SDK can point to:

```
http://localhost:8000/v1/<tenant>/chat/completions
```

---

## ğŸ§  Multi-tenancy explained

Every tenant (`/v1/acme`, `/v1/alpha`, etc):

- Gets its own vector store (for RAG)
- Has isolated LangMem memory (short- and long-term)
- Can upload files (auto-indexed + persisted)

This means you can serve multiple brands or users safely and scalably from a single backend.

---

## ğŸ’¬ OpenAI SDK Example

```python
import openai

openai.api_key = "sk-fake"
openai.base_url = "http://localhost:8000/v1/acme"

response = openai.ChatCompletion.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's 3 + 2?"}]
)

print(response["choices"][0]["message"]["content"])
```

### Streaming:

```python
stream = openai.ChatCompletion.create(
    model="gpt-4o",
    stream=True,
    messages=[{"role": "user", "content": "Tell me a short story about an AI fox."}]
)

for chunk in stream:
    print(chunk.choices[0].delta.get("content", ""), end="")
```

---

## ğŸ“ File Uploads

Send `file_data` parts inside messages to upload PDFs, CSVs, images, etc:

```json
{
  "role": "user",
  "content": [
    { "type": "text", "text": "Here's a report:" },
    { "type": "file_data", "file_data": {
        "name": "report.pdf",
        "mime": "application/pdf",
        "data": "...base64..."
    }}
  ]
}
```

Files are saved, parsed, embedded, and used in RAG on the fly.

---

## ğŸ§¾ Custom PDF extractor example

```python
from pdfminer.high_level import extract_text

def parse_pdf(path: Path, mime: str) -> str:
    if mime == "application/pdf":
        return extract_text(path)
    return "(unsupported format)"
```

```python
proxy = BrainProxy(
    openai_api_key="sk-...",
    extract_text=parse_pdf
)
```

---

## ğŸ“¦ Roadmap

- [x] Multi-agent manager hook
- [x] Usage hooks + token metering
- [ ] Use LiteLLM instead to support more models
- [ ] MCP support
- [ ] LangGraph integration

---

## âš–ï¸ License

MIT â€” free to use, fork, and build on.  
Made for backend devs who want to move fast âš¡

---

## â¤ï¸ Contributing

Issues and PRs welcome!

Letâ€™s build smarter backends â€” together.
