[
  {
    "objectID": "quaternions.html",
    "href": "quaternions.html",
    "title": "Tensor Quaternion Module",
    "section": "",
    "text": "source\n\n\n\n TensorQuaternionAngle (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorQuaternionInclination (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/orientation'\n\n\nhdf_files = get_hdf_files(f_path)\ntfm_src = CreateDict([DfHDFCreateWindows(win_sz=1000,stp_sz=100,clm='acc_x')])\nu = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z']\n# u = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z','mag_x','mag_y','mag_z']\ny =['opt_a','opt_b','opt_c','opt_d']\ndls = DataBlock(blocks=(SequenceBlock.from_hdf(u,TensorSequencesInput),\n                        SequenceBlock.from_hdf(y,TensorQuaternionInclination)),\n                get_items=tfm_src,\n                splitter=RandomSplitter(0.1)\n               ).dataloaders(hdf_files,shufflish=True,bs=128)",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#quaternion-type",
    "href": "quaternions.html#quaternion-type",
    "title": "Tensor Quaternion Module",
    "section": "",
    "text": "source\n\n\n\n TensorQuaternionAngle (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorQuaternionInclination (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/orientation'\n\n\nhdf_files = get_hdf_files(f_path)\ntfm_src = CreateDict([DfHDFCreateWindows(win_sz=1000,stp_sz=100,clm='acc_x')])\nu = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z']\n# u = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z','mag_x','mag_y','mag_z']\ny =['opt_a','opt_b','opt_c','opt_d']\ndls = DataBlock(blocks=(SequenceBlock.from_hdf(u,TensorSequencesInput),\n                        SequenceBlock.from_hdf(y,TensorQuaternionInclination)),\n                get_items=tfm_src,\n                splitter=RandomSplitter(0.1)\n               ).dataloaders(hdf_files,shufflish=True,bs=128)",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#basic-operations",
    "href": "quaternions.html#basic-operations",
    "title": "Tensor Quaternion Module",
    "section": "Basic Operations",
    "text": "Basic Operations\n\ntq1 = tensor([\n    [1,0,0,0],\n    [0.5,0.5,0.5,0.5],\n    ])\ntq2 = tensor([\n    [0.5,0.5,0.5,0.5],\n    [0.5,0.5,0.5,0.5],\n    ])\ntq1.shape\n\ntorch.Size([2, 4])\n\n\n\nsource\n\nrad2deg\n\n rad2deg (t)\n\n\ntest_eq(float(rad2deg(_pi)),180)\n\n\n\n\nScriptFunction object at 0x10f9e11c0&gt;\nquat1quat2*\n\n# q = tq1.repeat(1000,1)\n\n\n# %%timeit\n# torch.cumprod(q,dim=-1)\n\n\n# %%timeit\n# [q*q for _ in range(1000)]\n\n\ntest_eq(multiplyQuat(tq1,tq2),tensor([[ 0.5000,  0.5000,  0.5000,  0.5000],\n                                    [-0.5000,  0.5000,  0.5000,  0.5000]]))\n\n\nsource\n\n\nnorm_quaternion\n\n norm_quaternion (q)\n\n\ntest_eq(norm_quaternion(tq1*5),tq1)\ntest_eq(norm_quaternion(tq1/_pi),tq1)\ntest_eq(norm_quaternion(tq1[None,...]),tq1[None,...])\n\n\nsource\n\n\nconjQuat\n\n conjQuat (q)\n\n\ntest_eq(conjQuat(tq1),tensor([[ 1.0000, -0.0000, -0.0000, -0.0000],\n                             [ 0.5000, -0.5000, -0.5000, -0.5000]]))\n\n\n\n\nScriptFunction object at 0x1628d6980&gt;\nquat1inv(quat2)*\n\nsource\n\n\ndiffQuat\n\n diffQuat (q1, q2, norm=True)\n\n\ntest_eq(diffQuat(tq1,tq2),diffQuat(tq1,tq2*5))\ntest_ne(diffQuat(tq1,tq2),diffQuat(tq1,tq2*5,norm=False))\ntest_ne(diffQuat(tq1,tq2),diffQuat(tq1[None,...],tq2[None,...]))\n\n\nsource\n\n\nsafe_acos\n\n safe_acos (t, eps=4e-08)\n\nnumericaly stable variant of arcuscosine\n\ntest_ne(safe_acos(tensor(1.))*1e6,0)\ntest_eq(safe_acos(tensor(-0.)),_pi/2)\n\n\nsource\n\n\nsafe_acos_double\n\n safe_acos_double (t, eps=1e-16)\n\nnumericaly stable variant of arcuscosine, uses 64bit floats for internal computation for increased accuracy and gradient propagation\n\ntest_ne(safe_acos_double(tensor(1.))*1e6,0)\ntest_eq(safe_acos_double(tensor(-0.)),_pi/2)\n\n\nsource\n\n\nrelativeAngle\n\n relativeAngle (q1, q2)\n\n\nsource\n\n\ninclinationAngle\n\n inclinationAngle (q1, q2)\n\n\nprint('inclination:', rad2deg(inclinationAngle(tq1,tq2)))\nprint('relative:', rad2deg(relativeAngle(tq1,tq2)))\n\ninclination: tensor([9.0000e+01, 1.7075e-06])\nrelative: tensor([1.2000e+02, 1.7075e-06])\n\n\n\nsource\n\n\npitchAngle\n\n pitchAngle (q1, q2)\n\n\nsource\n\n\nrollAngle\n\n rollAngle (q1, q2)\n\n\nprint('roll:', rad2deg(rollAngle(tq1,tq2)))\nprint('pitch:', rad2deg(pitchAngle(tq1,tq2)))\n\nroll: tensor([0., 0.])\npitch: tensor([-90.,   0.])\n\n\n\nsource\n\n\ninclinationAngleAbs\n\n inclinationAngleAbs (q)\n\n\nrad2deg(inclinationAngleAbs(tq1))\n\ntensor([ 0., 90.])\n\n\n\nsource\n\n\nrand_quat\n\n rand_quat ()\n\n\nsource\n\n\nrot_vec\n\n rot_vec (v, q)\n\n\ng = tensor([[9.81,0,0]]*5)\n\n\nr_quat = rand_quat()\nrot_vec(g,r_quat)\n\ntensor([[ 1.3754,  7.8102, -5.7746],\n        [ 1.3754,  7.8102, -5.7746],\n        [ 1.3754,  7.8102, -5.7746],\n        [ 1.3754,  7.8102, -5.7746],\n        [ 1.3754,  7.8102, -5.7746]])\n\n\n\nsource\n\n\nquatFromAngleAxis\n\n quatFromAngleAxis (angle, axis)\n\n\nsource\n\n\nquatInterp\n\n quatInterp (quat, ind, extend=False)\n\n*Interpolates an array of quaternions of (non-integer) indices using Slerp. Sampling indices are in the range 0..N-1, for values outside of this range, depending on “extend”, the first/last element or NaN is returned.\nSee also csg_bigdata.dp.utils.vecInterp.\n:param quat: array of input quaternions (N(xB)x4) :param ind: vector containing the sampling indices, shape (M,) :param extend: if true, the input data is virtually extended by the first/last value :return: interpolated quaternions (Mx4)*\n\n1e-3/_pi*180\n\ntensor([0.0573])\n\n\n\nq = torch.rand((1000000,1,4))*2-1\nq /= q.norm()\nx = torch.linspace(0,q.shape[0]-1,150001)\nq_i = quatInterp(q,x)\ntorch.isnan(inclinationAngleAbs(q_i)).sum()\n\ntensor(0)\n\n\n\nq_i = quatInterp(q,x)\n\n\nplt.figure()\nplt.plot(inclinationAngleAbs(q))\nplt.plot(x,inclinationAngleAbs(q_i))",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#loss-functions",
    "href": "quaternions.html#loss-functions",
    "title": "Tensor Quaternion Module",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nsource\n\ninclination_loss\n\n inclination_loss (q1, q2)\n\n\ninclination_loss(tq1,tq2)\n\ntensor(0.2071)\n\n\n\nsource\n\n\ninclination_loss_abs\n\n inclination_loss_abs (q1, q2)\n\n\ninclination_loss_abs(tq1,tq2)\n\ntensor(0.1464)\n\n\n\nsource\n\n\ninclination_loss_squared\n\n inclination_loss_squared (q1, q2)\n\n\n# %%timeit\ninclination_loss_squared(tq1,tq2)\n\ntensor(0.0429)\n\n\n\nsource\n\n\ninclination_loss_smooth\n\n inclination_loss_smooth (q1, q2)\n\n\n# %%timeit\ninclination_loss_smooth(tq1,tq2)\n\ntensor(0.0214)\n\n\n\nsource\n\n\nabs_inclination\n\n abs_inclination (q1, q2)\n\n\nabs_inclination(tq1,tq2)\n\ntensor(0.7854)\n\n\n\nsource\n\n\nms_inclination\n\n ms_inclination (q1, q2)\n\n\nms_inclination(tq1,tq2)\n\ntensor(1.2337)\n\n\n\nsource\n\n\nrms_inclination\n\n rms_inclination (q1, q2)\n\n\nrms_inclination(tq1,tq2)\n\ntensor(1.1107)\n\n\n\nsource\n\n\nsmooth_inclination\n\n smooth_inclination (q1, q2)\n\n\nsmooth_inclination(tq1,tq2)\n\ntensor(0.5354)\n\n\n\nsource\n\n\nrms_inclination_deg\n\n rms_inclination_deg (q1, q2)\n\n\nrms_inclination_deg(tq1,tq2)\n\ntensor([63.6396])\n\n\n\nsource\n\n\nrms_pitch_deg\n\n rms_pitch_deg (q1, q2)\n\n\nrms_pitch_deg(tq1,tq2)\n\ntensor([63.6396])\n\n\n\nsource\n\n\nrms_roll_deg\n\n rms_roll_deg (q1, q2)\n\n\nrms_roll_deg(tq1,tq2)\n\ntensor([0.])\n\n\n\nsource\n\n\nmean_inclination_deg\n\n mean_inclination_deg (q1, q2)\n\n\nmean_inclination_deg(tq1,tq2)\n\ntensor([45.])\n\n\n\nsource\n\n\nangle_loss\n\n angle_loss (q1, q2)\n\n\nsource\n\n\nangle_loss_opt\n\n angle_loss_opt (q1, q2)\n\n\nsource\n\n\nms_rel_angle\n\n ms_rel_angle (q1, q2)\n\n\nms_rel_angle(tq1,tq2)\n\ntensor(2.1932)\n\n\n\nsource\n\n\nabs_rel_angle\n\n abs_rel_angle (q1, q2)\n\n\nsource\n\n\nrms_rel_angle_deg\n\n rms_rel_angle_deg (q1, q2)\n\n\nrms_rel_angle_deg(tq1,tq2)\n\ntensor([84.8528])\n\n\n\nsource\n\n\nmean_rel_angle_deg\n\n mean_rel_angle_deg (q1, q2)\n\n\nmean_rel_angle_deg(tq1,tq2)\n\ntensor([60.])\n\n\n\nsource\n\n\ndeg_rmse\n\n deg_rmse (inp, targ)",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#callbacks",
    "href": "quaternions.html#callbacks",
    "title": "Tensor Quaternion Module",
    "section": "Callbacks",
    "text": "Callbacks\nIn order to assure that the output of the model are close to unit quaternions the distance will be added to the loss\n\nsource\n\nQuaternionRegularizer\n\n QuaternionRegularizer (reg_unit=0.0, detach=False, modules=None,\n                        every=None, remove_end=True, is_forward=True,\n                        cpu=True, include_paramless=False, hook=None)\n\nCallback that adds AR and TAR to the loss, calculated by output of provided layer\n\nsource\n\n\naugmentation_groups\n\n augmentation_groups (u_groups)\n\nreturns the rotation list corresponding to the input groups\n\nu_raw_groups = [3,3]\ntest_eq(augmentation_groups(u_raw_groups),[[0,2],[3,5]])\n\n\nsource\n\n\nQuaternionAugmentation\n\n QuaternionAugmentation (inp_groups, **kwargs)\n\nA transform that before_call its state at each __call__\n\nn_skip = 2**8\n\n\ninp,out = get_inp_out_size(dls)\n# model = SimpleGRU(inp,out,num_layers=1,hidden_size=100)\nmodel = TCN(inp,out,hl_depth=8,hl_width=10)\n\nskip = partial(SkipNLoss,n_skip=n_skip)\nmetrics=rms_inclination_deg\ncbs = [QuaternionRegularizer(reg_unit=1,modules=[model])]\n\nlrn = Learner(dls,model,loss_func=ms_inclination,opt_func=ranger,metrics=metrics)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/env_tsfast/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n  WeightNorm.apply(module, name, dim)\n\n\n\nlrn.fit_one_cycle(1,lr_max=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrms_inclination_deg\ntime\n\n\n\n\n0\n2.289876\n1.290059\n65.076988\n00:01\n\n\n\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_31206/3976315585.py:9: UserWarning: Float64 precision not supported on mps:0 device. Falling back to float32. This may reduce numerical accuracy of quaternion operations. Error: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.\n  warnings.warn(f\"Float64 precision not supported on {t.device} device. Falling back to float32. This may reduce numerical accuracy of quaternion operations. Error: {e}\")",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#resampling-model",
    "href": "quaternions.html#resampling-model",
    "title": "Tensor Quaternion Module",
    "section": "Resampling Model",
    "text": "Resampling Model\n\nsource\n\nQuaternion_ResamplingModel\n\n Quaternion_ResamplingModel (model, fs_targ, fs_mean=0, fs_std=1,\n                             quaternion_sampling=True)\n\n*Module that resamples the signal before and after the prediction of its model. Usefull for using models on datasets with different samplingrates.\nsampling_method: method used for resampling [‘resample’,‘interpolate’]*\n\n# dls = DataBlock(blocks=(SequenceBlock.from_hdf(u + ['dt'],TensorSequencesInput),\n#                         SequenceBlock.from_hdf(y,TensorQuaternionInclination)),\n#                 get_items=tfm_src,\n#                 splitter=ApplyToDict(FuncSplitter(lambda o: 'experiment2' in str(o)))\n#                ).dataloaders(hdf_files,shufflish=True,bs=128)\n# model = TCN(inp,out,hl_depth=8,hl_width=10)\n# Learner(dls,Quaternion_ResamplingModel(model,10,quaternion_sampling=False),loss_func=ms_inclination).fit(1)",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#quaternion-datablock",
    "href": "quaternions.html#quaternion-datablock",
    "title": "Tensor Quaternion Module",
    "section": "Quaternion Datablock",
    "text": "Quaternion Datablock\n\nsource\n\nHDF2Quaternion\n\n HDF2Quaternion (clm_names, clm_shift=None, truncate_sz=None,\n                 to_cls=&lt;function noop&gt;, cached=True, fs_idx=None,\n                 dt_idx=None, fast_resample=True)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nsource\n\n\nQuaternionBlock\n\n QuaternionBlock (seq_extract, padding=False)\n\nA basic wrapper that links defaults transforms for the data block API\n\ntfm_src = CreateDict([DfResamplingFactor(2000/7,np.linspace(50,500,10)),DfHDFCreateWindows(win_sz=1000,stp_sz=100,clm='acc_x')])\ndls = DataBlock(blocks=(SequenceBlock.from_hdf(u,TensorSequencesInput),\n                        QuaternionBlock.from_hdf(y)),\n                get_items=tfm_src,\n                splitter=RandomSplitter(0.5)\n               ).dataloaders(hdf_files,bs=2)\n\n\n#test_eq(len(dls.items),83877)",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#inclination-datablock",
    "href": "quaternions.html#inclination-datablock",
    "title": "Tensor Quaternion Module",
    "section": "Inclination Datablock",
    "text": "Inclination Datablock\n\nsource\n\nTensorInclination\n\n TensorInclination (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nHDF2Inclination\n\n HDF2Inclination (clm_names, clm_shift=None, truncate_sz=None,\n                  to_cls=&lt;function noop&gt;, cached=True, fs_idx=None,\n                  dt_idx=None, fast_resample=True)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nsource\n\n\nInclinationBlock\n\n InclinationBlock (seq_extract, padding=False)\n\nA basic wrapper that links defaults transforms for the data block API\n\n# f_paths = '/mnt/Data/Systemidentification/Orientation_Estimation/'\n# hdf_files = get_hdf_files(f_paths)\n# tfm_src = CreateDict([DfHDFCreateWindows(win_sz=1000,stp_sz=100,clm='acc_x')])\n# u = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z']\n# # u = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z','mag_x','mag_y','mag_z']\n# y =['opt_a','opt_b','opt_c','opt_d']\n# dls = DataBlock(blocks=(SequenceBlock.from_hdf(u),\n#                         InclinationBlock.from_hdf(y)),\n#                 get_items=tfm_src,\n#                 splitter=ApplyToDict(FuncSplitter(lambda o: 'experiment2' in str(o)))\n#                ).dataloaders(hdf_files,shufflish=True,bs=128)",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "quaternions.html#show-results",
    "href": "quaternions.html#show-results",
    "title": "Tensor Quaternion Module",
    "section": "Show Results",
    "text": "Show Results\n\nsource\n\nplot_scalar_inclination\n\n plot_scalar_inclination (axs, in_sig, targ_sig, out_sig=None, **kwargs)\n\n\nsource\n\n\nplot_quaternion_inclination\n\n plot_quaternion_inclination (axs, in_sig, targ_sig, out_sig=None,\n                              **kwargs)\n\n\nsource\n\n\nplot_quaternion_rel_angle\n\n plot_quaternion_rel_angle (axs, in_sig, targ_sig, out_sig=None, **kwargs)\n\n\ndls.show_batch(max_n=3,ds_idx=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlrn.show_results(max_n=3,ds_idx=0,shuffle=True,quat=True)\n\n\n\n\n\n\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_31206/3976315585.py:9: UserWarning: Float64 precision not supported on mps:0 device. Falling back to float32. This may reduce numerical accuracy of quaternion operations. Error: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.\n  warnings.warn(f\"Float64 precision not supported on {t.device} device. Falling back to float32. This may reduce numerical accuracy of quaternion operations. Error: {e}\")",
    "crumbs": [
      "Tensor Quaternion Module"
    ]
  },
  {
    "objectID": "tune.html",
    "href": "tune.html",
    "title": "Hyperparameter Optimization Module",
    "section": "",
    "text": "from nbdev.config import get_config\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = L([f for f in get_hdf_files(f_path) if '_test.hdf5' not in str(f)])\ntfm_src = CreateDict([DfHDFCreateWindows(win_sz=400,stp_sz=100,clm='u')])\ndls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n                get_items=tfm_src,\n                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)",
    "crumbs": [
      "Hyperparameter Optimization Module"
    ]
  },
  {
    "objectID": "tune.html#optimizer-core",
    "href": "tune.html#optimizer-core",
    "title": "Hyperparameter Optimization Module",
    "section": "optimizer core",
    "text": "optimizer core\nFirst we need a log uniform distibution for variables with vast value ranges\n\nsource\n\nlog_uniform\n\n log_uniform (min_bound, max_bound, base=10)\n\nuniform sampling in an exponential range\n\n[log_uniform(1e-8, 1e-2)() for _ in range(5)]\n\n[4.591524992137234e-08,\n 0.003755410605938488,\n 2.920688605923387e-07,\n 3.4750213799838236e-06,\n 2.1312097874133118e-08]\n\n\n\nsource\n\n\nLearnerTrainable\n\n LearnerTrainable (config:Dict[str,Any]=None,\n                   logger_creator:Callable[[Dict[str,Any]],ForwardRef('Log\n                   ger')]=None, storage:Optional[ray.train._internal.stora\n                   ge.StorageContext]=None)\n\n*Abstract class for trainable models, functions, etc.\nA call to train() on a trainable will execute one logical iteration of training. As a rule of thumb, the execution time of one train call should be large enough to avoid overheads (i.e. more than a few seconds), but short enough to report progress periodically (i.e. at most a few minutes).\nCalling save() should save the training state of a trainable to disk, and restore(path) should restore a trainable to the given state.\nGenerally you only need to implement setup, step, save_checkpoint, and load_checkpoint when subclassing Trainable.\nOther implementation methods that may be helpful to override are log_result, reset_config, cleanup, and _export_model.\nTune will convert this class into a Ray actor, which runs on a separate process. By default, Tune will also change the current working directory of this process to its corresponding trial-level log directory self.logdir. This is designed so that different trials that run on the same physical node won’t accidentally write to the same location and overstep each other.\nThe behavior of changing the working directory can be disabled by setting the RAY_CHDIR_TO_TRIAL_DIR=0 environment variable. This allows access to files in the original working directory, but relative paths should be used for read only purposes, and you must make sure that the directory is synced on all nodes if running on multiple machines.\nThe TUNE_ORIG_WORKING_DIR environment variable was the original workaround for accessing paths relative to the original working directory. This environment variable is deprecated, and the RAY_CHDIR_TO_TRIAL_DIR environment variable described above should be used instead.\nThis class supports checkpointing to and restoring from remote storage.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nDict\nNone\n\n\n\nlogger_creator\nCallable\nNone\nDeprecated (2.7)\n\n\nstorage\nOptional\nNone\n\n\n\n\n\nsource\n\n\nCBRaySaveModel\n\n CBRaySaveModel (monitor='valid_loss', comp=None, min_delta=0.0,\n                 fname='model', every_epoch=False, at_end=False,\n                 with_opt=False, reset_on_fit=True)\n\nA TrackerCallback that saves the model’s best during training in a tune checkpoint directory\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\nfname\nstr\nmodel\nmodel name to be used when saving model.\n\n\nevery_epoch\nbool\nFalse\nif true, save model after every epoch; else save only when model is better than existing best.\n\n\nat_end\nbool\nFalse\nif true, save model when training ends; else load best model if there is only one saved model.\n\n\nwith_opt\nbool\nFalse\nif true, save optimizer state (if any available) when saving model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\nstop_shared_memory_managers\n\n stop_shared_memory_managers (obj)\n\nIteratively finds and stops all SharedMemoryManager instances contained within the provided object.\n\nsource\n\n\nlearner_optimize\n\n learner_optimize (config)\n\n\nclass TrainSpecificEpoch(Callback):\n    \"Skip training up to `epoch`\"\n    order = 70\n    \n    def __init__(self, epoch:int):\n        self._skip_to = epoch\n\n    def before_epoch(self):\n        print(self.epoch)\n        # if self.epoch &lt; self._skip_to:\n        #     raise CancelEpochException\n        # if self.epoch &gt; self._skip_to:\n        # raise CancelFitException\n\n\nclass TrainableModel(tune.Trainable):\n    def setup(self, config):\n        # Assuming create_lrn and dls are accessible here or passed in config\n        self.create_lrn = ray.get(config['create_lrn'])\n        self.dls = ray.get(config['dls'])\n        self.config = config\n\n        self.lrn = self.create_lrn(self.dls, config)\n\n        self.lrn.lr = config['lr'] if 'lr' in config else 3e-3\n        if 'wd' in config: self.lrn.wd = config['wd']\n        self._setup_callbacks()\n\n        if 'reporter' not in self.config:\n            self.lrn.add_cb(CBRayReporter())\n        else:\n            self.lrn.add_cb(ray.get(self.config['reporter'])())\n\n        if self.lrn.opt is None: self.lrn.create_opt()\n        self.lrn.opt.set_hyper('lr', self.lrn.lr)\n        lr = np.array([h['lr'] for h in self.lrn.opt.hypers])\n        pct_start = config['pct_start'] if 'pct_start' in config else 0.3\n        self.n_epoch = config['n_epoch'] if 'n_epoch' in config else 10\n        lr_scheds = {'lr': combined_cos(pct_start, lr, lr, lr/div_final)}\n        self.steps=0\n\n    def step(self):\n\n        self.fit(self.n_epoch, cbs=TrainSpecificEpoch(self.steps)+ParamScheduler(scheds)+L(cbs), wd=wd)\n        self.steps += 1\n\n        \n        scores = self.lrn.recorder.values[-1]\n        metrics = {\n            'train_loss': scores[0],\n            'valid_loss': scores[1]\n        }        \n        for metric,value in zip(self.learn.metrics,scores[2:]):\n            m_name = metric.name if hasattr(metric,'name') else str(metric)\n            metrics[m_name] = value\n        return metrics\n\n    def save_checkpoint(self, checkpoint_dir):\n        file = os.path.join(temp_checkpoint_dir,'model.pth')\n        save_model(file, self.learn.model,opt=None) \n\n    def load_checkpoint(self, checkpoint_path):\n        self.lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n\n\nclass TrainableModel(tune.Trainable):\n    def setup(self, config):\n        # Assuming create_lrn and dls are accessible here or passed in config\n        self.create_lrn = ray.get(config['create_lrn'])\n        self.dls = ray.get(config['dls'])\n        self.config = config\n        self.lrn_kwargs = {'n_epoch': 100, 'pct_start': 0.5}\n\n        for attr in ['n_epoch', 'pct_start']:\n            if attr in config:\n                self.lrn_kwargs[attr] = config[attr]\n\n        self.lrn = self.create_lrn(self.dls, config)\n        self.lrn.lr = config['lr'] if 'lr' in config else 3e-3\n\n\n    def step(self):\n        print(self.iteration)\n        # fit_kwargs = {**self.lrn_kwargs,**{'cbs':TrainSpecificEpoch(self.iteration)}}\n        # fit_kwargs = {**self.lrn_kwargs,**{'cbs':SkipToEpoch(self.iteration)}}\n        # fit_kwargs = self.lrn_kwargs\n        with self.lrn.no_bar(): \n            # ray.get(self.config['fit_method'])(self.lrn,**fit_kwargs)\n            # self.lrn.fit_flat_cos(**fit_kwargs)\n            self.lrn.fit_flat_cos(self.lrn_kwargs['n_epoch'],cbs=TrainSpecificEpoch(self.iteration))\n\n        \n        metrics = {\n            'train_loss': 1,#scores[0],\n            'valid_loss': 1,#scores[1],\n             tune.result.DONE: self.iteration &gt;= self.lrn_kwargs['n_epoch']-1\n        }  \n        \n        # scores = self.lrn.recorder.values[-1]\n        # metrics = {\n        #     'train_loss': scores[0],\n        #     'valid_loss': scores[1],\n        #      tune.result.DONE: self.epoch_iter &gt;= self.lrn_kwargs['n_epoch']\n        # }        \n        # for metric,value in zip(self.lrn.metrics,scores[2:]):\n        #     m_name = metric.name if hasattr(metric,'name') else str(metric)\n        #     metrics[m_name] = value\n        return metrics\n\n    def save_checkpoint(self, checkpoint_dir):\n        file = os.path.join(temp_checkpoint_dir,'model.pth')\n        save_model(file, self.learn.model,opt=None) \n\n    def load_checkpoint(self, checkpoint_path):\n        self.lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n\n\ndls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n                    SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n            get_items=tfm_src,\n            splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)\n\nThe mutation config dictionary consists of functions that sample from a distribution. In order to retrieve a dictionary with one realisation we need the function sample_config\n\nsource\n\n\nsample_config\n\n sample_config (config)\n\n\nsource\n\n\nCBRayReporter\n\n CBRayReporter (after_create=None, before_fit=None, before_epoch=None,\n                before_train=None, before_batch=None, after_pred=None,\n                after_loss=None, before_backward=None,\n                after_cancel_backward=None, after_backward=None,\n                before_step=None, after_cancel_step=None, after_step=None,\n                after_cancel_batch=None, after_batch=None,\n                after_cancel_train=None, after_train=None,\n                before_validate=None, after_cancel_validate=None,\n                after_validate=None, after_cancel_epoch=None,\n                after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nCallback reports progress after every epoch to the ray tune logger\n\nsource\n\n\nHPOptimizer\n\n HPOptimizer (create_lrn, dls)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nTest Population Based Training\n\ndef create_lrn(dls,config):\n    lr = config['lr']\n    alpha = config['alpha']\n    beta = config['beta']\n    weight_p = config['weight_p']\n    \n    lrn = RNNLearner(dls)\n    lrn.lr = lr\n    return lrn\n\n\nconfig={\n            \"lr\": tune.loguniform(1e-2, 1e-4),\n            \"alpha\": tune.loguniform(1e-5, 10),\n            \"beta\": tune.loguniform(1e-5, 10),\n            \"weight_p\": tune.uniform(0, 0.5)}\nmut_conf = {# distribution for resampling\n            \"lr\": log_uniform(1e-8, 1e-2),\n            \"alpha\": log_uniform(1e-5, 10),\n            \"beta\": log_uniform(1e-5, 10),\n            \"weight_p\": lambda: np.random.uniform(0, 0.5)}\n\nhp_opt = HPOptimizer(create_lrn,dls)\n# hp_opt.start_ray()\n# hp_opt.optimize_pbt('pbt_test',4,config,mut_conf,perturbation_interval=1,\n#                  stop={\"training_iteration\": 1 },\n#                  resources_per_trial={\"gpu\": 0.5},\n#                  storage_path=str(Path.home() / 'ray_results'))#no cpu count is necessary\n\n\n#hp_opt.best_model()\n\n\n\nTest Grid Search\n\n# dls.cpu()\n\n\ndef create_lrn(dls,config):\n    dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n                get_items=tfm_src,\n                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)\n    lrn = RNNLearner(dls,hidden_size=config['hidden_size'],metrics=[fun_rmse,mse])\n    return lrn\n\n\nhp_opt = HPOptimizer(create_lrn,None)\n\n\nsearch_space = {\n    \"hidden_size\": tune.grid_search([10,20,50,100]),\n    'n_epoch':10\n}\n\n\n# hp_opt.optimize(optimize_func=TrainableModel,\n#                 resources_per_trial={\"cpu\": 4},\n#                 config=search_space)\n\n\n# hp_opt.analysis.get_best_config('mean_loss',mode='min')\n\n\n\nTest Random Search",
    "crumbs": [
      "Hyperparameter Optimization Module"
    ]
  },
  {
    "objectID": "04_prediction/fransys.html",
    "href": "04_prediction/fransys.html",
    "title": "Dual RNN Models",
    "section": "",
    "text": "dls = create_dls_test(prediction=True)\ninit_sz = 50\ndls.show_batch(max_n=1)"
  },
  {
    "objectID": "04_prediction/fransys.html#model",
    "href": "04_prediction/fransys.html#model",
    "title": "Dual RNN Models",
    "section": "Model",
    "text": "Model\n\nsource\n\nDiag_RNN\n\n Diag_RNN (input_size, output_size, output_layer=1, hidden_size=100,\n           rnn_layer=1, linear_layer=1, stateful=False, hidden_p=0.0,\n           input_p=0.0, weight_p=0.0, rnn_type='gru',\n           ret_full_hidden=False, normalization='', **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nDiag_RNN_raw\n\n Diag_RNN_raw (input_size, output_size, output_layer=1, hidden_size=100,\n               rnn_layer=1, linear_layer=1, stateful=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nDiagLSTM\n\n DiagLSTM (input_size, output_size, output_layer=1, hidden_size=100,\n           rnn_layer=1, linear_layer=1, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nDiag_TCN\n\n Diag_TCN (input_size, output_size, output_layer, hl_width, mlp_layers=0,\n           hl_depth=1, act=&lt;class 'torch.nn.modules.activation.Mish'&gt;,\n           bn=False, stateful=False, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nARProg_Init\n\n ARProg_Init (n_u, n_y, init_sz, n_x=0, hidden_size=100, rnn_layer=1,\n              diag_model=None, linear_layer=1, final_layer=0,\n              hidden_p=0.0, input_p=0.0, weight_p=0.0, rnn_type='gru',\n              ret_full_hidden=False, stateful=False, normalization='',\n              **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = ARProg_Init(1,1,init_sz=init_sz,rnn_layer=1,hidden_size=50)\nlrn = Learner(dls,model,loss_func=SkipNLoss(mse,init_sz))\n# lrn.fit(1,lr=3e-3)\nlrn.fit_flat_cos(1,3e-3,pct_start=0.2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.054181\n0.059382\n00:01"
  },
  {
    "objectID": "04_prediction/fransys.html#fransys",
    "href": "04_prediction/fransys.html#fransys",
    "title": "Dual RNN Models",
    "section": "FranSys",
    "text": "FranSys\n\nsource\n\nFranSys\n\n FranSys (n_u, n_y, init_sz, n_x=0, hidden_size=100, rnn_layer=1,\n          diag_model=None, linear_layer=1, init_diag_only=False,\n          final_layer=0, hidden_p=0.0, input_p=0.0, weight_p=0.0,\n          rnn_type='gru', ret_full_hidden=False, stateful=False,\n          normalization='', **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = FranSys(1,1,init_sz=init_sz,linear_layer=1,rnn_layer=2,hidden_size=50)\nlrn = Learner(dls,model,loss_func=nn.MSELoss())\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.041958\n0.032479\n00:02\n\n\n\n\n\nRNN without linear layer as diagnosis module\n\n#TCN as Diagnosis Module\ndiag_tcn = Diag_TCN(2,50,2,hl_depth=6,hl_width=20,mlp_layers=3)\nmodel = FranSys(1,1,init_sz=init_sz,linear_layer=1,rnn_layer=2,hidden_size=50,diag_model=diag_tcn)\nlrn = Learner(dls,model,loss_func=nn.MSELoss())\nlrn.add_cb(TbpttResetCB())\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.037339\n0.029248\n00:01\n\n\n\n\n\n\ndiag_rnn = Diag_RNN_raw(2,50,2,stateful=False)\nmodel = FranSys(1,1,init_sz=init_sz,linear_layer=1,rnn_layer=2,hidden_size=50,diag_model=diag_rnn)\nlrn = Learner(dls,model,loss_func=nn.MSELoss())\nlrn.add_cb(TbpttResetCB())\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.036428\n0.023112\n00:02\n\n\n\n\n\nFast variant with initsz diagnosis only\n\nmodel = FranSys(1,1,init_sz=init_sz,linear_layer=1,rnn_layer=2,hidden_size=50,init_diag_only=True)\nlrn = Learner(dls,model,loss_func=nn.MSELoss(),opt_func=ranger)\nlrn.add_cb(TbpttResetCB())\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.075652\n0.069702\n00:01"
  },
  {
    "objectID": "04_prediction/fransys.html#callbacks",
    "href": "04_prediction/fransys.html#callbacks",
    "title": "Dual RNN Models",
    "section": "Callbacks",
    "text": "Callbacks\n\nsource\n\nFranSysCallback\n\n FranSysCallback (modules, p_state_sync=10000000.0, p_diag_loss=0.0,\n                  p_osp_sync=0, p_osp_loss=0, p_tar_loss=0,\n                  sync_type='mse', targ_loss_func=&lt;function mae&gt;,\n                  osp_n_skip=None, FranSys_model=None, detach=False,\n                  **kwargs)\n\nCallback that regularizes the output of the FranSys model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodules\n\n\n\n\n\np_state_sync\nfloat\n10000000.0\nscalingfactor for regularization of hidden state deviation between diag and prog module\n\n\np_diag_loss\nfloat\n0.0\nscalingfactor of loss calculation of diag hidden state to final layer\n\n\np_osp_sync\nint\n0\nscalingfactor for regularization of hidden state deviation between one step prediction and diag hidden states\n\n\np_osp_loss\nint\n0\nscalingfactor for loss calculation of one step prediction of prog module\n\n\np_tar_loss\nint\n0\nscalingfactor for time activation regularization of combined hiddenstate of diag and prog with target sequence length\n\n\nsync_type\nstr\nmse\n\n\n\ntarg_loss_func\nfunction\nmae\n\n\n\nosp_n_skip\nNoneType\nNone\nnumber of elements to skip before osp is applied, defaults to model.init_sz\n\n\nFranSys_model\nNoneType\nNone\n\n\n\ndetach\nbool\nFalse\n\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nmodel = FranSys(1,1,init_sz=init_sz,linear_layer=1,rnn_layer=2,hidden_size=50)\ncb = FranSysCallback([model.rnn_diagnosis,model.rnn_prognosis],\n                        p_state_sync=1e-1, \n                        p_diag_loss=0.0,\n                        p_osp_sync=0,\n                        p_osp_loss=0.1,\n                        sync_type='cos_pow')\nlrn = Learner(dls,model,loss_func=nn.MSELoss(),cbs=cb,opt_func=ranger)\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.164724\n0.062946\n00:02\n\n\n\n\n\n\nsource\n\n\nFranSysCallback_variable_init\n\n FranSysCallback_variable_init (init_sz_min, init_sz_max, **kwargs)\n\nCallback reports progress after every epoch to the ray tune logger"
  },
  {
    "objectID": "04_prediction/fransys.html#learner",
    "href": "04_prediction/fransys.html#learner",
    "title": "Dual RNN Models",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nFranSysLearner\n\n FranSysLearner (dls, init_sz, attach_output=False, loss_func=L1Loss(),\n                 metrics=[&lt;function fun_rmse at 0x1490f8af0&gt;],\n                 opt_func=&lt;function Adam&gt;, lr=0.003, cbs=[], n_x=0,\n                 hidden_size=100, rnn_layer=1, diag_model=None,\n                 linear_layer=1, init_diag_only=False, final_layer=0,\n                 hidden_p=0.0, input_p=0.0, weight_p=0.0, rnn_type='gru',\n                 ret_full_hidden=False, stateful=False, normalization='',\n                 **kwargs)\n\n\nlrn = FranSysLearner(dls,init_sz=50)\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.168541\n0.126720\n0.159639\n00:01\n\n\n\n\n\n\ndls = create_dls_test(prediction=False)\nlrn = FranSysLearner(dls,init_sz=50,attach_output=True)\nlrn.fit(1,lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.179930\n0.183227\n0.232352\n00:01"
  },
  {
    "objectID": "03_models/rnn.html",
    "href": "03_models/rnn.html",
    "title": "Models",
    "section": "",
    "text": "from nbdev.config import get_config\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,clm_shift=[0,-1]),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,clm_shift=[-1])),\n                 get_items=CreateDict([DfHDFCreateWindows(win_sz=100+1,stp_sz=100,clm='u')]),\n                 splitter=ApplyToDict(ParentSplitter()))\ndb = seq.dataloaders(get_hdf_files(f_path))"
  },
  {
    "objectID": "03_models/rnn.html#rnns",
    "href": "03_models/rnn.html#rnns",
    "title": "Models",
    "section": "RNNs",
    "text": "RNNs\n\nBasic RNN\n\nsource\n\n\nRNN\n\n RNN (input_size, hidden_size, num_layers, hidden_p=0.0, input_p=0.0,\n      weight_p=0.0, rnn_type='gru', ret_full_hidden=False, stateful=False,\n      normalization='', **kwargs)\n\ninspired by https://arxiv.org/abs/1708.02182\n\nsource\n\n\nSequential_RNN\n\n Sequential_RNN (input_size, hidden_size, num_layers, hidden_p=0.0,\n                 input_p=0.0, weight_p=0.0, rnn_type='gru',\n                 ret_full_hidden=False, stateful=False, normalization='',\n                 **kwargs)\n\nRNN Variant for Sequential Modules\n\nsource\n\n\nSimpleRNN\n\n SimpleRNN (input_size, output_size, num_layers=1, hidden_size=100,\n            linear_layers=0, return_state=False, hidden_p=0.0,\n            input_p=0.0, weight_p=0.0, rnn_type='gru',\n            ret_full_hidden=False, stateful=False, normalization='',\n            **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = SimpleRNN(2,1,2,stateful=False,normalization='batchnorm')\nlrn = Learner(db,model,loss_func=nn.MSELoss())#.fit(10)\n\n\nmodel = SimpleRNN(2,1,2,rnn_type='lstm')\nlrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.031366\n0.013281\n00:00\n\n\n\n\n\n\nmodel = SimpleRNN(2,1,2,rnn_type='gru')\nlrn = Learner(db,model,loss_func=nn.MSELoss()).fit(2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.018948\n0.006698\n00:02\n\n\n1\n0.010434\n0.002743\n00:02\n\n\n\n\n\n\n\nResidual RNN\n\nsource\n\n\nResidualBlock_RNN\n\n ResidualBlock_RNN (input_size, hidden_size, hidden_p=0.0, input_p=0.0,\n                    weight_p=0.0, rnn_type='gru', ret_full_hidden=False,\n                    stateful=False, normalization='', **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nSimpleResidualRNN\n\n SimpleResidualRNN (input_size, output_size, num_blocks=1,\n                    hidden_size=100, hidden_p=0.0, input_p=0.0,\n                    weight_p=0.0, rnn_type='gru', ret_full_hidden=False,\n                    stateful=False, normalization='', **kwargs)\n\n*A sequential container.\nModules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential).\nWhat’s the difference between a Sequential and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))*\n\nmodel = SimpleResidualRNN(2,1,1,stateful=False,normalization='')\nlrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.016374\n0.002262\n00:02\n\n\n\n\n\n\n\nDense RNN\n\nsource\n\n\nDenseBlock_RNN\n\n DenseBlock_RNN (num_layers, num_input_features, growth_rate,\n                 hidden_p=0.0, input_p=0.0, weight_p=0.0, rnn_type='gru',\n                 ret_full_hidden=False, stateful=False, normalization='',\n                 **kwargs)\n\n*A sequential container.\nModules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential).\nWhat’s the difference between a Sequential and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))*\n\nsource\n\n\nDenseLayer_RNN\n\n DenseLayer_RNN (input_size, hidden_size, hidden_p=0.0, input_p=0.0,\n                 weight_p=0.0, rnn_type='gru', ret_full_hidden=False,\n                 stateful=False, normalization='', **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nDenseNet_RNN\n\n DenseNet_RNN (input_size, output_size, growth_rate=32, block_config=(3,\n               3), num_init_features=32, hidden_p=0.0, input_p=0.0,\n               weight_p=0.0, rnn_type='gru', ret_full_hidden=False,\n               stateful=False, normalization='', **kwargs)\n\n*A sequential container.\nModules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential).\nWhat’s the difference between a Sequential and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))*\n\nmodel = DenseNet_RNN(2,1,growth_rate=10,block_config=(1,1),num_init_features=2)\nlrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.119397\n0.075041\n00:06\n\n\n\n\n\n\n\nSeperate RNN\n\nsource\n\n\nSeperateRNN\n\n SeperateRNN (input_list, output_size, num_layers=1, hidden_size=100,\n              linear_layers=1, hidden_p=0.0, input_p=0.0, weight_p=0.0,\n              rnn_type='gru', ret_full_hidden=False, stateful=False,\n              normalization='', **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "03_models/layers.html",
    "href": "03_models/layers.html",
    "title": "Models",
    "section": "",
    "text": "from tsfast.datasets import create_dls_test\ndls = create_dls_test()"
  },
  {
    "objectID": "03_models/layers.html#batchnorm",
    "href": "03_models/layers.html#batchnorm",
    "title": "Models",
    "section": "Batchnorm",
    "text": "Batchnorm\n\nsource\n\nBatchNorm_1D_Stateful\n\n BatchNorm_1D_Stateful (hidden_size, seq_len=None, stateful=False,\n                        batch_first=True, eps=1e-07, momentum=0.1,\n                        affine=True, track_running_stats=True)\n\nBatchnorm for stateful models. Stores batch statistics for for every timestep seperately to mitigate transient effects.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhidden_size\n\n\n\n\n\nseq_len\nNoneType\nNone\n\n\n\nstateful\nbool\nFalse\n\n\n\nbatch_first\nbool\nTrue\n\n\n\neps\nfloat\n1e-07\n\n\n\nmomentum\nfloat\n0.1\n\n\n\naffine\nbool\nTrue\n\n\n\ntrack_running_stats\nbool\nTrue\nnum_features"
  },
  {
    "objectID": "03_models/layers.html#linear",
    "href": "03_models/layers.html#linear",
    "title": "Models",
    "section": "Linear",
    "text": "Linear\n\nsource\n\nSeqLinear\n\n SeqLinear (input_size, output_size, hidden_size=100, hidden_layer=1,\n            act=&lt;class 'torch.nn.modules.activation.Mish'&gt;,\n            batch_first=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "03_models/layers.html#autoregressive-models",
    "href": "03_models/layers.html#autoregressive-models",
    "title": "Models",
    "section": "Autoregressive Models",
    "text": "Autoregressive Models\n\nsource\n\nNormalizer1D\n\n Normalizer1D (mean, std)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nAR_Model\n\n AR_Model (model, ar=True, stateful=False, model_has_state=False,\n           return_state=False, out_sz=None)\n\nAutoregressive model container which work autoregressively if the sequence y is not provided, otherwise it works as a normal model. This way it can be trained either with teacher forcing or with autoregression\n\nmodel = AR_Model(SeqLinear(2,1),model_has_state=False,ar=True,out_sz=1)\nmodel.init_normalize(dls.one_batch())\nlrn = Learner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/12 00:00&lt;?]"
  },
  {
    "objectID": "02_learner/losses.html",
    "href": "02_learner/losses.html",
    "title": "Learner",
    "section": "",
    "text": "from tsfast.datasets import create_dls_test\ndls = create_dls_test()\nmodel = SimpleRNN(1,1)"
  },
  {
    "objectID": "02_learner/losses.html#loss-functions",
    "href": "02_learner/losses.html#loss-functions",
    "title": "Learner",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nsource\n\nignore_nan\n\n ignore_nan (func)\n\nremove nan values from tensors before function execution, reduces tensor to a flat array, apply to functions such as mse\n\nn = 1000\ny_t = torch.ones(32,n,6)\ny_t[:,20]=np.nan\ny_p = torch.ones(32,n,6)*1.1\n\n\n(~torch.isnan(y_t)).shape\n\ntorch.Size([32, 1000, 6])\n\n\n\ny_t.shape\n\ntorch.Size([32, 1000, 6])\n\n\n\nassert torch.isnan(mse(y_p,y_t))\n\n\ntest_close(mse_nan(y_p,y_t),0.01)\n\n\nsource\n\n\nfloat64_func\n\n float64_func (func)\n\ncalculate function internally with float64 and convert the result back\n\nLearner(dls,model,loss_func=float64_func(nn.MSELoss())).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.055763\n0.059929\n00:01\n\n\n\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_76375/3967170634.py:17: UserWarning: Float64 precision not supported on mps:0 device. Using original precision. This may reduce numerical accuracy. Error: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.\n  warnings.warn(f\"Float64 precision not supported on {args[0].device} device. Using original precision. This may reduce numerical accuracy. Error: {e}\")\n\n\n\nsource\n\n\nSkipNLoss\n\n SkipNLoss (fn, n_skip=0)\n\nLoss-Function modifier that skips the first n samples of sequential data\n\nLearner(dls,model,loss_func=SkipNLoss(nn.MSELoss(),n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.051679\n0.052046\n00:01\n\n\n\n\n\n\nsource\n\n\nCutLoss\n\n CutLoss (fn, l_cut=0, r_cut=None)\n\nLoss-Function modifier that skips the first n samples of sequential data\n\nLearner(dls,model,loss_func=CutLoss(nn.MSELoss(),l_cut=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.028736\n0.018902\n00:01\n\n\n\n\n\n\nsource\n\n\nweighted_mae\n\n weighted_mae (input, target)\n\n\nLearner(dls,model,loss_func=SkipNLoss(weighted_mae,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.084046\n0.065088\n00:01\n\n\n\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_76375/691337496.py:13: UserWarning: torch.logspace not supported on mps:0 device. Using cpu. This may reduce numerical performance\n  warnings.warn(f\"torch.logspace not supported on {device} device. Using cpu. This may reduce numerical performance\")\n\n\n\nsource\n\n\nRandSeqLenLoss\n\n RandSeqLenLoss (fn, min_idx=1, max_idx=None, mid_idx=None)\n\nLoss-Function modifier that truncates the sequence length of every sequence in the minibatch inidiviually randomly. At the moment slow for very big batchsizes.\n\nLearner(dls,model,loss_func=RandSeqLenLoss(nn.MSELoss())).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.036072\n0.037482\n00:07\n\n\n\n\n\n\nsource\n\n\nfun_rmse\n\n fun_rmse (inp, targ)\n\nrmse loss function defined as a function not as a AccumMetric\n\nLearner(dls,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(fun_rmse,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.010846\n0.010722\n0.051925\n00:01\n\n\n\n\n\n\nsource\n\n\ncos_sim_loss\n\n cos_sim_loss (inp, targ)\n\nrmse loss function defined as a function not as a AccumMetric\n\nLearner(dls,model,loss_func=cos_sim_loss,metrics=SkipNLoss(fun_rmse,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.234125\n0.254100\n0.051972\n00:01\n\n\n\n\n\n\nsource\n\n\ncos_sim_loss_pow\n\n cos_sim_loss_pow (inp, targ)\n\nrmse loss function defined as a function not as a AccumMetric\n\nLearner(dls,model,loss_func=cos_sim_loss_pow,metrics=SkipNLoss(fun_rmse,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.468299\n0.509000\n0.051983\n00:01\n\n\n\n\n\n\nsource\n\n\nnrmse\n\n nrmse (inp, targ)\n\nrmse loss function scaled by variance of each target variable\n\ndls.one_batch()[0].shape\n\ntorch.Size([64, 100, 1])\n\n\n\nLearner(dls,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(nrmse,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nnrmse\ntime\n\n\n\n\n0\n0.010644\n0.010091\n0.181790\n00:01\n\n\n\n\n\n\nsource\n\n\nnrmse_std\n\n nrmse_std (inp, targ)\n\nrmse loss function scaled by standard deviation of each target variable\n\nLearner(dls,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(nrmse_std,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nnrmse_std\ntime\n\n\n\n\n0\n0.010193\n0.009726\n0.078454\n00:01\n\n\n\n\n\n\nsource\n\n\nmean_vaf\n\n mean_vaf (inp, targ)\n\n\nLearner(dls,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(mean_vaf,n_skip=30)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmean_vaf\ntime\n\n\n\n\n0\n0.009576\n0.009391\n97.983543\n00:01"
  },
  {
    "objectID": "01_datasets/external.html",
    "href": "01_datasets/external.html",
    "title": "Corefunctions",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "01_datasets/external.html#all-datasets",
    "href": "01_datasets/external.html#all-datasets",
    "title": "Corefunctions",
    "section": "All datasets",
    "text": "All datasets"
  },
  {
    "objectID": "05_inference/core.html",
    "href": "05_inference/core.html",
    "title": "Inference",
    "section": "",
    "text": "source\n\nInferenceWrapper\n\n InferenceWrapper (learner, device='cpu')\n\nA wrapper class to simplify inference with a trained tsfast/fastai Learner on NumPy data. Handles normalization and state reset automatically.\n\nfrom tsfast.datasets.core import create_dls_test\nfrom tsfast.learner import RNNLearner\nfrom tsfast.prediction import FranSysLearner\n\n\ndls = create_dls_test()\nlrn = RNNLearner(dls)\nmodel = InferenceWrapper(lrn)\n\n\nmodel(np.random.randn(100, 1)).shape\n\n(100, 1)\n\n\n\nmodel(np.random.randn(100)).shape\n\n(100, 1)\n\n\n\nmodel(np.random.randn(1,100,1)).shape\n\n(100, 1)\n\n\n\nlrn = FranSysLearner(dls,10,attach_output=True)\nmodel = InferenceWrapper(lrn)\n\n\nmodel(np.random.randn(100, 1),np.random.randn(100, 1)).shape\n\n(100, 1)"
  },
  {
    "objectID": "00_data/loader.html",
    "href": "00_data/loader.html",
    "title": "Truncated Backpropagation Through Time",
    "section": "",
    "text": "Pytorch Modules for Training Models for sequential data\n\nThe tbptt dataloader needs to split the minibatches that are created in several smaller minibatches that will be returned sequentially before the next minibatch may be created.\n\nsource\n\n\n\n TbpttDl (dataset, sub_seq_len=None, seq_len=None, shuffle=True,\n          num_workers=2, bs:int=64, verbose:bool=False,\n          do_setup:bool=True, pin_memory=False, timeout=0,\n          batch_size=None, drop_last=False, indexed=None, n=None,\n          device=None, persistent_workers=False, pin_memory_device='',\n          wif=None, before_iter=None, after_item=None, before_batch=None,\n          after_batch=None, after_iter=None, create_batches=None,\n          create_item=None, create_batch=None, retain=None, get_idxs=None,\n          sample=None, shuffle_fn=None, do_batch=None)\n\nTransformed DataLoader\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\n\n\ntfm_lst = [DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='u')]\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,clm_shift=[-1,-1]),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,clm_shift=[1])),\n                 get_items=CreateDict(tfm_lst),\n                 splitter=ApplyToDict(ParentSplitter()))\ndb = seq.dataloaders(get_hdf_files(f_path),dl_type=TbpttDl,sub_seq_len=99,num_workers=6)\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_61433/1256818916.py:21: UserWarning: Sequence length (1000) is not perfectly divisible by sub_seq_len (99). The last segment of each sequence in TbpttDl will be shorter.\n  warnings.warn(\n\n\n\ndb = seq.dataloaders(get_hdf_files(f_path),dl_type=TbpttDl,sub_seq_len=100,num_workers=6)\n\n\nl = [array(x[-1][0,:,0].cpu()) for x in db.train]\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_61433/3478302433.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n  l = [array(x[-1][0,:,0].cpu()) for x in db.train]\n\n\n\nplt.figure()\nplt.plot(np.concatenate(l))\n\n\n\n\n\n\n\n\nnum_workers can be &gt; 1 with the new synchronization procedure"
  },
  {
    "objectID": "00_data/loader.html#custom-dataloaders",
    "href": "00_data/loader.html#custom-dataloaders",
    "title": "Truncated Backpropagation Through Time",
    "section": "",
    "text": "Pytorch Modules for Training Models for sequential data\n\nThe tbptt dataloader needs to split the minibatches that are created in several smaller minibatches that will be returned sequentially before the next minibatch may be created.\n\nsource\n\n\n\n TbpttDl (dataset, sub_seq_len=None, seq_len=None, shuffle=True,\n          num_workers=2, bs:int=64, verbose:bool=False,\n          do_setup:bool=True, pin_memory=False, timeout=0,\n          batch_size=None, drop_last=False, indexed=None, n=None,\n          device=None, persistent_workers=False, pin_memory_device='',\n          wif=None, before_iter=None, after_item=None, before_batch=None,\n          after_batch=None, after_iter=None, create_batches=None,\n          create_item=None, create_batch=None, retain=None, get_idxs=None,\n          sample=None, shuffle_fn=None, do_batch=None)\n\nTransformed DataLoader\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\n\n\ntfm_lst = [DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='u')]\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,clm_shift=[-1,-1]),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,clm_shift=[1])),\n                 get_items=CreateDict(tfm_lst),\n                 splitter=ApplyToDict(ParentSplitter()))\ndb = seq.dataloaders(get_hdf_files(f_path),dl_type=TbpttDl,sub_seq_len=99,num_workers=6)\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_61433/1256818916.py:21: UserWarning: Sequence length (1000) is not perfectly divisible by sub_seq_len (99). The last segment of each sequence in TbpttDl will be shorter.\n  warnings.warn(\n\n\n\ndb = seq.dataloaders(get_hdf_files(f_path),dl_type=TbpttDl,sub_seq_len=100,num_workers=6)\n\n\nl = [array(x[-1][0,:,0].cpu()) for x in db.train]\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_61433/3478302433.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n  l = [array(x[-1][0,:,0].cpu()) for x in db.train]\n\n\n\nplt.figure()\nplt.plot(np.concatenate(l))\n\n\n\n\n\n\n\n\nnum_workers can be &gt; 1 with the new synchronization procedure"
  },
  {
    "objectID": "00_data/loader.html#tbptt_reset_callback",
    "href": "00_data/loader.html#tbptt_reset_callback",
    "title": "Truncated Backpropagation Through Time",
    "section": "TBPTT_Reset_Callback",
    "text": "TBPTT_Reset_Callback\nThe stateful model needs to reset its hidden state, when a new sequence begins. The callback reads the reset flag and acts accordingly.\n\nsource\n\nreset_model_state\n\n reset_model_state (model)\n\n\nsource\n\n\nTbpttResetCB\n\n TbpttResetCB (after_create=None, before_fit=None, before_epoch=None,\n               before_train=None, before_batch=None, after_pred=None,\n               after_loss=None, before_backward=None,\n               after_cancel_backward=None, after_backward=None,\n               before_step=None, after_cancel_step=None, after_step=None,\n               after_cancel_batch=None, after_batch=None,\n               after_cancel_train=None, after_train=None,\n               before_validate=None, after_cancel_validate=None,\n               after_validate=None, after_cancel_epoch=None,\n               after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nCallback resets the rnn model with every new sequence for tbptt, calls reset_state in every module of the model"
  },
  {
    "objectID": "00_data/loader.html#example",
    "href": "00_data/loader.html#example",
    "title": "Truncated Backpropagation Through Time",
    "section": "Example",
    "text": "Example\n\nfrom tsfast.learner import RNNLearner,SkipNLoss,fun_rmse\n\n\nlrn = RNNLearner(db,num_layers=1,rnn_type='gru',stateful=False,metrics=[SkipNLoss(fun_rmse,1),fun_rmse])\nlrn.add_cb(TbpttResetCB())\n\n&lt;fastai.learner.Learner&gt;\n\n\n\nlrn.fit_one_cycle(1,lr_max=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\nfun_rmse\ntime\n\n\n\n\n0\n0.041026\n0.014218\n0.117919\n0.119088\n00:01\n\n\n\n\n\n\ndb.train.sub_seq_len = 10\n\n\nlrn.fit_one_cycle(1,lr_max=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\nfun_rmse\ntime\n\n\n\n\n0\n0.001870\n0.000182\n0.013098\n0.013459\n00:01"
  },
  {
    "objectID": "00_data/loader.html#itemlst-transform-for-weight-calculation",
    "href": "00_data/loader.html#itemlst-transform-for-weight-calculation",
    "title": "Truncated Backpropagation Through Time",
    "section": "ItemLst Transform for weight calculation",
    "text": "ItemLst Transform for weight calculation\n\nsource\n\nuniform_p_of_category\n\n uniform_p_of_category (cat_name)\n\nScales sampling weights for an even distribution between every category\n\nsource\n\n\nuniform_p_of_float\n\n uniform_p_of_float (var_name, bins=10)\n\nScales sampling weights for an even distribution of the continous variable by creating equi sized bins\n\nsource\n\n\nuniform_p_of_float_with_gaps\n\n uniform_p_of_float_with_gaps (var_name, bins=100)\n\nScales sampling weights for an even distribution of the continous variable by creating equi sized bins\n\ndef train_valid(df):   \n    ''' test function that extracts valid and train from the path string'''\n    df['train'] = df.path.astype(str).str.contains('train',regex=False)\n    return df\n\n\n# %%time\ntfm_lst = [train_valid, DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='u') ,uniform_p_of_category('train'),uniform_p_of_float('l_slc'),uniform_p_of_float_with_gaps('r_slc')]\napply_df_tfms(get_hdf_files(f_path),tfm_lst)\n\n\n\n\n\n\n\n\npath\ntrain\nl_slc\nr_slc\np_sample\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\nFalse\n0\n1001\n0.001219\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\nFalse\n1000\n2001\n0.001219\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\nFalse\n2000\n3001\n0.001170\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\nFalse\n3000\n4001\n0.002437\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\nFalse\n4000\n5001\n0.002340\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\nTrue\n74000\n75001\n0.007357\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\nTrue\n75000\n76001\n0.007357\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\nTrue\n76000\n77001\n0.007357\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\nTrue\n77000\n78001\n0.006180\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\nTrue\n78000\n79001\n0.011124\n\n\n\n\n185 rows × 5 columns\n\n\n\n\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,clm_shift=[-1,-1]),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,clm_shift=[1])),\n                 get_items=CreateDict(tfm_lst),\n                 splitter=ApplyToDict(ParentSplitter()))\n# db = seq.dataloaders(get_hdf_files('test_data/battery'),dl_type=TbpttDl,sub_seq_len=200)\ndb = seq.dataloaders(get_hdf_files(f_path),dl_type=WeightedDL_Factory(TbpttDl),sub_seq_len=500)\n\n\ndb.train.wgts.shape\n\n(79,)\n\n\n\ndb.train.wgts[:5],db.valid.wgts[:5]\n\n(array([0.0034722 , 0.0034722 , 0.00333331, 0.0069444 , 0.00666662]),\n array([0.03704676, 0.03704676, 0.03556485, 0.07409344, 0.0711297 ]))\n\n\n\nlrn = RNNLearner(db,num_layers=1,rnn_type='gru',stateful=False,metrics=[SkipNLoss(fun_rmse,1),fun_rmse])\nlrn.add_cb(TbpttResetCB())\nlrn.fit_one_cycle(1,lr_max=3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\nfun_rmse\ntime\n\n\n\n\n0\n0.061885\n0.060174\n0.245287\n0.245301\n00:01"
  },
  {
    "objectID": "00_data/loader.html#importance-sampling-callback",
    "href": "00_data/loader.html#importance-sampling-callback",
    "title": "Truncated Backpropagation Through Time",
    "section": "Importance Sampling Callback",
    "text": "Importance Sampling Callback\n\n# #| export\n# class ImportanceSampling(Callback):\n#     #modify the dataloader weights to sample items with higher loss more often\n#     def __init__(self, filter_criterion=nn.HuberLoss(reduction='none')):\n#         self.filter_criterion = filter_criterion\n#         self.loss_list = None #store loss of each item in the dataloader\n\n#     def begin_fit(self):\n#         #empty the loss list at the beginning of each fit\n#         self.loss_list = None\n\n#     def after_pred(self):\n#         # store loss of each item in the batch\n#         if not self.training: return\n#         losses = self.filter_criterion(self.learn.pred, *self.learn.yb)\n#         if losses.ndim &gt;= 2: losses = losses.mean(tuple(range(1,losses.ndim)))  # If loss is multi-dimensional, take the mean over all but the first dimension\n#         #get the indices of each item in the batch from the dataloader\n#         # import pdb; pdb.set_trace()\n\n#         #if the loss_list is empty, initialize it with dataloader wgts\n#         if self.loss_list is None:\n#             self.loss_list = self.learn.dls.train.wgts.clone()\n#             #scale the loss_list mean to the mean loss of the batch\n#             self.loss_list *= self.loss_list.numel()/losses.mean()\n\n#         #store the loss of each item in the loss_list\n        \n#     def after_epoch(self):\n#         #modify the dataloader weights to sample items with higher loss more often\n#         if not self.training: return\n#         self.learn.dls.train.wgts = self.loss_list\n#         self.learn.dls.train.wgts /= self.learn.dls.train.wgts.sum()\n\n\n# lrn = RNNLearner(db,num_layers=1,stateful=False,metrics=[SkipNLoss(fun_rmse,1),fun_rmse],cbs=ImportanceSampling())\n# lrn.fit(1)\n\n\n# tfm_lst = [train_valid, DfHDFCreateWindows(win_sz=500+1,stp_sz=500,clm='u')]\n# seq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,clm_shift=[-1,-1]),\n#                         SequenceBlock.from_hdf(['y'],TensorSequencesOutput,clm_shift=[1])),\n#                  get_items=CreateDict(tfm_lst),\n#                  splitter=ApplyToDict(ParentSplitter()))\n# # db = seq.dataloaders(get_hdf_files('test_data/battery'),dl_type=TbpttDl,sub_seq_len=200)\n# db = seq.dataloaders(get_hdf_files('test_data/battery'),dl_type=WeightedDL_Factory(TfmdDL))\n\n# RNNLearner(db,num_layers=1,stateful=False,metrics=[SkipNLoss(fun_rmse,1),fun_rmse],cbs=ImportanceSampling()).fit(1)"
  },
  {
    "objectID": "00_data/transforms.html",
    "href": "00_data/transforms.html",
    "title": "Corefunctions",
    "section": "",
    "text": "source\n\n\n\n\n SeqSlice (l_slc=None, r_slc=None)\n\nTake a slice from an array-like object. Useful for e.g. shifting input and output\n\nl_shift = SeqSlice(r_slc=-1)\narr = np.ones((5))\ntest_eq(l_shift(arr),arr[:-1])\n\n\n\n\n\nsource\n\n\n\n\n SeqNoiseInjection (std=0.1, mean=0.0, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\nns_mean = tensor([0.,10.1,3.1])\nns_std = tensor([1.,1.1,0.1])\nx,x.shape\n\n(TensorSequencesInput([[ 1.,  1.,  1.],\n                       [-1., -1., -1.]]),\n torch.Size([2, 3]))\n\n\n\nseq_noise = SeqNoiseInjection(std=ns_std,mean=ns_mean)\nseq_noise(x)\n\nTensorSequencesInput([[ 1.,  1.,  1.],\n                      [-1., -1., -1.]])\n\n\n\nseq_noise = SeqNoiseInjection(std=ns_std*10)\nseq_noise(x)\n\nTensorSequencesInput([[ 1.,  1.,  1.],\n                      [-1., -1., -1.]])\n\n\n\nsource\n\n\n\n\n SeqNoiseInjection_Varying (std_std=0.1, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\nns_std = tensor([1.,1.1,0.1])\nx,x.shape\n\n(TensorSequencesInput([[0, 0, 0],\n                       [0, 0, 0]]),\n torch.Size([2, 3]))\n\n\n\nseq_noise = SeqNoiseInjection_Varying(std_std=ns_std)\nseq_noise(x)\n\nTensorSequencesInput([[0, 0, 0],\n                      [0, 0, 0]])\n\n\n\nsource\n\n\n\n\n SeqNoiseInjection_Grouped (std_std, std_idx, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\nns_std = tensor([1.,1.1,0.1])\nx,x.shape\n\n(TensorSequencesInput([[0, 0, 0],\n                       [0, 0, 0]]),\n torch.Size([2, 3]))\n\n\n\nseq_noise = SeqNoiseInjection_Grouped(std_std=[3.,0],std_idx=[0,0,1])\nseq_noise(x)\n\nTensorSequencesInput([[0, 0, 0],\n                      [0, 0, 0]])\n\n\n\n\n\n\nsource\n\n\n\n\n SeqBiasInjection (std=0.1, mean=0.0, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[[1,1,1],[-1,-1,-1.0]]]))\nns_mean = tensor([0.,10.1,3.1])\nns_std = tensor([1.,1.1,0.1])\nseq_bias = SeqBiasInjection(std=ns_std,mean=ns_std)\nseq_bias(x)[...,0]\n\nTensorSequencesInput([[ 1., -1.]])\n\n\n\nseq_bias.mean\n\ntensor([1.0000, 1.1000, 0.1000])\n\n\n\nseq_bias = SeqBiasInjection(std=ns_std*10)\nseq_bias(x)\n\nTensorSequencesInput([[[ 1.,  1.,  1.],\n                       [-1., -1., -1.]]])\n\n\n\n\n\nNormalize is programmed for TensorImage as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor.\n\nsource\n\n\n\n\n Normalize (mean=None, std=None, axes=(0, 2, 3))\n\nNormalize/denorm batch of TensorImage\n\nsource\n\n\n\n\n Normalize (mean=None, std=None, axes=(0, 2, 3))\n\nNormalize/denorm batch of TensorImage\n\nnorm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\nx,norm(x)\n\n(TensorSequencesInput([[[ 1.,  1.,  1.],\n                        [-1., -1., -1.]]]),\n TensorSequencesInput([[[  1.0000,  -8.2727, -21.0000],\n                        [ -1.0000, -10.0909, -41.0000]]]))"
  },
  {
    "objectID": "00_data/transforms.html#transformations",
    "href": "00_data/transforms.html#transformations",
    "title": "Corefunctions",
    "section": "",
    "text": "source\n\n\n\n\n SeqSlice (l_slc=None, r_slc=None)\n\nTake a slice from an array-like object. Useful for e.g. shifting input and output\n\nl_shift = SeqSlice(r_slc=-1)\narr = np.ones((5))\ntest_eq(l_shift(arr),arr[:-1])\n\n\n\n\n\nsource\n\n\n\n\n SeqNoiseInjection (std=0.1, mean=0.0, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\nns_mean = tensor([0.,10.1,3.1])\nns_std = tensor([1.,1.1,0.1])\nx,x.shape\n\n(TensorSequencesInput([[ 1.,  1.,  1.],\n                       [-1., -1., -1.]]),\n torch.Size([2, 3]))\n\n\n\nseq_noise = SeqNoiseInjection(std=ns_std,mean=ns_mean)\nseq_noise(x)\n\nTensorSequencesInput([[ 1.,  1.,  1.],\n                      [-1., -1., -1.]])\n\n\n\nseq_noise = SeqNoiseInjection(std=ns_std*10)\nseq_noise(x)\n\nTensorSequencesInput([[ 1.,  1.,  1.],\n                      [-1., -1., -1.]])\n\n\n\nsource\n\n\n\n\n SeqNoiseInjection_Varying (std_std=0.1, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\nns_std = tensor([1.,1.1,0.1])\nx,x.shape\n\n(TensorSequencesInput([[0, 0, 0],\n                       [0, 0, 0]]),\n torch.Size([2, 3]))\n\n\n\nseq_noise = SeqNoiseInjection_Varying(std_std=ns_std)\nseq_noise(x)\n\nTensorSequencesInput([[0, 0, 0],\n                      [0, 0, 0]])\n\n\n\nsource\n\n\n\n\n SeqNoiseInjection_Grouped (std_std, std_idx, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\nns_std = tensor([1.,1.1,0.1])\nx,x.shape\n\n(TensorSequencesInput([[0, 0, 0],\n                       [0, 0, 0]]),\n torch.Size([2, 3]))\n\n\n\nseq_noise = SeqNoiseInjection_Grouped(std_std=[3.,0],std_idx=[0,0,1])\nseq_noise(x)\n\nTensorSequencesInput([[0, 0, 0],\n                      [0, 0, 0]])\n\n\n\n\n\n\nsource\n\n\n\n\n SeqBiasInjection (std=0.1, mean=0.0, p=1.0)\n\nA transform that before_call its state at each __call__\n\nx = TensorSequencesInput(tensor([[[1,1,1],[-1,-1,-1.0]]]))\nns_mean = tensor([0.,10.1,3.1])\nns_std = tensor([1.,1.1,0.1])\nseq_bias = SeqBiasInjection(std=ns_std,mean=ns_std)\nseq_bias(x)[...,0]\n\nTensorSequencesInput([[ 1., -1.]])\n\n\n\nseq_bias.mean\n\ntensor([1.0000, 1.1000, 0.1000])\n\n\n\nseq_bias = SeqBiasInjection(std=ns_std*10)\nseq_bias(x)\n\nTensorSequencesInput([[[ 1.,  1.,  1.],\n                       [-1., -1., -1.]]])\n\n\n\n\n\nNormalize is programmed for TensorImage as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor.\n\nsource\n\n\n\n\n Normalize (mean=None, std=None, axes=(0, 2, 3))\n\nNormalize/denorm batch of TensorImage\n\nsource\n\n\n\n\n Normalize (mean=None, std=None, axes=(0, 2, 3))\n\nNormalize/denorm batch of TensorImage\n\nnorm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\nx,norm(x)\n\n(TensorSequencesInput([[[ 1.,  1.,  1.],\n                        [-1., -1., -1.]]]),\n TensorSequencesInput([[[  1.0000,  -8.2727, -21.0000],\n                        [ -1.0000, -10.0909, -41.0000]]]))"
  },
  {
    "objectID": "00_data/core.html",
    "href": "00_data/core.html",
    "title": "Corefunctions",
    "section": "",
    "text": "The data will be extracted and prepared via transforms. Those are grouped in: - Type Transforms: Those extraxt the needed components from the source items, like input sequences or target scalar values. The work on single tensors. - Item Transforms: Those Transforms may work on tuple level and therefore may process relationships between input and output. - Batch Transform: Those transforms work on batch level. They receive batched tensors and may apply lazy transforms like normalization very effeciently.\nAn application example may look like the following: - sourceitems: - path extraction with hdf5 file endings - create pandas dataframe with information for type transforms, like slices - filter items in pandas dataframe - type transforms: - extract hdf5 input and output sequence - create windows - item transforms: - filter sequence by value - shift output sequence by 1 element - batch transforms: - noise injection - normalization\n\nsource\n\n\n\n obj_in_lst (lst, cls)\n\nretrieve first object of type cls from a list\n\nsource\n\n\n\n\n count_parameters (model)\n\nretrieve number of trainable parameters of a model\n\n\n\nThe file paths may be extracted with get_files of fastai2. get_hdf_files removes the need of writing the hdf5 file extension.\nThen a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function.\n\n\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\n\n\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\nlen(hdf_files),hdf_files[0]\n\n(3,\n Path('/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'))\n\n\n\nsource\n\n\n\n\n get_hdf_files (path, recurse=True, folders=None)\n\nGet hdf5 files in path recursively, only in folders, if specified.\n\nhdf_files = get_hdf_files(f_path)\nlen(hdf_files),hdf_files[0]\n\n(3,\n Path('/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'))\n\n\n\n\n\nIn order to extract mulitple realizations of one file with different modifications, we create a list of properties. Pandas Dataframes are to slow for iteration but very fast and convenient for creations. So after creation of the pandas Dataframe we convert it to a list of dictionaries.\n\nsource\n\n\n\n\n apply_df_tfms (src, pd_tfms=None)\n\nCreate Pandas Dataframe out of a list of items, with a list of df transforms applied\n\ndf = apply_df_tfms(hdf_files)\ndf.head()\n\n\n\n\n\n\n\n\npath\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n\n\n\n\n\n\n\n\ntest_eq(apply_df_tfms(hdf_files),apply_df_tfms(apply_df_tfms(hdf_files)))\n\n\nsource\n\n\n\n\n CreateDict (pd_tfms=None)\n\nCreate List of Dictionarys out of a list of items, with a list of df transforms applied\n\nl_dict =CreateDict()(hdf_files)\nl_dict\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5'},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5'}]\n\n\n\nsource\n\n\n\n\n ValidClmContains (lst_valid)\n\nadd validation column using a list of strings that are part of the validation frames\n\nlst_valid = ['valid']\nCreateDict([ValidClmContains(lst_valid)])(hdf_files)\n\nCPU times: user 432 μs, sys: 4 μs, total: 436 μs\nWall time: 449 μs\n\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5',\n  'valid': False},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5',\n  'valid': False}]\n\n\n\nsource\n\n\n\n\n ValidClmIs (lst_valid)\n\nadds validation column using a list of validation filenames\n\nlst_valid = ['test_data/battery/train/Sim_RealisticCycle2.hdf5',\n'test_data/battery/valid/Sim_RealisticCycle3.hdf5']\nCreateDict([ValidClmIs(lst_valid)])(hdf_files)\n\nCPU times: user 275 μs, sys: 7 μs, total: 282 μs\nWall time: 284 μs\n\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': False},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5',\n  'valid': False},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5',\n  'valid': False}]\n\n\n\nsource\n\n\n\n\n FilterClm (clm_name, func=&lt;function &lt;lambda&gt;&gt;)\n\nadds validation column using a list of validation filenames\n\nCreateDict([ValidClmIs(lst_valid),FilterClm('valid')])(hdf_files)\n\n[]\n\n\n\nsource\n\n\n\n\n get_hdf_seq_len (df, clm, ds=None)\n\nextract the sequence length of the dataset with the ‘clm’ name and ‘f_path’ path\n\nsource\n\n\n\n\n df_get_hdf_seq_len (df, clm, ds=None)\n\nextracts the sequence length of every file in advance to prepare repeated window extractions with ‘DfHDFCreateWindows’\n\nsource\n\n\n\n\n DfHDFGetSeqLen (clm)\n\n\ndf_get_hdf_seq_len(df,'u')\n\n\n\n\n\n\n\n\npath\nseq_len\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n\n\n\n\n\n\n\n\nDfHDFGetSeqLen('u')(df)\n\n\n\n\n\n\n\n\npath\nseq_len\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n DfResamplingFactor (src_fs, lst_targ_fs)\n\n\ntarg_fs = [50,100,300]\ntest_eq(len(DfResamplingFactor(100,targ_fs)(df)),9)   \ndf['src_fs'] = 200.\ntest_eq(len(DfResamplingFactor('src_fs',targ_fs)(df)),9)\n\n\nsource\n\n\n\n\n DfHDFCreateWindows (win_sz, stp_sz, clm, fixed_start=False,\n                     fixed_end=False)\n\ncreate windows of sequences, splits sequence into multiple items\n\ncreate_win = DfHDFCreateWindows(win_sz=100.2,stp_sz=100,clm='u')\nwin_df = create_win(df)\nwin_df\n\nCPU times: user 710 μs, sys: 496 μs, total: 1.21 ms\nWall time: 807 μs\n\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n100.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n100\n200.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n200\n300.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n300\n400.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n400\n500.2\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79400\n79500.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79500\n79600.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79600\n79700.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79700\n79800.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79800\n79900.2\n\n\n\n\n1877 rows × 5 columns\n\n\n\n\nwin_df = DfHDFCreateWindows(win_sz=20_000,stp_sz=1000,clm='u')(df)\ntest_eq(len(win_df),131)\nwin_df\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n1000\n21000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n2000\n22000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n3000\n23000\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n56000\n76000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n57000\n77000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n58000\n78000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n59000\n79000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n60000\n80000\n\n\n\n\n131 rows × 5 columns\n\n\n\n\ntest_eq(create_win(df_get_hdf_seq_len(df,'u')) , create_win(df))\n\n\nres_win_df = create_win(DfResamplingFactor(20,[0.1])(df))\nres_win_df\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\ntarg_fs\nresampling_factor\nl_slc\nr_slc\n\n\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n0\n100.2\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n100\n200.2\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n200\n300.2\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n300\n400.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0.1\n0.005\n0\n100.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0.1\n0.005\n100\n200.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0.1\n0.005\n200\n300.2\n\n\n\n\n\n\n\n\ntest_eq(len(res_win_df),7)\n\n\nquery_expr = 'l_slc &lt;= 200'\nfilt_df = win_df.query(query_expr)\nfilt_df\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0\n20000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0\n20000\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n DfApplyFuncSplit (split_func, func1, func2)\n\napply two different functions on the dataframe, func1 on the first indices of split_func, func2 on the second indices. Split_func is a Training, Validation split function\n\ncreate_win_split = DfApplyFuncSplit(\n    IndexSplitter([1,2]),\n    DfHDFCreateWindows(win_sz=10000,stp_sz=1,clm='u'),\n    DfHDFCreateWindows(win_sz=10000,stp_sz=10000,clm='u')\n)\ncreate_win_split(df)\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n10000\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n1\n10001\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n2\n10002\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n3\n10003\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n4\n10004\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n30000\n40000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n40000\n50000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n50000\n60000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n60000\n70000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n70000\n80000\n\n\n\n\n10017 rows × 5 columns\n\n\n\n\nsource\n\n\n\n\n DfFilterQuery (query)\n\n\ntest_eq(DfFilterQuery(query_expr)(win_df),filt_df)\n\n\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\nsrc_dicts[:5]\n\nCPU times: user 7.65 ms, sys: 1.21 ms, total: 8.86 ms\nWall time: 8.36 ms\n\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 0,\n  'r_slc': 101},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 10,\n  'r_slc': 111},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 20,\n  'r_slc': 121},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 30,\n  'r_slc': 131},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 40,\n  'r_slc': 141}]\n\n\n\nsource\n\n\n\n\n DfDropClmExcept (clms=['path', 'l_slc', 'r_slc', 'p_sample',\n                  'resampling_factor'])\n\ndrop unused dataframe columns as a last optional step to accelerate dictionary conversion\n\n\n\n\nDer Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus: - (Sequence, Scalar, Sequence) &lt;-&gt; (input,input,output)\n\n\nTwo different functions, based on pandas df and on lists\n\n\nSometimes we need to shift columns of a sequence by a specific value. Then we cant simply slice the array but have to handle each column individually. First a performance test has to be made.\n\nsource\n\n\n\n\n\n calc_shift_offsets (clm_shift)\n\n\nshft = [0,0,-1,1]\ncalc_shift_offsets(shft)\n\n(array([1, 1, 0, 2]), array([-1, -1, -2,  0]), np.int64(2))\n\n\nboth shifting methods have their own performance character. vstack needs double the time on short sequences, while the creation of a seperate array with copy becomes worse starting at around 5000 elements\n\n# ta = array([[1,2,3]*2]*10000)\n\n\n# %%timeit\n# y = np.vstack([ta[i:-ta.shape[1]+i,i] for i in range(ta.shape[1])]).T\n\n\n# %%timeit\n# x = np.zeros((ta.shape[0]-ta.shape[1],ta.shape[1]))\n# for i in range(ta.shape[1]):\n#     x[:,i] = ta[i:-ta.shape[1]+i,i]\n\n\n\nHDF5 performance is massively affected by the dtype of the signals. f4 (32 bit floating point) Numbers are faster to load and lead to smaller files then f8 numbers.\n\nsource\n\n\n\n\n\n running_mean (x, N)\n\n\nsource\n\n\n\n\n downsample_mean (x, N)\n\n\nsource\n\n\n\n\n resample_interp (x, resampling_factor, sequence_first=True,\n                  lowpass_cut=1.0, upsample_cubic_cut=None)\n\n*signal resampling using linear or cubic interpolation\nx: signal to resample with shape: features x resampling_dimension or resampling_dimension x features if sequence_first=True resampling_factor: Factor &gt; 0 that scales the signal lowpass_cut: Upper boundary for resampling_factor that activates the lowpassfilter, low values exchange accuracy for performance, default is 0.7 upsample_cubic_cut: Lower boundary for resampling_factor that activates cubic interpolation at high upsampling values. Improves signal dynamics in exchange of performance. None deactivates cubic interpolation*\n\nx = np.random.normal(size=(100000,9))\ntest_eq(resample_interp(x,0.3).shape[0],30000)\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_31734/551091804.py:35: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n  x = array(nn.functional.interpolate(x_int, size=targ_size, mode='linear',align_corners=False)[0])\n\n\n\nsource\n\n\n\n\n hdf_extract_sequence (hdf_path, clms, dataset=None, l_slc=None,\n                       r_slc=None, resampling_factor=None, fs_idx=None,\n                       dt_idx=False, fast_resample=True)\n\n*extracts a sequence with the shape [seq_len x num_features]\nhdf_path: file path of hdf file, may be a string or path type clms: list of dataset names of sequences in hdf file dataset: dataset root for clms. Useful for multiples sequences stored in one file. l_slc: left boundary for extraction of a window of the whole sequence r_slc: right boundary for extraction of a window of the whole sequence resampling_factor: scaling factor for the sequence length, uses ‘resample_interp’ for resampling fs_idx: clms list idx of fs entry in sequence. Will be scaled by resampling_factor after resampling dt_idx: clms list idx of dt entry in sequence. Will be scaled by resampling_factor after resampling fast_resample: if True, uses linear interpolation with anti-aliasing filter for faster resampling. Is less accurate than fft based resampling*\n\nsource\n\n\n\n\n Memoize (fn)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n MemoizeMP (fn)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n HDF2Sequence (clm_names, clm_shift=None, truncate_sz=None,\n               to_cls=&lt;function noop&gt;, cached=True, fs_idx=None,\n               dt_idx=None, fast_resample=True)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\n# %%timeit\nhdf2seq = HDF2Sequence(['u','y'],cached=False)\nhdf2seq(hdf_files[0])\n\narray([[ 0.27021001,  0.24308601],\n       [ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(20000, 2))\n\n\n\nhdf2seq = HDF2Sequence(['u','y'],clm_shift=[1,1])\nhdf2seq(hdf_files[0])\n\narray([[ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       [-0.0295274 ,  0.23656251],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(19999, 2))\n\n\n\nhdf2seq = HDF2Sequence(['u','y'],cached='shared')\nprint(hdf2seq(hdf_files[0]))\n\n[[ 0.27021001  0.24308601]\n [ 0.14557693  0.23587583]\n [ 0.05459135  0.23415912]\n ...\n [ 0.78831281 -0.1781944 ]\n [ 0.61458185 -0.14866701]\n [ 0.55758711 -0.11364614]]\n\n\n\n# %%timeit\nhdf2seq(hdf_files[0])\n\narray([[ 0.27021001,  0.24308601],\n       [ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(20000, 2))\n\n\n\nhdf2seq = HDF2Sequence(['u','y'],cached=True)\n\n\n# %%timeit\nhdf2seq(hdf_files[0])\n\narray([[ 0.27021001,  0.24308601],\n       [ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(20000, 2))\n\n\nDie Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden\n\nhdf2seq = HDF2Sequence(['u'])\nhdf2seq(hdf_files[0]).shape\n\n(20000, 1)\n\n\n\npipe = Pipeline(HDF2Sequence(['u','y']))\n\n\n# res_pipe = pipe(hdf_files)\n# len(res_pipe), res_pipe[0][0]\n\n\n\nCaching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data.\n\ntfms=[  [HDF2Sequence(['u','y'],cached=None)],\n        [HDF2Sequence(['y'],cached=None)]]\ndsrc = Datasets(src_dicts[:1000],tfms=tfms)\n\n\nlen(dsrc)\n\n1000\n\n\n\n# %%time\n# for x in dsrc:\n#     x\n\n\ntfms=[  [HDF2Sequence(['u','y'],cached=True,clm_shift=[1,2])],\n        [HDF2Sequence(['y'],cached=True)]]\ndsrc = Datasets(src_dicts[:1000],tfms=tfms)\n\n\n# # %%timeit\n# for x in dsrc:\n#     x\n\nCaching is way faster because every file gets loaded multiple times\n\n\n\n\n\nsource\n\n\n\n\n hdf2scalars (hdf_path, c_names, dataset=None)\n\n\n# hdf2scalars('/mnt/data/sicwell/hdf5/Cycles/ch3/cycle00568.hdf5',['soc','temperature1'],dataset='measurement_00000')\n\n\nsource\n\n\n\n\n HDF2Scalars (clm_names, to_cls=&lt;function noop&gt;)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\n# HDF2Scalars(['soc','temperature1'])({'path':'/mnt/data/sicwell/hdf5/Cycles/ch3/cycle00568.hdf5','dataset':'measurement_00000'})\n\n\n\n\n\nsource\n\n\n\n\n ScalarSequenceElement (idx, to_cls=&lt;function noop&gt;)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nScalarSequenceElement(-1)(hdf2seq(hdf_files[0]))\n\narray([0.55758711])\n\n\n\n\n\n\nsource\n\n\n\n\n TensorSequencesOutput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorSequencesInput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorSequences (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nf = TensorSequencesInput.from_hdf(['u'])\ntype(f(hdf_files[0]))\n\nnumpy.ndarray\n\n\n\n# TensorSequences(np.ones((30,2))).show()\n\n\n\n\n\n\n toTensorSequencesOutput (*args, split_idx=None, **kwargs)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\n\n\n\n\n toTensorSequencesInput (*args, split_idx=None, **kwargs)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nsource\n\n\n\n\n TensorScalarsOutput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorScalarsInput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorScalars (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\nThe tensor subclassing mechanism since pytorch 1.7 keeps the tensor type in tensor operations. Operations with different branches of subclasses of tensors require a implementation of ‘torch_function’. Fastai implements ‘TensorBase.register_func’ to mark methods that behave for the given types like the default torch operation.\nhttps://pytorch.org/docs/stable/notes/extending.html#extending-torch\n\nx1 = TensorSequencesInput(torch.rand((10,10)))\nx2 = TensorSequencesOutput(torch.rand((10,10)))\ntorch.nn.functional.mse_loss(x1,x2)\n\nTensorSequencesInput(0.1615)\n\n\n\n\n\n\ntfms=[  [HDF2Sequence(['u']),toTensorSequencesInput],\n        [HDF2Sequence(['y']),toTensorSequencesOutput]]\nds = Datasets(get_hdf_files(f_path),tfms=tfms)\n\n\ndls = ds.dataloaders(bs=1)\ndls.one_batch()[0].shape\n\ntorch.Size([1, 88000, 1])\n\n\n\n\n\n\n\nsource\n\n\n\n plot_sequence (axs, in_sig, targ_sig, out_sig=None, **kwargs)\n\n\nsource\n\n\n\n\n plot_seqs_single_figure (n_samples, n_targ, samples, plot_func,\n                          outs=None, **kwargs)\n\n\nsource\n\n\n\n\n plot_seqs_multi_figures (n_samples, n_targ, samples, plot_func,\n                          outs=None, **kwargs)\n\n\ndls.show_batch()"
  },
  {
    "objectID": "00_data/core.html#extract-source-items",
    "href": "00_data/core.html#extract-source-items",
    "title": "Corefunctions",
    "section": "",
    "text": "The file paths may be extracted with get_files of fastai2. get_hdf_files removes the need of writing the hdf5 file extension.\nThen a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function.\n\n\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\n\n\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\nlen(hdf_files),hdf_files[0]\n\n(3,\n Path('/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'))\n\n\n\nsource\n\n\n\n\n get_hdf_files (path, recurse=True, folders=None)\n\nGet hdf5 files in path recursively, only in folders, if specified.\n\nhdf_files = get_hdf_files(f_path)\nlen(hdf_files),hdf_files[0]\n\n(3,\n Path('/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'))\n\n\n\n\n\nIn order to extract mulitple realizations of one file with different modifications, we create a list of properties. Pandas Dataframes are to slow for iteration but very fast and convenient for creations. So after creation of the pandas Dataframe we convert it to a list of dictionaries.\n\nsource\n\n\n\n\n apply_df_tfms (src, pd_tfms=None)\n\nCreate Pandas Dataframe out of a list of items, with a list of df transforms applied\n\ndf = apply_df_tfms(hdf_files)\ndf.head()\n\n\n\n\n\n\n\n\npath\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n\n\n\n\n\n\n\n\ntest_eq(apply_df_tfms(hdf_files),apply_df_tfms(apply_df_tfms(hdf_files)))\n\n\nsource\n\n\n\n\n CreateDict (pd_tfms=None)\n\nCreate List of Dictionarys out of a list of items, with a list of df transforms applied\n\nl_dict =CreateDict()(hdf_files)\nl_dict\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5'},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5'}]\n\n\n\nsource\n\n\n\n\n ValidClmContains (lst_valid)\n\nadd validation column using a list of strings that are part of the validation frames\n\nlst_valid = ['valid']\nCreateDict([ValidClmContains(lst_valid)])(hdf_files)\n\nCPU times: user 432 μs, sys: 4 μs, total: 436 μs\nWall time: 449 μs\n\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5',\n  'valid': False},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5',\n  'valid': False}]\n\n\n\nsource\n\n\n\n\n ValidClmIs (lst_valid)\n\nadds validation column using a list of validation filenames\n\nlst_valid = ['test_data/battery/train/Sim_RealisticCycle2.hdf5',\n'test_data/battery/valid/Sim_RealisticCycle3.hdf5']\nCreateDict([ValidClmIs(lst_valid)])(hdf_files)\n\nCPU times: user 275 μs, sys: 7 μs, total: 282 μs\nWall time: 284 μs\n\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': False},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5',\n  'valid': False},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5',\n  'valid': False}]\n\n\n\nsource\n\n\n\n\n FilterClm (clm_name, func=&lt;function &lt;lambda&gt;&gt;)\n\nadds validation column using a list of validation filenames\n\nCreateDict([ValidClmIs(lst_valid),FilterClm('valid')])(hdf_files)\n\n[]\n\n\n\nsource\n\n\n\n\n get_hdf_seq_len (df, clm, ds=None)\n\nextract the sequence length of the dataset with the ‘clm’ name and ‘f_path’ path\n\nsource\n\n\n\n\n df_get_hdf_seq_len (df, clm, ds=None)\n\nextracts the sequence length of every file in advance to prepare repeated window extractions with ‘DfHDFCreateWindows’\n\nsource\n\n\n\n\n DfHDFGetSeqLen (clm)\n\n\ndf_get_hdf_seq_len(df,'u')\n\n\n\n\n\n\n\n\npath\nseq_len\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n\n\n\n\n\n\n\n\nDfHDFGetSeqLen('u')(df)\n\n\n\n\n\n\n\n\npath\nseq_len\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n DfResamplingFactor (src_fs, lst_targ_fs)\n\n\ntarg_fs = [50,100,300]\ntest_eq(len(DfResamplingFactor(100,targ_fs)(df)),9)   \ndf['src_fs'] = 200.\ntest_eq(len(DfResamplingFactor('src_fs',targ_fs)(df)),9)\n\n\nsource\n\n\n\n\n DfHDFCreateWindows (win_sz, stp_sz, clm, fixed_start=False,\n                     fixed_end=False)\n\ncreate windows of sequences, splits sequence into multiple items\n\ncreate_win = DfHDFCreateWindows(win_sz=100.2,stp_sz=100,clm='u')\nwin_df = create_win(df)\nwin_df\n\nCPU times: user 710 μs, sys: 496 μs, total: 1.21 ms\nWall time: 807 μs\n\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n100.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n100\n200.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n200\n300.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n300\n400.2\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n400\n500.2\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79400\n79500.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79500\n79600.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79600\n79700.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79700\n79800.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n79800\n79900.2\n\n\n\n\n1877 rows × 5 columns\n\n\n\n\nwin_df = DfHDFCreateWindows(win_sz=20_000,stp_sz=1000,clm='u')(df)\ntest_eq(len(win_df),131)\nwin_df\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n1000\n21000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n2000\n22000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n3000\n23000\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n56000\n76000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n57000\n77000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n58000\n78000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n59000\n79000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n60000\n80000\n\n\n\n\n131 rows × 5 columns\n\n\n\n\ntest_eq(create_win(df_get_hdf_seq_len(df,'u')) , create_win(df))\n\n\nres_win_df = create_win(DfResamplingFactor(20,[0.1])(df))\nres_win_df\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\ntarg_fs\nresampling_factor\nl_slc\nr_slc\n\n\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n0\n100.2\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n100\n200.2\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n200\n300.2\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0.1\n0.005\n300\n400.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0.1\n0.005\n0\n100.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0.1\n0.005\n100\n200.2\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0.1\n0.005\n200\n300.2\n\n\n\n\n\n\n\n\ntest_eq(len(res_win_df),7)\n\n\nquery_expr = 'l_slc &lt;= 200'\nfilt_df = win_df.query(query_expr)\nfilt_df\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n20000\n\n\n1\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5\n88000\n200.0\n0\n20000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n0\n20000\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n DfApplyFuncSplit (split_func, func1, func2)\n\napply two different functions on the dataframe, func1 on the first indices of split_func, func2 on the second indices. Split_func is a Training, Validation split function\n\ncreate_win_split = DfApplyFuncSplit(\n    IndexSplitter([1,2]),\n    DfHDFCreateWindows(win_sz=10000,stp_sz=1,clm='u'),\n    DfHDFCreateWindows(win_sz=10000,stp_sz=10000,clm='u')\n)\ncreate_win_split(df)\n\n\n\n\n\n\n\n\npath\nseq_len\nsrc_fs\nl_slc\nr_slc\n\n\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n0\n10000\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n1\n10001\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n2\n10002\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n3\n10003\n\n\n0\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5\n20000\n200.0\n4\n10004\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n30000\n40000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n40000\n50000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n50000\n60000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n60000\n70000\n\n\n2\n/Users/daniel/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5\n80000\n200.0\n70000\n80000\n\n\n\n\n10017 rows × 5 columns\n\n\n\n\nsource\n\n\n\n\n DfFilterQuery (query)\n\n\ntest_eq(DfFilterQuery(query_expr)(win_df),filt_df)\n\n\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\nsrc_dicts[:5]\n\nCPU times: user 7.65 ms, sys: 1.21 ms, total: 8.86 ms\nWall time: 8.36 ms\n\n\n[{'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 0,\n  'r_slc': 101},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 10,\n  'r_slc': 111},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 20,\n  'r_slc': 121},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 30,\n  'r_slc': 131},\n {'path': '/Users/daniel/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5',\n  'valid': True,\n  'l_slc': 40,\n  'r_slc': 141}]\n\n\n\nsource\n\n\n\n\n DfDropClmExcept (clms=['path', 'l_slc', 'r_slc', 'p_sample',\n                  'resampling_factor'])\n\ndrop unused dataframe columns as a last optional step to accelerate dictionary conversion"
  },
  {
    "objectID": "00_data/core.html#convert-paths-to-sequence-objects",
    "href": "00_data/core.html#convert-paths-to-sequence-objects",
    "title": "Corefunctions",
    "section": "",
    "text": "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus: - (Sequence, Scalar, Sequence) &lt;-&gt; (input,input,output)\n\n\nTwo different functions, based on pandas df and on lists\n\n\nSometimes we need to shift columns of a sequence by a specific value. Then we cant simply slice the array but have to handle each column individually. First a performance test has to be made.\n\nsource\n\n\n\n\n\n calc_shift_offsets (clm_shift)\n\n\nshft = [0,0,-1,1]\ncalc_shift_offsets(shft)\n\n(array([1, 1, 0, 2]), array([-1, -1, -2,  0]), np.int64(2))\n\n\nboth shifting methods have their own performance character. vstack needs double the time on short sequences, while the creation of a seperate array with copy becomes worse starting at around 5000 elements\n\n# ta = array([[1,2,3]*2]*10000)\n\n\n# %%timeit\n# y = np.vstack([ta[i:-ta.shape[1]+i,i] for i in range(ta.shape[1])]).T\n\n\n# %%timeit\n# x = np.zeros((ta.shape[0]-ta.shape[1],ta.shape[1]))\n# for i in range(ta.shape[1]):\n#     x[:,i] = ta[i:-ta.shape[1]+i,i]\n\n\n\nHDF5 performance is massively affected by the dtype of the signals. f4 (32 bit floating point) Numbers are faster to load and lead to smaller files then f8 numbers.\n\nsource\n\n\n\n\n\n running_mean (x, N)\n\n\nsource\n\n\n\n\n downsample_mean (x, N)\n\n\nsource\n\n\n\n\n resample_interp (x, resampling_factor, sequence_first=True,\n                  lowpass_cut=1.0, upsample_cubic_cut=None)\n\n*signal resampling using linear or cubic interpolation\nx: signal to resample with shape: features x resampling_dimension or resampling_dimension x features if sequence_first=True resampling_factor: Factor &gt; 0 that scales the signal lowpass_cut: Upper boundary for resampling_factor that activates the lowpassfilter, low values exchange accuracy for performance, default is 0.7 upsample_cubic_cut: Lower boundary for resampling_factor that activates cubic interpolation at high upsampling values. Improves signal dynamics in exchange of performance. None deactivates cubic interpolation*\n\nx = np.random.normal(size=(100000,9))\ntest_eq(resample_interp(x,0.3).shape[0],30000)\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_31734/551091804.py:35: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n  x = array(nn.functional.interpolate(x_int, size=targ_size, mode='linear',align_corners=False)[0])\n\n\n\nsource\n\n\n\n\n hdf_extract_sequence (hdf_path, clms, dataset=None, l_slc=None,\n                       r_slc=None, resampling_factor=None, fs_idx=None,\n                       dt_idx=False, fast_resample=True)\n\n*extracts a sequence with the shape [seq_len x num_features]\nhdf_path: file path of hdf file, may be a string or path type clms: list of dataset names of sequences in hdf file dataset: dataset root for clms. Useful for multiples sequences stored in one file. l_slc: left boundary for extraction of a window of the whole sequence r_slc: right boundary for extraction of a window of the whole sequence resampling_factor: scaling factor for the sequence length, uses ‘resample_interp’ for resampling fs_idx: clms list idx of fs entry in sequence. Will be scaled by resampling_factor after resampling dt_idx: clms list idx of dt entry in sequence. Will be scaled by resampling_factor after resampling fast_resample: if True, uses linear interpolation with anti-aliasing filter for faster resampling. Is less accurate than fft based resampling*\n\nsource\n\n\n\n\n Memoize (fn)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n MemoizeMP (fn)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n HDF2Sequence (clm_names, clm_shift=None, truncate_sz=None,\n               to_cls=&lt;function noop&gt;, cached=True, fs_idx=None,\n               dt_idx=None, fast_resample=True)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\n# %%timeit\nhdf2seq = HDF2Sequence(['u','y'],cached=False)\nhdf2seq(hdf_files[0])\n\narray([[ 0.27021001,  0.24308601],\n       [ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(20000, 2))\n\n\n\nhdf2seq = HDF2Sequence(['u','y'],clm_shift=[1,1])\nhdf2seq(hdf_files[0])\n\narray([[ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       [-0.0295274 ,  0.23656251],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(19999, 2))\n\n\n\nhdf2seq = HDF2Sequence(['u','y'],cached='shared')\nprint(hdf2seq(hdf_files[0]))\n\n[[ 0.27021001  0.24308601]\n [ 0.14557693  0.23587583]\n [ 0.05459135  0.23415912]\n ...\n [ 0.78831281 -0.1781944 ]\n [ 0.61458185 -0.14866701]\n [ 0.55758711 -0.11364614]]\n\n\n\n# %%timeit\nhdf2seq(hdf_files[0])\n\narray([[ 0.27021001,  0.24308601],\n       [ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(20000, 2))\n\n\n\nhdf2seq = HDF2Sequence(['u','y'],cached=True)\n\n\n# %%timeit\nhdf2seq(hdf_files[0])\n\narray([[ 0.27021001,  0.24308601],\n       [ 0.14557693,  0.23587583],\n       [ 0.05459135,  0.23415912],\n       ...,\n       [ 0.78831281, -0.1781944 ],\n       [ 0.61458185, -0.14866701],\n       [ 0.55758711, -0.11364614]], shape=(20000, 2))\n\n\nDie Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden\n\nhdf2seq = HDF2Sequence(['u'])\nhdf2seq(hdf_files[0]).shape\n\n(20000, 1)\n\n\n\npipe = Pipeline(HDF2Sequence(['u','y']))\n\n\n# res_pipe = pipe(hdf_files)\n# len(res_pipe), res_pipe[0][0]\n\n\n\nCaching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data.\n\ntfms=[  [HDF2Sequence(['u','y'],cached=None)],\n        [HDF2Sequence(['y'],cached=None)]]\ndsrc = Datasets(src_dicts[:1000],tfms=tfms)\n\n\nlen(dsrc)\n\n1000\n\n\n\n# %%time\n# for x in dsrc:\n#     x\n\n\ntfms=[  [HDF2Sequence(['u','y'],cached=True,clm_shift=[1,2])],\n        [HDF2Sequence(['y'],cached=True)]]\ndsrc = Datasets(src_dicts[:1000],tfms=tfms)\n\n\n# # %%timeit\n# for x in dsrc:\n#     x\n\nCaching is way faster because every file gets loaded multiple times\n\n\n\n\n\nsource\n\n\n\n\n hdf2scalars (hdf_path, c_names, dataset=None)\n\n\n# hdf2scalars('/mnt/data/sicwell/hdf5/Cycles/ch3/cycle00568.hdf5',['soc','temperature1'],dataset='measurement_00000')\n\n\nsource\n\n\n\n\n HDF2Scalars (clm_names, to_cls=&lt;function noop&gt;)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\n# HDF2Scalars(['soc','temperature1'])({'path':'/mnt/data/sicwell/hdf5/Cycles/ch3/cycle00568.hdf5','dataset':'measurement_00000'})\n\n\n\n\n\nsource\n\n\n\n\n ScalarSequenceElement (idx, to_cls=&lt;function noop&gt;)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nScalarSequenceElement(-1)(hdf2seq(hdf_files[0]))\n\narray([0.55758711])\n\n\n\n\n\n\nsource\n\n\n\n\n TensorSequencesOutput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorSequencesInput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorSequences (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nf = TensorSequencesInput.from_hdf(['u'])\ntype(f(hdf_files[0]))\n\nnumpy.ndarray\n\n\n\n# TensorSequences(np.ones((30,2))).show()\n\n\n\n\n\n\n toTensorSequencesOutput (*args, split_idx=None, **kwargs)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\n\n\n\n\n toTensorSequencesInput (*args, split_idx=None, **kwargs)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nsource\n\n\n\n\n TensorScalarsOutput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorScalarsInput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\n\n\n TensorScalars (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\nThe tensor subclassing mechanism since pytorch 1.7 keeps the tensor type in tensor operations. Operations with different branches of subclasses of tensors require a implementation of ‘torch_function’. Fastai implements ‘TensorBase.register_func’ to mark methods that behave for the given types like the default torch operation.\nhttps://pytorch.org/docs/stable/notes/extending.html#extending-torch\n\nx1 = TensorSequencesInput(torch.rand((10,10)))\nx2 = TensorSequencesOutput(torch.rand((10,10)))\ntorch.nn.functional.mse_loss(x1,x2)\n\nTensorSequencesInput(0.1615)\n\n\n\n\n\n\ntfms=[  [HDF2Sequence(['u']),toTensorSequencesInput],\n        [HDF2Sequence(['y']),toTensorSequencesOutput]]\nds = Datasets(get_hdf_files(f_path),tfms=tfms)\n\n\ndls = ds.dataloaders(bs=1)\ndls.one_batch()[0].shape\n\ntorch.Size([1, 88000, 1])"
  },
  {
    "objectID": "00_data/core.html#show-batches-and-results",
    "href": "00_data/core.html#show-batches-and-results",
    "title": "Corefunctions",
    "section": "",
    "text": "source\n\n\n\n plot_sequence (axs, in_sig, targ_sig, out_sig=None, **kwargs)\n\n\nsource\n\n\n\n\n plot_seqs_single_figure (n_samples, n_targ, samples, plot_func,\n                          outs=None, **kwargs)\n\n\nsource\n\n\n\n\n plot_seqs_multi_figures (n_samples, n_targ, samples, plot_func,\n                          outs=None, **kwargs)\n\n\ndls.show_batch()"
  },
  {
    "objectID": "00_data/block.html",
    "href": "00_data/block.html",
    "title": "Corefunctions",
    "section": "",
    "text": "A Datasets combines all implemented components on item level.\n\nsource\n\n\n\n pad_sequence (batch, sorting=False)\n\ncollate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow\n\n\n\n\nfrom nbdev.config import get_config\nfrom tsfast.data.core import CreateDict, ValidClmContains,DfHDFCreateWindows\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\n\n\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\n\ntfms=[  [HDF2Sequence(['u','y']),SeqSlice(l_slc=1),toTensorSequencesInput],\n        [HDF2Sequence(['y']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\nsplits = PercentageSplitter()([x['path'] for x in src_dicts])\ndsrc = Datasets(src_dicts,tfms=tfms,splits=splits)\n\n\n# %%timeit\n# dsrc[0]\n\n\ndb = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\ndb.one_batch()[0].shape\n\ntorch.Size([128, 100, 2])\n\n\n\n\n\n\nsource\n\n\n\n\n SequenceBlock (seq_extract, padding=False)\n\nA basic wrapper that links defaults transforms for the data block API\n\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,padding=True,cached=None),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,cached=None)),\n                get_items=tfm_src,\n                splitter=ApplyToDict(ParentSplitter()))\n\n\ndls = seq.dataloaders(hdf_files)\n\n\nsource\n\n\n\n\n ScalarBlock (scl_extract)\n\nA basic wrapper that links defaults transforms for the data block API\n\nsource\n\n\n\n\n ScalarNormalize (mean=None, std=None, axes=(0,))\n\nA transform with a __repr__ that shows its attrs"
  },
  {
    "objectID": "00_data/block.html#dataloaders-creation",
    "href": "00_data/block.html#dataloaders-creation",
    "title": "Corefunctions",
    "section": "",
    "text": "A Datasets combines all implemented components on item level.\n\nsource\n\n\n\n pad_sequence (batch, sorting=False)\n\ncollate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow\n\n\n\n\nfrom nbdev.config import get_config\nfrom tsfast.data.core import CreateDict, ValidClmContains,DfHDFCreateWindows\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\n\n\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\n\ntfms=[  [HDF2Sequence(['u','y']),SeqSlice(l_slc=1),toTensorSequencesInput],\n        [HDF2Sequence(['y']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\nsplits = PercentageSplitter()([x['path'] for x in src_dicts])\ndsrc = Datasets(src_dicts,tfms=tfms,splits=splits)\n\n\n# %%timeit\n# dsrc[0]\n\n\ndb = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\ndb.one_batch()[0].shape\n\ntorch.Size([128, 100, 2])\n\n\n\n\n\n\nsource\n\n\n\n\n SequenceBlock (seq_extract, padding=False)\n\nA basic wrapper that links defaults transforms for the data block API\n\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,padding=True,cached=None),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,cached=None)),\n                get_items=tfm_src,\n                splitter=ApplyToDict(ParentSplitter()))\n\n\ndls = seq.dataloaders(hdf_files)\n\n\nsource\n\n\n\n\n ScalarBlock (scl_extract)\n\nA basic wrapper that links defaults transforms for the data block API\n\nsource\n\n\n\n\n ScalarNormalize (mean=None, std=None, axes=(0,))\n\nA transform with a __repr__ that shows its attrs"
  },
  {
    "objectID": "00_data/split.html",
    "href": "00_data/split.html",
    "title": "Corefunctions",
    "section": "",
    "text": "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\nSplitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n\n\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True).sorted()\n\n\nsplitter = IndexSplitter([1,2])\ntest_eq(splitter(hdf_files),[[0],[1,2]])\n\n\nlist_dict = CreateDict()(hdf_files)\nlist_dict\n\n[{'path': '/home/pheenix/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5'},\n {'path': '/home/pheenix/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5'},\n {'path': '/home/pheenix/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'}]\n\n\n\ntest_eq(splitter(list_dict),splitter(hdf_files))\n\n\n\n\nItems, bei denen die definierte Funktion True zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht.\n\nsplitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\nsplitter(hdf_files)\ntest_eq(splitter(hdf_files),[[0,1],[2]])\n\n((#2) [np.int64(0),np.int64(1)], (#1) [2])\n\n\n\n\n\nSplitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet\n\nsource\n\n\n\n\n ParentSplitter (train_name='train', valid_name='valid')\n\nSplit items from the parent folder names (train_name and valid_name).\n\nsplitter = ParentSplitter()\ntest_eq(splitter(hdf_files),[[1],[2]])\n\n\ntest_eq(splitter(list_dict),splitter(hdf_files))\n\n\n\n\n\nsource\n\n\n\n\n PercentageSplitter (pct=0.8)\n\nSplit items in order in relative quantity.\n\nsplitter = PercentageSplitter(0.7)\n#test_eq(splitter(hdf_files),[[0,1],[2]])\n\n\n\n\nIn Case of the Datablock API your items are a list of dictionaries. If you want to apply a Splitter to the path stored within you need a wrapper function.\n\nsource\n\n\n\n\n ApplyToDict (fn, key='path')\n\n\nsplitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\ntest_fail(lambda: splitter(list_dict))\n\n\ndict_splitter = ApplyToDict(splitter)\ntest_eq(dict_splitter(list_dict),splitter(hdf_files))\ndict_splitter(list_dict)\n\n((#2) [np.int64(0),np.int64(1)], (#1) [2])\n\n\n\n\n\nUsing the ‘valid’ column of the Dataframe that has been created by a transformation.\n\nfrom tsfast.data.core import CreateDict, ValidClmContains,DfHDFCreateWindows\n\n\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\n\n\nvalid_clm_splitter(src_dicts)\n\n((#16780) [np.int64(0),np.int64(1),np.int64(2),np.int64(3),np.int64(4),np.int64(5),np.int64(6),np.int64(7),np.int64(8),np.int64(9)...],\n (#1990) [16780,16781,16782,16783,16784,16785,16786,16787,16788,16789...])"
  },
  {
    "objectID": "00_data/split.html#split-in-training-validation",
    "href": "00_data/split.html#split-in-training-validation",
    "title": "Corefunctions",
    "section": "",
    "text": "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\nSplitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n\n\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True).sorted()\n\n\nsplitter = IndexSplitter([1,2])\ntest_eq(splitter(hdf_files),[[0],[1,2]])\n\n\nlist_dict = CreateDict()(hdf_files)\nlist_dict\n\n[{'path': '/home/pheenix/Development/tsfast/test_data/WienerHammerstein/test/WienerHammerstein_test.hdf5'},\n {'path': '/home/pheenix/Development/tsfast/test_data/WienerHammerstein/train/WienerHammerstein_train.hdf5'},\n {'path': '/home/pheenix/Development/tsfast/test_data/WienerHammerstein/valid/WienerHammerstein_valid.hdf5'}]\n\n\n\ntest_eq(splitter(list_dict),splitter(hdf_files))\n\n\n\n\nItems, bei denen die definierte Funktion True zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht.\n\nsplitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\nsplitter(hdf_files)\ntest_eq(splitter(hdf_files),[[0,1],[2]])\n\n((#2) [np.int64(0),np.int64(1)], (#1) [2])\n\n\n\n\n\nSplitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet\n\nsource\n\n\n\n\n ParentSplitter (train_name='train', valid_name='valid')\n\nSplit items from the parent folder names (train_name and valid_name).\n\nsplitter = ParentSplitter()\ntest_eq(splitter(hdf_files),[[1],[2]])\n\n\ntest_eq(splitter(list_dict),splitter(hdf_files))\n\n\n\n\n\nsource\n\n\n\n\n PercentageSplitter (pct=0.8)\n\nSplit items in order in relative quantity.\n\nsplitter = PercentageSplitter(0.7)\n#test_eq(splitter(hdf_files),[[0,1],[2]])\n\n\n\n\nIn Case of the Datablock API your items are a list of dictionaries. If you want to apply a Splitter to the path stored within you need a wrapper function.\n\nsource\n\n\n\n\n ApplyToDict (fn, key='path')\n\n\nsplitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\ntest_fail(lambda: splitter(list_dict))\n\n\ndict_splitter = ApplyToDict(splitter)\ntest_eq(dict_splitter(list_dict),splitter(hdf_files))\ndict_splitter(list_dict)\n\n((#2) [np.int64(0),np.int64(1)], (#1) [2])\n\n\n\n\n\nUsing the ‘valid’ column of the Dataframe that has been created by a transformation.\n\nfrom tsfast.data.core import CreateDict, ValidClmContains,DfHDFCreateWindows\n\n\ntfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\nsrc_dicts = tfm_src(hdf_files)\n\n\nvalid_clm_splitter(src_dicts)\n\n((#16780) [np.int64(0),np.int64(1),np.int64(2),np.int64(3),np.int64(4),np.int64(5),np.int64(6),np.int64(7),np.int64(8),np.int64(9)...],\n (#1990) [16780,16781,16782,16783,16784,16785,16786,16787,16788,16789...])"
  },
  {
    "objectID": "spectogram.html",
    "href": "spectogram.html",
    "title": "Corefunctions",
    "section": "",
    "text": "5.3 Spectrogram-Datablock\n\nsource\n\n\nTensorSpectrogramOutput\n\n TensorSpectrogramOutput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nTensorSpectrogramInput\n\n TensorSpectrogramInput (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nTensorSpectrogram\n\n TensorSpectrogram (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nfrom nbdev.config import get_config\n\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n\n\nseq_spec = TensorSpectrogramInput(HDF2Sequence(['u'],to_cls=TensorSpectrogramInput)._hdf_extract_sequence(hdf_files[0],r_slc=2000))\nseq_spec.shape\n\ntorch.Size([2000, 1])\n\n\n\nsource\n\n\nSequence2Spectrogram\n\n Sequence2Spectrogram (scaling='log', n_fft:int=400,\n                       win_length:Optional[int]=None,\n                       hop_length:Optional[int]=None, pad:int=0,\n                       window_fn:Callable[...,torch.Tensor]=&lt;built-in\n                       method hann_window of type object at 0x11cf2c330&gt;,\n                       power:Optional[float]=2.0, normalized:bool=False,\n                       wkwargs:Optional[dict]=None, **kwargs)\n\ncalculates the FFT of a sequence\n\nsource\n\n\nSpectrogramBlock\n\n SpectrogramBlock (seq_extract, padding=False, n_fft=100, hop_length=None,\n                   normalized=False)\n\nA basic wrapper that links defaults transforms for the data block API\n\ndls_spec = DataBlock(blocks=(SpectrogramBlock.from_hdf(['u','y'],n_fft=100,hop_length=10,normalized=True),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n                get_items= CreateDict([DfHDFCreateWindows(win_sz=2000+1,stp_sz=10,clm='u')]),\n                splitter=ApplyToDict(ParentSplitter())).dataloaders(hdf_files)\n\n\ndls_spec.one_batch()[0].shape\n\ntorch.Size([64, 2, 51, 201])",
    "crumbs": [
      "Corefunctions"
    ]
  },
  {
    "objectID": "01_datasets/core.html",
    "href": "01_datasets/core.html",
    "title": "Corefunctions",
    "section": "",
    "text": "project_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,padding=True,cached=None),\n                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,cached=None)),\n                get_items=CreateDict([DfHDFCreateWindows(win_sz=100+1,stp_sz=100,clm='u')]),\n                splitter=ApplyToDict(ParentSplitter()))\ndls = seq.dataloaders(hdf_files)\n\ncreating a Dataloader manually for each Dataset is cumbersome. The flexibility of the datablocks api is not needed when we can take some assumptions. 1. The individual .hdf5 files have a “train”, “valid” and “test” parent directory which divides them 2. All input and output signals are 1d and have the same length\nTo implement this function, we need to store the normalization parameters of a dataset into a file, so a trained model can work with the dataset even when it is loaded again.\nWe assume that the dataloader always provides normalized data. When the model is deployed, it has to normalize the data itself.\n\nsource\n\nextract_mean_std_from_dls\n\n extract_mean_std_from_dls (dls)\n\n\nextract_mean_std_from_dls(dls)\n\n(TensorSequencesInput([[[-0.0100, -0.0461]]]),\n TensorSequencesInput([[[0.6691, 0.2276]]]))\n\n\n\ntest_eq(type(extract_mean_std_from_dls(dls)),tuple)\ntest_eq(len(extract_mean_std_from_dls(dls)),2)\n\n\nsource\n\n\ndict_file_save\n\n dict_file_save (key, value, f_path='dls_normalize.p')\n\nsave value to a dictionary file, appends if it already exists\n\nval = [1,2,5]\ndict_file_save('tst_key',val)\n\n\nsource\n\n\ndict_file_load\n\n dict_file_load (key, f_path='dls_normalize.p')\n\nload value from a dictionary file\n\ntest_eq(dict_file_load('tst_key'),val)\n\n\nsource\n\n\nextract_mean_std_from_hdffiles\n\n extract_mean_std_from_hdffiles (lst_files, lst_signals)\n\nCalculate the mean and standard deviation of the signals from the provided HDF5 files.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nlst_files\nList of paths to HDF5 files\n\n\nlst_signals\nList of signal names, each a dataset within the HDF5 files\n\n\n\n\nextract_mean_std_from_hdffiles(hdf_files,['u','y'])\n\n(array([-0.00387449, -0.03890106], dtype=float32),\n array([0.65983725, 0.23847243], dtype=float32))\n\n\n\nsource\n\n\nextract_mean_std_from_dataset\n\n extract_mean_std_from_dataset (lst_files, u, x, y)\n\n\nextract_mean_std_from_dataset(hdf_files,['u'],[],['y'])\n\n((array([-0.00387546], dtype=float32), array([0.6569116], dtype=float32)),\n (None, None),\n (array([-0.0389192], dtype=float32), array([0.23567446], dtype=float32)))\n\n\n\nsource\n\n\nis_dataset_directory\n\n is_dataset_directory (ds_path)\n\n*Checks if the given directory path is a dataset with hdf5 files.\n:param ds_path: The path to the directory to check. :return: True if the directory contains ‘train’, ‘valid’, and ‘test’ subdirectories, each of which must contain at least one HDF5 file. False otherwise.*\n\ntest_eq(is_dataset_directory(f_path),True)\ntest_eq(is_dataset_directory(f_path.parent),False)\n\n\nsource\n\n\ncreate_dls\n\n create_dls (u, y, dataset:pathlib.Path|list, win_sz:int=100, x:list=[],\n             stp_sz:int=1, sub_seq_len:int=None, bs:int=64,\n             prediction:bool=False, input_delay:bool=False,\n             valid_stp_sz:int=None, cached:bool=True, num_workers:int=5,\n             max_batches_training:int=300, max_batches_valid:int=None,\n             dls_id:str=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nu\n\n\nlist of input signal names\n\n\ny\n\n\nlist of output signal names\n\n\ndataset\npathlib.Path | list\n\npath to dataset with train,valid and test folders, or list of filepaths\n\n\nwin_sz\nint\n100\ninitial window size\n\n\nx\nlist\n[]\noptional list of state signal names\n\n\nstp_sz\nint\n1\nstep size between consecutive windows\n\n\nsub_seq_len\nint\nNone\nif provided uses truncated backpropagation throug time with this sub sequence length\n\n\nbs\nint\n64\nbatch size\n\n\nprediction\nbool\nFalse\nif true, the output is concatenated to the input, mainly for prediction tasks\n\n\ninput_delay\nbool\nFalse\nif true, the input is delayed by one step\n\n\nvalid_stp_sz\nint\nNone\nstep size between consecutive validation windows, defaults to win_sz\n\n\ncached\nbool\nTrue\nif true, the data is cached in RAM\n\n\nnum_workers\nint\n5\nnumber of processes for the dataloader, 0 for no multiprocessing\n\n\nmax_batches_training\nint\n300\nlimits the number of training batches in a single epoch\n\n\nmax_batches_valid\nint\nNone\nlimits the number of validation batches in a single epoch\n\n\ndls_id\nstr\nNone\nidentifier for the dataloader to cache the normalization values, does not cache when not provided\n\n\n\n\ndls = create_dls(u=['u'],y=['y'],dataset=f_path,dls_id='wh')\ndls.show_batch(max_n=1)\nplt.title('Training Dataloader Simulation')\ndls[2].show_batch(max_n=1)\nplt.title('Test Dataloader Simulation')\n\nText(0.5, 1.0, 'Test Dataloader Simulation')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndls = create_dls(u=['u'],y=['y'],dataset=f_path,dls_id='wh',prediction=True)\ndls.show_batch(max_n=1)\nplt.title('Training Dataloader Prediction')\ndls[2].show_batch(max_n=1)\nplt.title('Test Dataloader Prediction')\n\nText(0.5, 1.0, 'Test Dataloader Prediction')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# def loader_factory(**defaults):\n    \n#     @delegates(create_dataloader)\n#     def loader(**kwargs):\n#         combined_args = {**defaults, **kwargs}\n#         return create_dataloader(**combined_args)\n    \n#     return loader\n\n# create_dls_test = loader_factory(\n#         u=['u'],y=['y'],\n#         dataset=f_path,\n#         win_sz=100,\n#         stp_sz=100\n#     )\n\n\ndls = create_dls_test()\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "02_learner/learner.html",
    "href": "02_learner/learner.html",
    "title": "Learner",
    "section": "",
    "text": "from tsfast.datasets.core import *\ndls = create_dls_test(prediction=True)\nmodel = SimpleRNN(1,1)"
  },
  {
    "objectID": "02_learner/learner.html#rnn-learner",
    "href": "02_learner/learner.html#rnn-learner",
    "title": "Learner",
    "section": "RNN Learner",
    "text": "RNN Learner\nThe Learners include model specific optimizations. Removing the first n_skip samples of the loss function of transient time, greatly improves training stability. In\n\nsource\n\nRNNLearner\n\n RNNLearner (dls, loss_func=L1Loss(), metrics=[&lt;function fun_rmse at\n             0x145fb9630&gt;], n_skip=0, num_layers=1, hidden_size=100,\n             stateful=False, opt_func=&lt;function Adam&gt;, cbs=None,\n             linear_layers=0, return_state=False, hidden_p=0.0,\n             input_p=0.0, weight_p=0.0, rnn_type='gru',\n             ret_full_hidden=False, normalization='', **kwargs)\n\n\nRNNLearner(dls,rnn_type='gru').fit(1,1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.026982\n0.025624\n0.226313\n00:01\n\n\n\n\n\n\nRNNLearner(dls,rnn_type='gru',stateful=True).fit(1,1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.024248\n0.022891\n0.213916\n00:01"
  },
  {
    "objectID": "02_learner/learner.html#tcn-learner",
    "href": "02_learner/learner.html#tcn-learner",
    "title": "Learner",
    "section": "TCN Learner",
    "text": "TCN Learner\nPerforms better on multi input data. Higher beta values allow a way smoother prediction. Way faster then RNNs in prediction.\n\nsource\n\nTCNLearner\n\n TCNLearner (dls, num_layers=3, hidden_size=100, loss_func=L1Loss(),\n             metrics=[&lt;function fun_rmse at 0x145fb9630&gt;], n_skip=None,\n             opt_func=&lt;function Adam&gt;, cbs=None, hl_depth=1, hl_width=10,\n             act=&lt;class 'torch.nn.modules.activation.Mish'&gt;, bn=False,\n             stateful=False, **kwargs)\n\n\nTCNLearner(dls).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.112962\n0.010284\n0.143394\n00:00"
  },
  {
    "objectID": "02_learner/learner.html#crnn-learner",
    "href": "02_learner/learner.html#crnn-learner",
    "title": "Learner",
    "section": "CRNN Learner",
    "text": "CRNN Learner\n\nsource\n\nCRNNLearner\n\n CRNNLearner (dls, loss_func=L1Loss(), metrics=[&lt;function fun_rmse at\n              0x145fb9630&gt;], n_skip=0, opt_func=&lt;function Adam&gt;, cbs=None,\n              num_ft=10, num_cnn_layers=4, num_rnn_layers=2, hs_cnn=10,\n              hs_rnn=10, hidden_p=0, input_p=0, weight_p=0,\n              rnn_type='gru', stateful=False, **kwargs)\n\n\nCRNNLearner(dls,rnn_type='gru').fit(1,3e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.016305\n0.004414\n0.093948\n00:02"
  },
  {
    "objectID": "02_learner/learner.html#autoregressive-learner",
    "href": "02_learner/learner.html#autoregressive-learner",
    "title": "Learner",
    "section": "Autoregressive Learner",
    "text": "Autoregressive Learner\n\nsource\n\nAR_TCNLearner\n\n AR_TCNLearner (dls, hl_depth=3, alpha=1, beta=1, early_stop=0,\n                metrics=None, n_skip=None, opt_func=&lt;function Adam&gt;,\n                hl_width=10, act=&lt;class\n                'torch.nn.modules.activation.Mish'&gt;, bn=False,\n                stateful=False, **kwargs)\n\n\nAR_TCNLearner(dls).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.024635\n0.021864\n0.210655\n00:00\n\n\n\n\n\n\nsource\n\n\nAR_RNNLearner\n\n AR_RNNLearner (dls, alpha=0, beta=0, early_stop=0, metrics=None,\n                n_skip=0, opt_func=&lt;function Adam&gt;, num_layers=1,\n                hidden_size=100, linear_layers=0, return_state=False,\n                hidden_p=0.0, input_p=0.0, weight_p=0.0, rnn_type='gru',\n                ret_full_hidden=False, stateful=False, normalization='',\n                **kwargs)\n\n\nAR_RNNLearner(dls).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n0\n0.011750\n0.004110\n0.090632\n00:01"
  },
  {
    "objectID": "02_learner/callbacks.html",
    "href": "02_learner/callbacks.html",
    "title": "Learner",
    "section": "",
    "text": "from tsfast.datasets import create_dls_test\ndls = create_dls_test()\nmodel = SimpleRNN(1,1)\nlrn = Learner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.055792\n0.059331\n00:01"
  },
  {
    "objectID": "02_learner/callbacks.html#callbacks",
    "href": "02_learner/callbacks.html#callbacks",
    "title": "Learner",
    "section": "Callbacks",
    "text": "Callbacks\n\nsource\n\nGradientClipping\n\n GradientClipping (clip_val=10)\n\nCallback cutts of the gradient of every minibtch at clip_val\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=GradientClipping(10)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.057712\n0.060895\n00:01\n\n\n\n\n\n\nsource\n\n\nGradientNormPrint\n\n GradientNormPrint (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nCallback prints the norm of the gradient of every minibtch\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=GradientNormPrint()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.058281\n0.060542\n00:01\n\n\n\n\n\nGradient norm: 0.32\nGradient norm: 0.27\nGradient norm: 0.25\nGradient norm: 0.15\nGradient norm: 0.07\nGradient norm: 0.02\nGradient norm: 0.04\nGradient norm: 0.04\nGradient norm: 0.13\nGradient norm: 0.09\nGradient norm: 0.06\nGradient norm: 0.06\n\n\n\nsource\n\n\nGradientBatchFiltering\n\n GradientBatchFiltering (filter_val=10)\n\nCallback skips batches with a gradient norm larger than filter_val\n\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=GradientBatchFiltering(11.0)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.052950\n0.052226\n00:01\n\n\n\n\n\n\nsource\n\n\nWeightClipping\n\n WeightClipping (module, clip_limit=1)\n\nCallback that clips the weights of a given module at clip_limit after every iteration\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=WeightClipping(model,clip_limit=1)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.054952\n0.058467\n00:01\n\n\n\n\n\n\nsource\n\n\nSkipFirstNCallback\n\n SkipFirstNCallback (n_skip=0)\n\nCallback skips first n samples from prediction and target, optionally with_loss\n\nsource\n\n\nSkipNaNCallback\n\n SkipNaNCallback (after_create=None, before_fit=None, before_epoch=None,\n                  before_train=None, before_batch=None, after_pred=None,\n                  after_loss=None, before_backward=None,\n                  after_cancel_backward=None, after_backward=None,\n                  before_step=None, after_cancel_step=None,\n                  after_step=None, after_cancel_batch=None,\n                  after_batch=None, after_cancel_train=None,\n                  after_train=None, before_validate=None,\n                  after_cancel_validate=None, after_validate=None,\n                  after_cancel_epoch=None, after_epoch=None,\n                  after_cancel_fit=None, after_fit=None)\n\nCallback skips minibatches with a NaN loss\n\nsource\n\n\nCancelNaNCallback\n\n CancelNaNCallback (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nCallback cancels trainig minibatches with a NaN loss\n\nsource\n\n\nVarySeqLen\n\n VarySeqLen (min_len=50)\n\nCallback varies sequence length of every mini batch\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=VarySeqLen(10)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.061609\n0.061791\n00:01\n\n\n\n\n\n\nsource\n\n\nsched_lin_p\n\n sched_lin_p (start, end, pos, p=0.75)\n\n\nsource\n\n\nsched_ramp\n\n sched_ramp (start, end, pos, p_left=0.2, p_right=0.6)\n\n\ninit_sz = 200\npred_sz = 600\nwin_sz = init_sz+pred_sz\ntruncate_length = init_sz+10\nplt.figure()\nplt.plot([win_sz-sched_lin_p(win_sz-truncate_length,0,pct) for pct in np.linspace(0,1,100)])\nplt.plot([win_sz-sched_ramp(win_sz-truncate_length,0,pct,0.2,0.6) for pct in np.linspace(0,1,100)])\n\n\n\n\n\n\n\n\n\nsource\n\n\nCB_TruncateSequence\n\n CB_TruncateSequence (truncate_length=50, scheduler=&lt;function sched_ramp&gt;)\n\nCallback varies sequence length of every mini batch\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=CB_TruncateSequence(50,sched_lin_p)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.055499\n0.057812\n00:01\n\n\n\n\n\n\nsource\n\n\nCB_AddLoss\n\n CB_AddLoss (_loss_func, alpha=1.0)\n\nCallback that adds the results of a given loss_function to the mini_batch after the original loss function has been applied\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=CB_AddLoss(nn.MSELoss(),alpha=10)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.629243\n0.059755\n00:01\n\n\n\n\n\n\n# #| export\n# class BatchLossFilter(Callback):\n#     \"\"\" \n#     Callback that selects the hardest samples in every batch representing a percentage of the total loss.\n#     \"\"\"\n#     def __init__(self, loss_perc=1., filter_criterion=nn.HuberLoss(reduction='none'), schedule_func:Optional[callable]=None):\n#         store_attr()  # Stores all passed arguments as class attributes\n\n#     def before_batch(self):\n#         \"\"\"\n#         Selects hardest samples before processing each batch.\n#         \"\"\"\n#         if not self.training: return  # Skip if not in training mode\n#         if self.schedule_func is None: loss_perc = self.loss_perc\n#         else: loss_perc = self.loss_perc * self.schedule_func(self.pct_train)  # Adjust loss_perc if a schedule function is given\n#         if loss_perc == 1.: return  # If loss_perc is 1, all samples are included, no need to filter\n\n#         with torch.no_grad():  # No gradients needed for the filtering operation\n#             losses = self.filter_criterion(self.learn.model(self.x), self.y)  # Compute individual losses\n#             if losses.ndim &gt;= 2: losses = losses.mean(tuple(range(1,losses.ndim)))  # If loss is multi-dimensional, take the mean over all but the first dimension\n#             losses /= losses.sum()  # Normalize losses to make them sum up to 1\n            \n#             idxs = torch.argsort(losses, descending=True)  # Sort indices by loss\n#             cut_idx = max(1, torch.argmax((losses[idxs].cumsum(0) &gt; loss_perc).float()))  # Determine the cut-off index where cumulative sum exceeds loss_perc\n#             idxs = idxs[:cut_idx]  # Select the hardest samples\n\n#             self.learn.xb = tuple(xbi[idxs] for xbi in self.learn.xb)  # Filter the input batch\n#             self.learn.yb = tuple(ybi[idxs] for ybi in self.learn.yb)  # Filter the output batch\n\n\nsource\n\n\nBatchLossFilter\n\n BatchLossFilter (loss_perc=1.0, filter_criterion=HuberLoss(),\n                  schedule_func:Optional[&lt;built-infunctioncallable&gt;]=None)\n\nCallback that selects the hardest samples in every batch representing a percentage of the total loss.\n\n# #| export\n# class BatchLossFilter(Callback):\n#     \"\"\" \n#     Callback that selects the hardest samples in every batch representing a percentage of the total loss.\n#     \"\"\"\n#     order = -9\n#     def __init__(self, loss_perc=1., filter_criterion=nn.HuberLoss(reduction='none'), schedule_func:Optional[callable]=None):\n#         store_attr() \n\n#     def after_pred(self):\n#         \"\"\"\n#         Calculate losses and select hardest samples after model prediction and before loss computation.\n#         \"\"\"\n#         if not self.training: return  # Skip if not in training mode\n#         if self.schedule_func is None: loss_perc = self.loss_perc\n#         else: loss_perc = self.loss_perc * self.schedule_func(self.pct_train)  # Adjust loss_perc if a schedule function is given\n#         if loss_perc == 1.: return  # If loss_perc is 1, all samples are included, no need to filter\n\n#         with torch.no_grad():  # No gradients needed for the filtering operation\n#             losses = self.filter_criterion(self.pred, *self.learn.yb)  # Compute individual losses with model's predictions\n#             if losses.ndim &gt;= 2: losses = losses.mean(tuple(range(1,losses.ndim)))  # If loss is multi-dimensional, take the mean over all but the first dimension\n#             losses /= losses.sum()  # Normalize losses to make them sum up to 1\n            \n#             idxs = torch.argsort(losses, descending=True)  # Sort indices by loss\n#             cut_idx = max(1, torch.argmax((losses[idxs].cumsum(0) &gt; loss_perc).float()))  # Determine the cut-off index where cumulative sum exceeds loss_perc\n#             self.idxs = idxs[:cut_idx]  # Store the indices of the hardest samples\n\n#     def after_loss(self):\n#         \"\"\"\n#         Recalculate the loss with the selected hardest samples.\n#         \"\"\"\n#         if not self.training: return  # Skip if not in training mode\n#         self.learn.loss_grad = self.loss_func(self.pred[self.idxs], *(yb[self.idxs] for yb in self.learn.yb))  # Compute the loss with hardest samples\n\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=BatchLossFilter(loss_perc=0.8)).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.074184\n0.059581\n00:03\n\n\n\n\n\n\nsource\n\n\nTimeSeriesRegularizer\n\n TimeSeriesRegularizer (alpha=0.0, beta=0.0, dim=None, detach=False,\n                        modules=None, every=None, remove_end=True,\n                        is_forward=True, cpu=True,\n                        include_paramless=False, hook=None)\n\nCallback that adds AR and TAR to the loss, calculated by output of provided layer\n\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=[TimeSeriesRegularizer(1.0,1.2,modules=[model.rnn])]).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.053709\n0.054955\n00:01\n\n\n\n\n\n\nsource\n\n\nARInitCB\n\n ARInitCB (after_create=None, before_fit=None, before_epoch=None,\n           before_train=None, before_batch=None, after_pred=None,\n           after_loss=None, before_backward=None,\n           after_cancel_backward=None, after_backward=None,\n           before_step=None, after_cancel_step=None, after_step=None,\n           after_cancel_batch=None, after_batch=None,\n           after_cancel_train=None, after_train=None,\n           before_validate=None, after_cancel_validate=None,\n           after_validate=None, after_cancel_epoch=None, after_epoch=None,\n           after_cancel_fit=None, after_fit=None)\n\nAdds the target variable to the input tuple for autoregression\n\nLearner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.037985\n0.027872\n00:01\n\n\n\n\n\n\nsource\n\n\nplot_grad_flow\n\n plot_grad_flow (named_parameters)\n\nPlots the gradients flowing through different layers in the net during training. Can be used for checking for possible gradient vanishing / exploding problems. modified version of https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/8*\nCall multiple time for transparent overlays, representing the mean gradients*\n\nsource\n\n\nCB_PlotGradient\n\n CB_PlotGradient (n_draws=20)\n\nPlot the Gradient Distribution for every trainable parameter\n\nLearner(dls,model,loss_func=nn.MSELoss(),cbs=CB_PlotGradient()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.021996\n0.018407\n00:01"
  },
  {
    "objectID": "03_models/cnn.html",
    "href": "03_models/cnn.html",
    "title": "Models",
    "section": "",
    "text": "from tsfast.datasets import create_dls_test\ndls = create_dls_test()"
  },
  {
    "objectID": "03_models/cnn.html#cnn",
    "href": "03_models/cnn.html#cnn",
    "title": "Models",
    "section": "CNN",
    "text": "CNN\n\nsource\n\nConv1D\n\n Conv1D (input_size, output_size, kernel_size=3, activation=&lt;class\n         'torch.nn.modules.activation.Mish'&gt;, wn=True, bn=False,\n         stride:Union[int,Tuple[int]]=1,\n         padding:Union[str,int,Tuple[int]]=0,\n         dilation:Union[int,Tuple[int]]=1, groups:int=1, bias:bool=True,\n         padding_mode:str='zeros', device=None, dtype=None, **kwargs)\n\n\nsource\n\n\nCNN\n\n CNN (input_size, output_size, hl_depth=1, hl_width=10, act=&lt;class\n      'torch.nn.modules.activation.Mish'&gt;, bn=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = CNN(1,1,hl_depth=3)\nlrn = Learner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.070932\n0.068136\n00:00"
  },
  {
    "objectID": "03_models/cnn.html#tcn",
    "href": "03_models/cnn.html#tcn",
    "title": "Models",
    "section": "TCN",
    "text": "TCN\n\nsource\n\nCausalConv1d\n\n CausalConv1d (in_channels, out_channels, kernel_size, stride=1,\n               dilation=1, groups=1, bias=True, stateful=False)\n\n*Applies a 1D convolution over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size :math:(N, C_{\\text{in}}, L) and output :math:(N, C_{\\text{out}}, L_{\\text{out}}) can be precisely described as:\n.. math:: (N_i, C_{j}) = (C{j}) + {k = 0}^{C_{in} - 1} (C_{_j}, k) (N_i, k)\nwhere :math:\\star is the valid cross-correlation_ operator, :math:N is a batch size, :math:C denotes a number of channels, :math:L is a length of signal sequence.\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\n\n:attr:stride controls the stride for the cross-correlation, a single number or a one-element tuple.\n:attr:padding controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or a tuple of ints giving the amount of implicit padding applied on both sides.\n:attr:dilation controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link_ has a nice visualization of what :attr:dilation does.\n:attr:groups controls the connections between inputs and outputs. :attr:in_channels and :attr:out_channels must both be divisible by :attr:groups. For example,\n\nAt groups=1, all inputs are convolved to all outputs.\nAt groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\nAt groups= :attr:in_channels, each input channel is convolved with its own set of filters (of size :math:\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}).\n\n\nNote: When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a “depthwise convolution”.\nIn other words, for an input of size :math:`(N, C_{in}, L_{in})`,\na depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n:math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\nNote: In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See :doc:/notes/randomness for more information.\nNote: padding='valid' is the same as no padding. padding='same' pads the input so the output has the shape as the input. However, this mode doesn’t support any stride values other than 1.\nNote: This module supports complex data types i.e. complex32, complex64, complex128.\nArgs: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the convolution kernel_size (int or tuple): Size of the convolving kernel stride (int or tuple, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to both sides of the input. Default: 0 dilation (int or tuple, optional): Spacing between kernel elements. Default: 1 groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional): If True, adds a learnable bias to the output. Default: True padding_mode (str, optional): 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\nShape: - Input: :math:(N, C_{in}, L_{in}) or :math:(C_{in}, L_{in}) - Output: :math:(N, C_{out}, L_{out}) or :math:(C_{out}, L_{out}), where\n  .. math::\n      L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n                \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\nAttributes: weight (Tensor): the learnable weights of the module of shape :math:(\\text{out\\_channels},         \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size}). The values of these weights are sampled from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}} bias (Tensor): the learnable bias of the module of shape (out_channels). If :attr:bias is True, then the values of these weights are sampled from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}\nExamples::\n&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)\n&gt;&gt;&gt; input = torch.randn(20, 16, 50)\n&gt;&gt;&gt; output = m(input)\n.. _cross-correlation: https://en.wikipedia.org/wiki/Cross-correlation\n.. _link: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md*\n\nsource\n\n\nCConv1D\n\n CConv1D (input_size, output_size, kernel_size=2, activation=&lt;class\n          'torch.nn.modules.activation.Mish'&gt;, wn=True, bn=False,\n          stride=1, dilation=1, groups=1, bias=True, stateful=False,\n          **kwargs)\n\n\nsource\n\n\nTCN_Block\n\n TCN_Block (input_size, output_size, num_layers=1, activation=&lt;class\n            'torch.nn.modules.activation.Mish'&gt;, wn=True, bn=False,\n            stateful=False, stride=1, dilation=1, groups=1, bias=True,\n            **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nTCN\n\n TCN (input_size, output_size, hl_depth=1, hl_width=10, act=&lt;class\n      'torch.nn.modules.activation.Mish'&gt;, bn=False, stateful=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = TCN(1,1,hl_depth=3)\nlrn = Learner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.328497\n0.211080\n00:00\n\n\n\n\n\n\nsource\n\n\nSeperateTCN\n\n SeperateTCN (input_list, output_size, hl_depth=1, hl_width=10, act=&lt;class\n              'torch.nn.modules.activation.Mish'&gt;, bn=False,\n              stateful=False, final_layer=3)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "03_models/cnn.html#crnns",
    "href": "03_models/cnn.html#crnns",
    "title": "Models",
    "section": "CRNNs",
    "text": "CRNNs\n\nsource\n\nCRNN\n\n CRNN (input_size, output_size, num_ft=10, num_cnn_layers=4,\n       num_rnn_layers=2, hs_cnn=10, hs_rnn=10, hidden_p=0, input_p=0,\n       weight_p=0, rnn_type='gru', stateful=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = CRNN(1,1,10)\nlrn = Learner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.056243\n0.058065\n00:02\n\n\n\n\n\n\nmodel = CRNN(1,1,10,rnn_type='gru')\nlrn = Learner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.199177\n0.105373\n00:01\n\n\n\n\n\n\nsource\n\n\nSeperateCRNN\n\n SeperateCRNN (input_list, output_size, num_ft=10, num_cnn_layers=4,\n               num_rnn_layers=2, hs_cnn=10, hs_rnn=10, hidden_p=0,\n               input_p=0, weight_p=0, rnn_type='gru', stateful=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "04_prediction/core.html",
    "href": "04_prediction/core.html",
    "title": "Core Elements for Multi Step Ahead Prediction",
    "section": "",
    "text": "from nbdev.config import get_config"
  },
  {
    "objectID": "04_prediction/core.html#create-a-dataloader-for-multi-step-ahead-prediction",
    "href": "04_prediction/core.html#create-a-dataloader-for-multi-step-ahead-prediction",
    "title": "Core Elements for Multi Step Ahead Prediction",
    "section": "Create a dataloader for multi step ahead prediction",
    "text": "Create a dataloader for multi step ahead prediction\nWe create a prediction dataloader by including the system output with a timeshift to the input. The output is shifted one step so that it is always older than the predicted one.\n\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\n\n\nhdf_files = get_hdf_files(f_path)\ninit_sz = 300\nu = ['u']\ny = ['y']\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(u+y,TensorSequencesInput,clm_shift=[0,-1]),\n                        SequenceBlock.from_hdf(y,TensorSequencesOutput,clm_shift=[-1])),\n                 get_items=CreateDict([DfHDFCreateWindows(win_sz=500+1,stp_sz=100,clm='u')]),\n                 splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o))))\ndls_pred = seq.dataloaders(hdf_files,bs=32,shuffle=True)\n\n\ndls_pred.one_batch()[0][0].shape,dls_pred.one_batch()[0][1].shape\n\n(torch.Size([500, 2]), torch.Size([500, 2]))\n\n\n\ndls_pred.show_batch(max_n=1)"
  },
  {
    "objectID": "04_prediction/core.html#create-a-learner-callback-for-prediction",
    "href": "04_prediction/core.html#create-a-learner-callback-for-prediction",
    "title": "Core Elements for Multi Step Ahead Prediction",
    "section": "Create a learner Callback for prediction",
    "text": "Create a learner Callback for prediction\nInstead of creating a specialized dataloader we can instead create a Callback for the learner to add the historic system output data. This creates more flexibility for the learner and requires only one kind of dataloader per dataset.\n\nsource\n\nPredictionCallback\n\n PredictionCallback (t_offset=1, std=None, mean=None)\n\nConcatenates the optionally normalized system output to the input data for autoregression, assumes 1-tuple as input\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt_offset\nint\n1\nthe number of steps output is shifted in the past, shortens the sequence length by that number\n\n\nstd\nNoneType\nNone\nstandard deviation of the output tensor\n\n\nmean\nNoneType\nNone\nmean of the output tensor\n\n\n\nWe create a dataloader without system output as input and compare the signals after the callback.\n\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(u,TensorSequencesInput),\n                        SequenceBlock.from_hdf(y,TensorSequencesOutput)),\n                 get_items=CreateDict([DfHDFCreateWindows(win_sz=500,stp_sz=100,clm='u')]),\n                 splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o))))\ndls = seq.dataloaders(hdf_files,bs=32)\n\nEvaluate that a simulation model works with the dataset\n\nmodel = SimpleRNN(1,1)\nLearner(dls,model,loss_func=nn.MSELoss()).fit(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/52 00:00&lt;?]\n    \n    \n\n\nand a prediction model which expects a 2d input does not work without the callback\n\nmodel = SimpleRNN(2,1)\ntest_fail(lambda: Learner(dls,model,loss_func=nn.MSELoss()).fit(1))\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/52 00:00&lt;?]\n    \n    \n\n\n\n# dls_pred.show_batch(max_n=1)\n# dls.show_batch(max_n=1)\n\n\nmodel = SimpleRNN(2,1)\npred_callback = PredictionCallback(1)\npred_callback.init_normalize(dls.one_batch())\nlrn = Learner(dls,model,loss_func=nn.MSELoss(),cbs=pred_callback)\nlrn.fit(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/52 00:00&lt;?]"
  },
  {
    "objectID": "04_prediction/ar.html",
    "href": "04_prediction/ar.html",
    "title": "Dual RNN Models",
    "section": "",
    "text": "from nbdev.config import get_config\nproject_root = get_config().config_file.parent\nf_path = project_root / 'test_data/WienerHammerstein'\nhdf_files = get_hdf_files(f_path)\ninit_sz = 300\nu = ['u']\ny = ['y']\nseq = DataBlock(blocks=(SequenceBlock.from_hdf(u+y,TensorSequencesInput,clm_shift=[0,-1]),\n                        SequenceBlock.from_hdf(y,TensorSequencesOutput,clm_shift=[-1])),\n                 get_items=CreateDict([DfHDFCreateWindows(win_sz=500+1,stp_sz=100,clm='u')]),\n                 splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o))))\ndb = seq.dataloaders(hdf_files,bs=32,dl_type=TfmdDL)\ndb.one_batch()[0][0].shape,db.one_batch()[0][1].shape\n\n(torch.Size([500, 2]), torch.Size([500, 2]))\ndb.show_batch(max_n=1)"
  },
  {
    "objectID": "04_prediction/ar.html#autoregressive-prognosis",
    "href": "04_prediction/ar.html#autoregressive-prognosis",
    "title": "Dual RNN Models",
    "section": "Autoregressive Prognosis",
    "text": "Autoregressive Prognosis\n\nSame RNN used for State Estimation and Prediction\n\nsource\n\n\nARProg\n\n ARProg (n_u, n_x, n_y, init_sz, hidden_p=0.0, input_p=0.0, weight_p=0.0,\n         rnn_type='gru', ret_full_hidden=False, stateful=False,\n         normalization='', **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nxb,yb = db.one_batch()\nmodel = ARProg(len(u),0,len(y),init_sz=init_sz,num_layers=1,hidden_size=100).to(xb.device)\nmodel(xb).shape\n\ntorch.Size([32, 500, 1])\n\n\n\nfrom fastai.callback.training import ShortEpochCallback\n\n\nmodel = ARProg(len(u),0,len(y),init_sz=init_sz,num_layers=1,hidden_size=50)\nlrn = Learner(db,model,loss_func=SkipNLoss(mse,init_sz))\nlrn.add_cb(TbpttResetCB())\nlrn.add_cb(ShortEpochCallback())\n#lrn.fit(1,lr=3e-3)\nlrn.fit_flat_cos(1,3e-3,pct_start=0.2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n00:01"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tsfast",
    "section": "",
    "text": "A deep learning library for time series analysis and system identification built on top of PyTorch & fastai.\n\ntsfast is an open-source deep learning package that focuses on system identification and time series analysis tasks. Built on the foundations of PyTorch and fastai, it provides efficient implementations of various deep learning models and utilities.\n\n\n\nYou can install the latest stable version from pip using:\npip install tsfast\nFor development installation:\ngit clone https://github.com/daniel-om-weber/tsfast\ncd tsfast\npip install -e '.[dev]'\n\n\n\nHere is a quick example using a test dataloader. It demonstrates loading and visualizing data, training a RNN, and visualizing the results.\n\nfrom tsfast.basics import *\ndls = create_dls_test()\ndls.show_batch(max_n=1)\n\n\n\n\n\n\n\n\n\nlrn = RNNLearner(dls)\nlrn.fit_flat_cos(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/12 00:00&lt;?]\n    \n    \n\n\n\nlrn.show_results(max_n=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor detailed documentation, visit our documentation site.\nKey documentation sections: - Core Functions - Data Processing - Models - Learner API - Hyperparameter Optimization\n\n\n\n\nPython ≥ 3.9\nfastai\nPyTorch\nsysbench_loader\nmatplotlib\nray[tune] (for hyperparameter optimization)\n\n\n\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n\n\nThis project is licensed under the Apache 2.0 License.\n\n\n\nIf you use tsfast in your research, please cite:\n@Misc{tsfast,\nauthor = {Daniel O.M. Weber},\ntitle = {tsfast - A deep learning library for time series analysis and system identification},\nhowpublished = {Github},\nyear = {2024},\nurl = {https://github.com/daniel-om-weber/tsfast}\n}",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "tsfast",
    "section": "",
    "text": "A deep learning library for time series analysis and system identification built on top of PyTorch & fastai.\n\ntsfast is an open-source deep learning package that focuses on system identification and time series analysis tasks. Built on the foundations of PyTorch and fastai, it provides efficient implementations of various deep learning models and utilities.",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "tsfast",
    "section": "",
    "text": "You can install the latest stable version from pip using:\npip install tsfast\nFor development installation:\ngit clone https://github.com/daniel-om-weber/tsfast\ncd tsfast\npip install -e '.[dev]'",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "tsfast",
    "section": "",
    "text": "Here is a quick example using a test dataloader. It demonstrates loading and visualizing data, training a RNN, and visualizing the results.\n\nfrom tsfast.basics import *\ndls = create_dls_test()\ndls.show_batch(max_n=1)\n\n\n\n\n\n\n\n\n\nlrn = RNNLearner(dls)\nlrn.fit_flat_cos(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nfun_rmse\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/12 00:00&lt;?]\n    \n    \n\n\n\nlrn.show_results(max_n=1)",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "tsfast",
    "section": "",
    "text": "For detailed documentation, visit our documentation site.\nKey documentation sections: - Core Functions - Data Processing - Models - Learner API - Hyperparameter Optimization",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "tsfast",
    "section": "",
    "text": "Python ≥ 3.9\nfastai\nPyTorch\nsysbench_loader\nmatplotlib\nray[tune] (for hyperparameter optimization)",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "tsfast",
    "section": "",
    "text": "Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "tsfast",
    "section": "",
    "text": "This project is licensed under the Apache 2.0 License.",
    "crumbs": [
      "tsfast"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "tsfast",
    "section": "",
    "text": "If you use tsfast in your research, please cite:\n@Misc{tsfast,\nauthor = {Daniel O.M. Weber},\ntitle = {tsfast - A deep learning library for time series analysis and system identification},\nhowpublished = {Github},\nyear = {2024},\nurl = {https://github.com/daniel-om-weber/tsfast}\n}",
    "crumbs": [
      "tsfast"
    ]
  }
]