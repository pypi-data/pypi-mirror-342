# coding: utf-8

"""
    Sureel API

    API for the Sureel network.

    The version of the OpenAPI document: 
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing_extensions import Annotated
from openapi_client.models.control_net_request_control_net import ControlNetRequestControlNet
from openapi_client.models.style_image import StyleImage
from typing import Optional, Set
from typing_extensions import Self

class AvatarImageBlendingRequest(BaseModel):
    """
    AvatarImageBlendingRequest
    """ # noqa: E501
    prompt: Optional[StrictStr] = Field(default=None, description="Text input to condition the image synthesis.     More details improve the resulting image quality, although repeating concepts and overly long prompts should be avoided.")
    content_image_path: StrictStr = Field(description="The `contentImagePath` can either be an image URL or the image path returned from the `/ldm/upload-images` or `/artist/post-upload-urls` endpoint.         Supported formats include JPG and PNG.", alias="contentImagePath")
    style_images: Annotated[List[StyleImage], Field(min_length=1, max_length=1)] = Field(alias="styleImages")
    control_net: Optional[ControlNetRequestControlNet] = Field(default=None, alias="controlNet")
    negative_prompt: Optional[StrictStr] = Field(default=None, description="A list of negative attributes or undesired features that describes what the desired image should **not** look like.", alias="negativePrompt")
    apply_watermark: Optional[StrictBool] = Field(default=None, description="Flag whether to apply a watermark to the images or not.     The watermark can be either the Sureel watermark or a custom watermark provided through the `customWatermark` parameter.", alias="applyWatermark")
    custom_watermark: Optional[StrictStr] = Field(default=None, description="Define a custom watermark image to be applied to the images.             It should be square and must have an alpha channel.             The watermark should be oriented towards the lower left corner of the watermark image.             The `customWatermark` can either be an image URL or the image path returned from the `/ldm/upload-images` or `/artist/post-upload-urls` endpoint.             Supported formats include PNG.", alias="customWatermark")
    style_preset: Optional[StrictStr] = Field(default=None, description="Preset styles for image generation that describe the desired style of the image,     such as “painting”, “photography”, “anime” or “digital art”.     It must be a comma-separated list of styles.", alias="stylePreset")
    inference_steps: Optional[Annotated[int, Field(le=100, strict=True, ge=10)]] = Field(default=None, description="Number of diffusion steps that iteratively improve the images generated by the model.", alias="inferenceSteps")
    cfg: Optional[Union[Annotated[float, Field(le=15, strict=True, ge=0)], Annotated[int, Field(le=15, strict=True, ge=0)]]] = Field(default=None, description="Classifier-free guidance: This scale determines how strictly the diffusion process adheres to the prompt.             Higher values keep the image closer to the prompt.")
    seed: Optional[Annotated[int, Field(le=2147483647, strict=True, ge=-2147483648)]] = Field(default=None, description="Seed to control randomness in the generation process and enable reproducibility.             If not specified, the generation process is random.             If the seed is fixed to the same number, the generation will always yield the same result.")
    generate_attribution: Optional[StrictBool] = Field(default=None, description="Flag whether to enable the attribution calculation. Attribution describes the influence of training data on content generated using AI.     When training an AI model with different collections of data, each collection has a certain amount of influence on the resulting model.     When generating new content with the resulting model, the influence each collection has on the new content is referred to as the attribution of the new content.", alias="generateAttribution")
    generate_provenance: Optional[StrictBool] = Field(default=None, description="Flag whether to enable the provenance calculation. Style image required. Provenance describes the entire influence history of a piece of content generated using AI.     It can be seen as the content's attribution over time.     Whenever any data is used to generate new content using AI, the provenance of that new content is extended to include the influence of that data.     In  summary, the provenance of a piece of content is obtained by computing the full attribution to all training and input data each time AI is used to generate or alter that piece of content.", alias="generateProvenance")
    batch_size: Optional[Annotated[int, Field(le=12, strict=True, ge=1)]] = Field(default=None, description="Number of images that are generated in a single request.", alias="batchSize")
    generate_preview: Optional[StrictBool] = Field(default=None, description="Flag whether to generate 256px and 512px preview images for lower resolution previews in a front-end.", alias="generatePreview")
    webhook: Optional[StrictStr] = Field(default=None, description="Webhook URL where you receive notifications regarding completed requests.")
    __properties: ClassVar[List[str]] = ["prompt", "contentImagePath", "styleImages", "controlNet", "negativePrompt", "applyWatermark", "customWatermark", "stylePreset", "inferenceSteps", "cfg", "seed", "generateAttribution", "generateProvenance", "batchSize", "generatePreview", "webhook"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of AvatarImageBlendingRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in style_images (list)
        _items = []
        if self.style_images:
            for _item_style_images in self.style_images:
                if _item_style_images:
                    _items.append(_item_style_images.to_dict())
            _dict['styleImages'] = _items
        # override the default output from pydantic by calling `to_dict()` of control_net
        if self.control_net:
            _dict['controlNet'] = self.control_net.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of AvatarImageBlendingRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "prompt": obj.get("prompt"),
            "contentImagePath": obj.get("contentImagePath"),
            "styleImages": [StyleImage.from_dict(_item) for _item in obj["styleImages"]] if obj.get("styleImages") is not None else None,
            "controlNet": ControlNetRequestControlNet.from_dict(obj["controlNet"]) if obj.get("controlNet") is not None else None,
            "negativePrompt": obj.get("negativePrompt"),
            "applyWatermark": obj.get("applyWatermark"),
            "customWatermark": obj.get("customWatermark"),
            "stylePreset": obj.get("stylePreset"),
            "inferenceSteps": obj.get("inferenceSteps"),
            "cfg": obj.get("cfg"),
            "seed": obj.get("seed"),
            "generateAttribution": obj.get("generateAttribution"),
            "generateProvenance": obj.get("generateProvenance"),
            "batchSize": obj.get("batchSize"),
            "generatePreview": obj.get("generatePreview"),
            "webhook": obj.get("webhook")
        })
        return _obj


