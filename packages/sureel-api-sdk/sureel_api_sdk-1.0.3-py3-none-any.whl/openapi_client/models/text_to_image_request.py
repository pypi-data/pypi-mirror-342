# coding: utf-8

"""
    Sureel API

    API for the Sureel network.

    The version of the OpenAPI document: 
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing_extensions import Annotated
from openapi_client.models.resolution import Resolution
from openapi_client.models.text_to_image_request_upscale import TextToImageRequestUpscale
from openapi_client.models.v1_ai_audio_ldm_text_to_artist_audio_post_request_artist_preset_inner import V1AiAudioLdmTextToArtistAudioPostRequestArtistPresetInner
from typing import Optional, Set
from typing_extensions import Self

class TextToImageRequest(BaseModel):
    """
    TextToImageRequest
    """ # noqa: E501
    prompt: StrictStr = Field(description="Text input to condition the image synthesis.     More details improve the resulting image quality, although repeating concepts and overly long prompts should be avoided.")
    resolution: Resolution
    negative_prompt: Optional[StrictStr] = Field(default=None, description="A list of negative attributes or undesired features that describes what the desired image should **not** look like.", alias="negativePrompt")
    apply_watermark: Optional[StrictBool] = Field(default=None, description="Flag whether to apply a watermark to the images or not.     The watermark can be either the Sureel watermark or a custom watermark provided through the `customWatermark` parameter.", alias="applyWatermark")
    custom_watermark: Optional[StrictStr] = Field(default=None, description="Define a custom watermark image to be applied to the images.             It should be square and must have an alpha channel.             The watermark should be oriented towards the lower left corner of the watermark image.             The `customWatermark` can either be an image URL or the image path returned from the `/ldm/upload-images` or `/artist/post-upload-urls` endpoint.             Supported formats include PNG.", alias="customWatermark")
    style_preset: Optional[StrictStr] = Field(default=None, description="Preset styles for image generation that describe the desired style of the image,     such as “painting”, “photography”, “anime” or “digital art”.     It must be a comma-separated list of styles.", alias="stylePreset")
    artist_preset: Optional[Annotated[List[V1AiAudioLdmTextToArtistAudioPostRequestArtistPresetInner], Field(max_length=2)]] = Field(default=None, description="List of artist collections who should contribute to the generated data.", alias="artistPreset")
    upscale: Optional[TextToImageRequestUpscale] = None
    inference_steps: Optional[Annotated[int, Field(le=100, strict=True, ge=10)]] = Field(default=None, description="Number of diffusion steps that iteratively improve the images generated by the model.", alias="inferenceSteps")
    cfg: Optional[Union[Annotated[float, Field(le=15, strict=True, ge=0)], Annotated[int, Field(le=15, strict=True, ge=0)]]] = Field(default=None, description="Classifier-free guidance: This scale determines how strictly the diffusion process adheres to the prompt.             Higher values keep the image closer to the prompt.")
    seed: Optional[Annotated[int, Field(le=2147483647, strict=True, ge=-2147483648)]] = Field(default=None, description="Seed to control randomness in the generation process and enable reproducibility.             If not specified, the generation process is random.             If the seed is fixed to the same number, the generation will always yield the same result.")
    generate_attribution: Optional[StrictBool] = Field(default=None, description="Flag whether to enable the attribution calculation. Attribution describes the influence of training data on content generated using AI.     When training an AI model with different collections of data, each collection has a certain amount of influence on the resulting model.     When generating new content with the resulting model, the influence each collection has on the new content is referred to as the attribution of the new content.", alias="generateAttribution")
    batch_size: Optional[Annotated[int, Field(le=12, strict=True, ge=1)]] = Field(default=None, description="Number of images that are generated in a single request.", alias="batchSize")
    remove_bg: Optional[StrictBool] = Field(default=None, description="Flag whether to remove the semantic background of the image.", alias="removeBg")
    generate_preview: Optional[StrictBool] = Field(default=None, description="Flag whether to generate 256px and 512px preview images for lower resolution previews in a front-end.", alias="generatePreview")
    webhook: Optional[StrictStr] = Field(default=None, description="Webhook URL where you receive notifications regarding completed requests.")
    __properties: ClassVar[List[str]] = ["prompt", "resolution", "negativePrompt", "applyWatermark", "customWatermark", "stylePreset", "artistPreset", "upscale", "inferenceSteps", "cfg", "seed", "generateAttribution", "batchSize", "removeBg", "generatePreview", "webhook"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of TextToImageRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of resolution
        if self.resolution:
            _dict['resolution'] = self.resolution.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in artist_preset (list)
        _items = []
        if self.artist_preset:
            for _item_artist_preset in self.artist_preset:
                if _item_artist_preset:
                    _items.append(_item_artist_preset.to_dict())
            _dict['artistPreset'] = _items
        # override the default output from pydantic by calling `to_dict()` of upscale
        if self.upscale:
            _dict['upscale'] = self.upscale.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of TextToImageRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "prompt": obj.get("prompt"),
            "resolution": Resolution.from_dict(obj["resolution"]) if obj.get("resolution") is not None else None,
            "negativePrompt": obj.get("negativePrompt"),
            "applyWatermark": obj.get("applyWatermark"),
            "customWatermark": obj.get("customWatermark"),
            "stylePreset": obj.get("stylePreset"),
            "artistPreset": [V1AiAudioLdmTextToArtistAudioPostRequestArtistPresetInner.from_dict(_item) for _item in obj["artistPreset"]] if obj.get("artistPreset") is not None else None,
            "upscale": TextToImageRequestUpscale.from_dict(obj["upscale"]) if obj.get("upscale") is not None else None,
            "inferenceSteps": obj.get("inferenceSteps"),
            "cfg": obj.get("cfg"),
            "seed": obj.get("seed"),
            "generateAttribution": obj.get("generateAttribution"),
            "batchSize": obj.get("batchSize"),
            "removeBg": obj.get("removeBg"),
            "generatePreview": obj.get("generatePreview"),
            "webhook": obj.get("webhook")
        })
        return _obj


