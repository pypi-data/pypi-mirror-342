"""
Reusable Python utilities.
"""
from __future__ import annotations

import atexit
import csv
import ctypes
import inspect
import json
import locale
import logging
import logging.config
import os
import re
import socket
import ssl
import struct
import subprocess
import sys
import threading
import unicodedata
from argparse import ArgumentParser, RawTextHelpFormatter, _SubParsersAction
from contextlib import (AbstractContextManager, ExitStack, contextmanager,
                        nullcontext)
from datetime import date, datetime, time, timedelta, timezone, tzinfo
from decimal import Decimal
from enum import Enum, Flag
from http.client import HTTPResponse
from importlib import import_module
from io import BytesIO, IOBase, StringIO, UnsupportedOperation
from ipaddress import AddressValueError, IPv4Address, IPv6Address, ip_address
from pathlib import Path
from pkgutil import iter_modules
from pprint import pprint
from queue import Queue
from shutil import which
from signal import Signals
from sqlite3 import Connection
from textwrap import dedent
from threading import Thread
from time import sleep as sleep_s
from time import time as time_s
from time import time_ns
from traceback import format_exception
from types import GeneratorType, ModuleType, TracebackType
from typing import (TYPE_CHECKING, Any, Callable, Generic, Iterable, Iterator,
                    MutableMapping, Sequence, TextIO, TypeVar, Union, overload)
from urllib.error import HTTPError, URLError
from urllib.parse import ParseResult, quote, quote_plus, unquote, urlencode, urlparse, urlunparse
from urllib.request import Request, urlopen
from uuid import UUID
from warnings import catch_warnings

import _csv

# Literal: was introduced in Python 3.8
try:
    from typing import Literal, Protocol, get_args, get_origin
except ImportError:
    from typing_extensions import Literal, Protocol, get_args, get_origin

if TYPE_CHECKING:
    from zut.db import Db

__prog__ = 'zut'

# version: generated by setuptools_scm during build
try:
    from zut._version import __version__, __version_tuple__  # type: ignore
except ModuleNotFoundError:
    __version__ = None
    __version_tuple__ = None


T = TypeVar('T')


#region Backports/polyfills

if sys.version_info >= (3, 8):
    from functools import cached_property  # pylint: disable=no-name-in-module
else:
    # Backport `cached_property`
    from threading import RLock
    from typing import Any
    from typing import Callable
    from typing import Optional
    from typing import Type
    from typing import TypeVar

    _NOT_FOUND = object()
    _T = TypeVar("_T")
    _S = TypeVar("_S")

    # noinspection PyPep8Naming
    class cached_property:  # NOSONAR  # pylint: disable=invalid-name  # noqa: N801
        """Cached property implementation.

        Transform a method of a class into a property whose value is computed once
        and then cached as a normal attribute for the life of the instance.
        Similar to property(), with the addition of caching.
        Useful for expensive computed properties of instances
        that are otherwise effectively immutable.
        """

        def __init__(self, func: Callable[[Any], _T]) -> None:
            """Cached property implementation."""
            self.func = func
            self.attrname: Optional[str] = None
            self.__doc__ = func.__doc__
            self.lock = RLock()

        def __set_name__(self, owner: Type[Any], name: str) -> None:
            """Assign attribute name and owner."""
            if self.attrname is None:
                self.attrname = name
            elif name != self.attrname:
                raise TypeError(
                    "Cannot assign the same cached_property to two different names "
                    f"({self.attrname!r} and {name!r})."
                )

        def __get__(self, instance: Optional[_S], owner: Optional[Type[Any]] = None) -> Any:
            """Property-like getter implementation.

            :return: property instance if requested on class or value/cached value if requested on instance.
            :rtype: Union[cached_property[_T], _T]
            :raises TypeError: call without calling __set_name__ or no '__dict__' attribute
            """
            if instance is None:
                return self
            if self.attrname is None:
                raise TypeError("Cannot use cached_property instance without calling __set_name__ on it.")
            try:
                cache = instance.__dict__
            except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)
                msg = (
                    f"No '__dict__' attribute on {type(instance).__name__!r} "
                    f"instance to cache {self.attrname!r} property."
                )
                raise TypeError(msg) from None
            val = cache.get(self.attrname, _NOT_FOUND)
            if val is _NOT_FOUND:
                with self.lock:
                    # check if another thread filled cache while we awaited lock
                    val = cache.get(self.attrname, _NOT_FOUND)
                    if val is _NOT_FOUND:
                        val = self.func(instance)
                        try:
                            cache[self.attrname] = val
                        except TypeError:
                            msg = (
                                f"The '__dict__' attribute on {type(instance).__name__!r} instance "
                                f"does not support item assignment for caching {self.attrname!r} property."
                            )
                            raise TypeError(msg) from None
            return val


try:
    from django.db.models import TextChoices
except ImportError:
    TextChoices = Enum

#endregion


#region Text

def slugify(value: str, *, separator: str|None = '-', keep: str|None = '_', as_separator: str|None = None, strip_separator: bool = True, strip_keep: bool = True, if_none: str|None = 'none', additional_conversions: dict[str,str]|None = None) -> str:
    """ 
    Generate a slug.

    Difference between `keep` and `as_separator`
    - `keep`: these characters are kept as is in the resulting slug
    - `as_separator`: these characters are transformed to a separator before the operation

    Identical to `django.utils.text.slugify` if no options are given.
    """
    if value is None:
        return if_none
    
    separator = separator if separator is not None else ''
    keep = keep if keep is not None else ''

    if as_separator:
        value = re.sub(f"[{re.escape(as_separator)}]", separator, value)

    # Normalize the string: replace diacritics by standard characters, lower the string, etc
    value = str(value)
    if additional_conversions:
        converted_value = ''
        for char in value:
            converted_char = additional_conversions.get(char)
            if converted_char is None:
                converted_char = char
            converted_value += converted_char
        value = converted_value
    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
    value = value.lower()

    # Remove special characters
    remove_sequence = r'^a-zA-Z0-9\s' + re.escape(separator) + re.escape(keep)
    value = re.sub(f"[{remove_sequence}]", "", value)

    # Replace spaces and successive separators by a single separator
    replace_sequence = r'\s' + re.escape(separator)
    value = re.sub(f"[{replace_sequence}]+", separator, value)
    
    # Strips separator and kept characters
    strip_chars = (separator if strip_separator else '') + (keep if strip_keep else '')
    value = value.strip(strip_chars)

    return value


def slugify_snake(value: str, separator: str|None = '_', if_none: str|None = 'none', additional_conversions: dict[str,str]|None = None) -> str:
    """
    CamèlCase => camel_case
    """
    if value is None:
        return if_none
    
    separator = separator if separator is not None else ''
    
    # Normalize the string: replace diacritics by standard characters, etc
    # NOTE: don't lower the string
    value = str(value)
    if additional_conversions:
        converted_value = ''
        for char in value:
            converted_char = additional_conversions.get(char)
            if converted_char is None:
                converted_char = char
            converted_value += converted_char
        value = converted_value
    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
    
    value = re.sub(r"[^\w\s-]", "", value)
    value = re.sub(r"[-_\s]+", separator, value).strip(separator)
    value = re.sub(r'(.)([A-Z][a-z]+)', f'\\1{separator}\\2', value)
    return re.sub(r'([a-z0-9])([A-Z])', f'\\1{separator}\\2', value).lower()


def slugen(value: str, separator: str|None = '-', keep: str|None = None) -> str:
    """
    Similar as `slugify` except than some defaults are changed compared to Django's version and some additional letters are handled.
    Closer to Postgres' unaccent result.
    """
    #Input examples that give different results than `slugify`: `AN_INPUT`, `Ørland`
    if value is None:
        return value
    
    value = value.replace('_', '-')
    
    return slugify(value, separator=separator, keep=keep, if_none=None, additional_conversions={ # non-ASCII letters that are not separated by "NFKD" normalization
        "œ": "oe",
        "Œ": "OE",
        "ø": "o",
        "Ø": "O",
        "æ": "ae",
        "Æ": "AE",
        "ß": "ss",
        "ẞ": "SS",
        "đ": "d",
        "Đ": "D",
        "ð": "d",
        "Ð": "D",
        "þ": "th",
        "Þ": "th",
        "ł": "l",
        "Ł": "L",
        "´": "", # in order to have same result as for "'"
    })


def skip_utf8_bom(fp: TextIO|BytesIO, encoding = 'utf-8'):
    """
    Skip UTF8 byte order mark, if any.
    - `fp`: opened file pointer.
    - `encoding`: if given, do nothing unless encoding is utf-8 or alike.
    """
    if encoding and not encoding in {'utf8', 'utf-8', 'utf-8-sig'}:
        return False

    try:
        data = fp.read(1)
    except UnsupportedOperation: # e.g. empty file
        return False
    
    if isinstance(data, str): # text mode
        if len(data) >= 1 and data[0] == UTF8_BOM:
            return True
        
    elif isinstance(data, bytes): # binary mode
        if len(data) >= 1 and data[0] == UTF8_BOM_BINARY[0]:
            data += fp.read(2)
            if data[0:3] == UTF8_BOM_BINARY:
                return True
    
    fp.seek(0)
    return False


def fix_utf8_surrogateescape(text: str, potential_encoding = 'cp1252') -> tuple[str,bool]:
    """ Fix potential encoding issues for files open with `file.open('r', encoding='utf-8', errors='surrogateescape')`. """
    fixed = False
    for c in text:
        c_ord = ord(c)
        if c_ord >= SURROGATE_MIN_ORD and c_ord <= SURROGATE_MAX_ORD:
            fixed = True
            break

    if not fixed:
        return text, False
    
    return text.encode('utf-8', 'surrogateescape').decode(potential_encoding, 'replace'), fixed


def fix_restricted_xml_control_characters(text: str, replace = '?'):
    """
    Replace invalid XML control characters. See: https://www.w3.org/TR/xml11/#charsets.
    """
    if text is None:
        return None
    
    replaced_line = ''
    for c in text:
        n = ord(c)
        if (n >= 0x01 and n <= 0x08) or (n >= 0x0B and n <= 0x0C) or (n >= 0x0E and n <= 0x1F) or (n >= 0x7F and n <= 0x84) or (n >= 0x86 and n <= 0x9F):
            c = replace
        replaced_line += c
    return replaced_line


UTF8_BOM = '\ufeff'
UTF8_BOM_BINARY = UTF8_BOM.encode('utf-8')

SURROGATE_MIN_ORD = ord('\uDC80')
SURROGATE_MAX_ORD = ord('\uDCFF')


class Filter:
    def __init__(self, spec: str|re.Pattern, *, normalize: bool = False):
        self.normalize = normalize

        if isinstance(spec, re.Pattern):
            self.spec = spec

        elif isinstance(spec, str) and spec.startswith('^'):
            m = re.match(r'^(.*\$)(A|I|L|U|M|S|X)+$', spec, re.IGNORECASE)
            if m:
                pattern = m[1]
                flags = re.NOFLAG
                for letter in m[2]:
                    flags |= re.RegexFlag[letter.upper()]
            else:
                pattern = spec
                flags = re.NOFLAG

            self.spec = re.compile(pattern, flags)

        elif isinstance(spec, str):
            if self.normalize:
                spec = self.normalize_spec(spec)

            if '*' in spec:
                name_parts = spec.split('*')
                pattern_parts = [re.escape(name_part) for name_part in name_parts]
                pattern = r'^' + r'.*'.join(pattern_parts) + r'$'
                self.spec = re.compile(pattern)
            else:
                self.spec = spec

        else:
            raise TypeError(f"Filter spec must be a string or regex pattern, got {type(spec).__name__}")
       

    def __repr__(self) -> str:
        return self.spec.pattern if isinstance(self.spec, re.Pattern) else self.spec


    def matches(self, value: str, is_normalized: bool = False):
        if value is None:
            value = ""
        elif not isinstance(value, str):
            value = str(value)

        if self.normalize and not is_normalized:
            value = self.normalize_value(value)

        if isinstance(self.spec, re.Pattern):
            if self.spec.match(value):
                return True
            
        elif self.spec == value:
            return True


    @classmethod
    def normalize_spec(cls, spec: str):
        return slugify(spec, separator=None, keep='*', strip_keep=False, if_none=None)
    
    
    @classmethod
    def normalize_value(cls, value: str):
        return slugify(value, separator=None, keep=None, if_none=None)


class Filters:
    def __init__(self, specs: list[str|re.Pattern]|str|re.Pattern, *, normalize: bool = False):
        self.filters: list[Filter] = []

        if specs:
            if isinstance(specs, (str,re.Pattern)):
                specs = [specs]

            for spec in specs:
                self.filters.append(Filter(spec, normalize=normalize))


    def __len__(self):
        return len(self.filters)


    def matches(self, value: str, if_no_filter: bool = False):
        if not self.filters:
            return if_no_filter
        
        if value is None:
            value = ""
        elif not isinstance(value, str):
            value = str(value)
        
        normalized_value = None    

        for str_filter in self.filters:
            if str_filter.normalize:
                if normalized_value is None:
                    normalized_value = Filter.normalize_value(value)
                if str_filter.matches(normalized_value, is_normalized=True):
                    return True
            else:
                if str_filter.matches(value):
                    return True
                
        return False
    

_to_erase: list[int] = []

def write_live(text: str, newline=False):
    """
    Write text to stdout, keeping track of what was written, so that it can be erased next time.

    Text lines are stripped to terminal column length.
    """
    erase_live()    
    columns, _ = os.get_terminal_size()

    lines = text.split('\n')
    for i, line in enumerate(lines):
        line = line.rstrip()
        
        nb_chars = len(line)
        if nb_chars > columns:
            line = line[:columns-1] + '…'
            nb_chars = columns

        _to_erase.insert(0, nb_chars)

        sys.stdout.write(line)
        if newline or i < len(lines) - 1:
            sys.stdout.write('\n')

    if newline:
        _to_erase.insert(0, 0)
    
    sys.stdout.flush()


def erase_live():
    """
    Erase text written using :func:`write_live`.

    Text lines are stripped to terminal column length.
    """
    if not _to_erase:
        return
    
    for i, nb_chars in enumerate(_to_erase):
        if i == 0:
            sys.stdout.write('\r') # move to beginning of line
        else:
            sys.stdout.write('\033[F') # move to beginning of previous line
        sys.stdout.write(' ' * nb_chars)
    sys.stdout.write('\r')

    _to_erase.clear()

#endregion


#region Numbers

def human_bytes(value: int, *, unit: str = 'iB', divider: int = 1024, decimals: int = 1, max_multiple: str|None = None) -> str:
    """
    Get a human-readable representation of a number of bytes.
    
    :param max_multiple: may be `K`, `M`, `G` or `T`.
    """
    return human_number(value, unit=unit, divider=divider, decimals=decimals, max_multiple=max_multiple)


def human_number(value: int, *, unit: str = '', divider: int = 1000, decimals: int = 1, max_multiple: str|None = None) -> str:
    """
    Get a human-readable representation of a number.

    :param max_multiple: may be `K`, `M`, `G` or `T`.
    """
    if value is None:
        return None

    suffixes = []

    # Append non-multiple suffix (bytes)
    # (if unit is 'iB' we dont display the 'i' as it makes more sens to display "123 B" than "123 iB")
    if unit:
        suffixes.append(' ' + (unit[1:] if len(unit) >= 2 and unit[0] == 'i' else unit))
    else:
        suffixes.append('')

    # Append multiple suffixes
    for multiple in ['K', 'M', 'G', 'T']:
        suffixes.append(f' {multiple}{unit}')
        if max_multiple and max_multiple.upper() == multiple:
            break

    i = 0
    suffix = suffixes[i]
    divided_value = value

    while divided_value > 1000 and i < len(suffixes) - 1:
        divided_value /= divider
        i += 1
        suffix = suffixes[i]

    # Format value
    formatted_value = ('{0:,.'+('0' if i == 0 else str(decimals))+'f}').format(divided_value)
    
    # Display formatted value with suffix
    return f'{formatted_value}{suffix}'

#endregion


#region Convert

@overload
def convert(value: Any, to: type[T], *, nullval = None, if_none = None, accept_localized = True) -> T:
    ...

@overload
def convert(value: Any, to: Callable|None, *, nullval = None, if_none = None, accept_localized = True):
    ...

def convert(value: Any, to: type[T]|Callable|None, *, nullval = None, if_none = None, accept_localized = True):    
    if value == nullval:
        return if_none
    
    if to is None:
        return value
    
    # GenericAlias: was introducted in Python 3.9
    item_type: type|None = None
    try:
        from types import GenericAlias
        if isinstance(to, GenericAlias):
            type_args = get_args(to)
            to = get_origin(to)
            if to == list or to == tuple or to == set:
                if len(type_args) != 1:
                    raise ValueError(f"Only one generic type parameter may be used for {to}")
                item_type = type_args[0]
            else:
                raise ValueError(f"Generic {to} not supported")
    except ImportError:
        pass
    
    if not isinstance(to, type): # to is a callable
        return to(value) # type: ignore
    
    if isinstance(value, to):
        return value
    
    if value is None:
        return if_none
    
    strvalue = str(value)

    if to == str:
        return strvalue
    
    elif to == bool:
        return parse_bool(strvalue)

    elif to == float or to == Decimal:
        return parse_decimal(strvalue, to, accept_localized=accept_localized) # type: ignore
    
    elif to == date:
        return parse_date(strvalue, accept_localized=accept_localized)
    
    elif to == datetime or to == time:
        return parse_datetime(strvalue, accept_localized=accept_localized)
    
    elif to == list or to == tuple or to == set:
        array = parse_array(strvalue, item_type)
        if to == list:
            return array
        else:
            return to(array) # type: ignore
    
    else:
        return to(strvalue) # type: ignore
    

# ----------  Shortcuts to be used as single callables ---------- 

def convert_to_bool(value: Any, *, nullval = None, if_none = None):
    return convert(value, bool, nullval=nullval, if_none=if_none)


def convert_to_int(value: Any, *, nullval = None, if_none = None):
    return convert(value, int, nullval=nullval, if_none=if_none)


def convert_to_decimal(value: Any, *, nullval = None, if_none = None):
    return convert(value, Decimal, nullval=nullval, if_none=if_none)


def convert_to_date(value: Any, *, nullval = None, if_none = None):
    return convert(value, date, nullval=nullval, if_none=if_none)


def convert_to_datetime(value: Any, *, nullval = None, if_none = None):
    return convert(value, datetime, nullval=nullval, if_none=if_none)


# ---------- Parsers ---------- 

def parse_bool(value: bool|str):
    # same rules as RawConfigParser.BOOLEAN_STATES
    if value is None or value == '':
        return None
    elif isinstance(value, bool):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")

    lower = value.lower()
    if lower in {'1', 'yes', 'true', 'on'}:
        return True
    elif lower in {'0', 'no', 'false', 'off'}:
        return False
    else:
        raise ValueError('Not a boolean: %s' % lower)


def parse_decimal(value: Decimal|float|str, to = Decimal, *, accept_localized = True):
    if value is None or value == '':
        return None
    elif isinstance(value, to):
        return value
    elif isinstance(value, (float,Decimal)):
        return to(value)
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value).__name__}")
    
    if accept_localized and get_lang() == 'fr_FR' and isinstance(value, str):
        if re.match(r'^\-?[0-9]{1,3}(?:[ 0-9]{3})*(?:,[0-9]+)?$', value):
            value = value.replace(' ', '').replace(',', '.')
    return to(value)


def parse_datetime(value: datetime|str, *, accept_localized = True):
    if value is None or value == '':
        return None
    elif isinstance(value, datetime):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value).__name__}")

    if accept_localized and get_lang() == 'fr_FR':
        m = re.match(r'^(?P<day>[0-9]{1,2})/(?P<month>[0-9]{1,2})/(?P<year>[0-9]{4}) (?P<hour>[0-9]{1,2}):(?P<minute>[0-9]{1,2})(?::(?P<second>[0-9]{1,2}))$', value)
        if m:
            return datetime(int(m['year']), int(m['month']), int(m['day']), int(m['hour']), int(m['minute']), int(m['second']) if m['second'] else 0)

    try:
        return datetime.fromisoformat(value)
    except ValueError:
        return datetime.strptime(value, '%Y-%m-%d %H:%M:%S.%f %z') # format not accepted by fromisoformat (contrary to other non-ISO but still frequent "%Y-%m-%d %H:%M:%S.%f")


def parse_date(value: date|str, *, accept_localized = True):
    if value is None or value == '':
        return None
    elif isinstance(value, datetime):
        return value.date()
    elif isinstance(value, date):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")
    
    if accept_localized and get_lang() == 'fr_FR':
        m = re.match(r'^(?P<day>[0-9]{1,2})/(?P<month>[0-9]{1,2})/(?P<year>[0-9]{4})$', value)
        if m:
            return date(int(m['year']), int(m['month']), int(m['day']))
    
    return date.fromisoformat(value)


def parse_array(value: list|tuple|str, item_type: type|None = None):
    """ Parse an array literal (using PostgreSQL syntax) into a list. """
    # See: https://www.postgresql.org/docs/current/arrays.html#ARRAYS-INPUT
    if value is None:
        return None
    elif isinstance(value, (list,tuple)):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")

    if len(value) == 0:
        return None
    elif value[0] != '{' or value[-1] != '}':
        raise ValueError(f"Invalid postgresql array literal '{value}': does not start with '{{' and end with '}}'")
        
    def split(text: str):
        pos = 0

        def get_quoted_part(start_pos: int):
            nonlocal pos
            pos = start_pos
            while True:
                try:
                    next_pos = text.index('"', pos + 1)
                except ValueError:
                    raise ValueError(f"Unclosed quote from position {pos}: {text[pos:]}")
                
                pos = next_pos
                if text[pos - 1] == '\\' and (pos <= 2 or text[pos - 2] != '\\'): # escaped quote
                    pos += 1 # will search next quote
                else:
                    value = text[start_pos+1:pos]
                    pos += 1
                    if pos == len(text): # end
                        pass
                    else:
                        if text[pos] != ',':
                            raise ValueError(f"Quoted part \"{value}\" is followed by \"{text[pos]}\", expected a comma")
                        pos += 1
                    return value

        def get_unquoted_part(start_pos: int):
            nonlocal pos
            try:
                pos = text.index(',', start_pos)
                value = text[start_pos:pos]
                pos += 1
            except ValueError:
                pos = len(text) # end
                value = text[start_pos:]

            if value.lower() == 'null':
                return None
            return value

        def unescape(part: str|None):
            if part is None:
                return part
            part = part.replace('\\"', '"').replace('\\\\', '\\')
            if item_type:
                part = convert(part, item_type)
            return part
        
        parts: list[str] = []
        while pos < len(text):
            char = text[pos]
            if char == ',':
                part = ''
                pos += 1
            elif char == '"':
                part = get_quoted_part(pos)
            elif char == '{':
                raise NotImplementedError("Parsing sub arrays is not implemented yet") # ROADMAP
            else:
                part = get_unquoted_part(pos)
            parts.append(unescape(part))

        return parts

    return split(value[1:-1])


# ---------- Other conversions ----------

def get_array_literal(values: Iterable) -> str:
    """ Parse an Iterable into an array literal (using PostgreSQL syntax). """
    # See: https://www.postgresql.org/docs/current/arrays.html#ARRAYS-INPUT

    if values is None:
        return None
    
    escaped: list[str] = []
    for value in values:
        if value is None:
            value = "null"
        elif isinstance(value, (list,tuple)):
            value = get_array_literal(value)
        else:
            if not isinstance(value, str):
                value = str(value)
            if value.lower() == "null":
                value = f'"{value}"'
            elif ',' in value or '"' in value or '\\' in value or '{' in value or '}' in value:
                value = '"' + value.replace('\\', '\\\\').replace('"', '\\"') + '"'
        escaped.append(value)

    return '{' + ','.join(escaped) + '}'


def get_func_parameters(func: Callable, *args: str):
    """
    Convert `args` (list of strings typically comming from the command line) into typed args and kwargs for `func`.
    """
    if not args:
        return tuple(), dict()
    
    # Determine argument types
    signature = inspect.signature(func)
    var_positional_type = None
    var_keyword_type = None
    parameter_types = {}
    positional_types = []
    for parameter in signature.parameters.values():
        parameter_type = None if parameter.annotation is inspect.Parameter.empty else parameter.annotation
        if parameter.kind == inspect.Parameter.VAR_POSITIONAL:
            var_positional_type = parameter_type
        elif parameter.kind == inspect.Parameter.VAR_KEYWORD:
            var_keyword_type = parameter_type
        else:
            parameter_types[parameter.name] = parameter_type
            if parameter.kind in [inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]:
                positional_types.append(parameter_type)
    
    # Distinguish args and kwargs
    positionnal_args = []
    keyword_args = {}
    for arg in args:
        m = re.match(r'^([a-z0-9_]+)=(.+)$', arg)
        if m:
            keyword_args[m[1]] = m[2]
        else:
            positionnal_args.append(arg)

    # Convert kwargs
    for parameter, value in keyword_args.items():
        if parameter in parameter_types:
            target_type = parameter_types[parameter]
            if target_type:
                keyword_args[parameter] = convert(value, target_type)

        elif var_keyword_type:
            keyword_args[parameter] = convert(value, var_keyword_type)

    # Convert args
    for i, value in enumerate(positionnal_args):
        if i < len(positional_types):
            target_type = positional_types[i]
            if target_type:
                positionnal_args[i] = convert(value, target_type)

        elif var_positional_type:
            positionnal_args[i] = convert(value, var_positional_type)

    return positionnal_args, keyword_args

#endregion


#region CSV

_default_decimal_separator = None

def get_default_decimal_separator(*, csv_delimiter: str|None = None) -> str:
    global _default_decimal_separator
    if _default_decimal_separator is None:
        if os.environ.get('CSV_DELIMITER') == ',':
            _default_decimal_separator = '.'
        else:
            with use_locale():
                _default_decimal_separator = locale.localeconv()["decimal_point"]
    if csv_delimiter and _default_decimal_separator == csv_delimiter:
        return ',' if csv_delimiter == '.' else '.'
    return _default_decimal_separator

def set_default_decimal_separator(separator: str):
    global _default_decimal_separator
    _default_decimal_separator = separator


_default_csv_delimiter = None

def get_default_csv_delimiter() -> str:
    global _default_csv_delimiter
    if _default_csv_delimiter is None:
        _default_csv_delimiter = os.environ.get('CSV_DELIMITER')
        if not _default_csv_delimiter:
            decimal_separator = get_default_decimal_separator()
            _default_csv_delimiter = ';' if decimal_separator == ',' else ','
    return _default_csv_delimiter

def set_default_csv_delimiter(delimiter: str, *, unless_env = False):
    """
    Set the default CSV delimiter to the given value.

    If `unless_env` is true and environment variable `CSV_DELIMITER` is defined, use this value instead.
    """
    global _default_csv_delimiter
    
    if unless_env:
        env_delimiter = os.environ.get('CSV_DELIMITER')
        if env_delimiter:
            _default_csv_delimiter = env_delimiter
            return
        
    _default_csv_delimiter = delimiter


def get_csv_columns(file: TextIO|str|os.PathLike, *, encoding = 'utf-8', delimiter: str|None = None, quotechar = '"', force_delimiter: bool|None = None):
    columns, _, _ = examine_csv_file(file, encoding=encoding, delimiter=delimiter, quotechar=quotechar, force_delimiter=force_delimiter)
    return columns


def examine_csv_file(file: TextIO|str|os.PathLike, *, encoding = 'utf-8', delimiter: str|None = None, quotechar = '"', force_delimiter: bool|None = None, need_ends_with_newline: bool = False) -> tuple[list[str]|None,str|None,bool|None]:
    """
    Returns `(columns, delimiter, ends_with_newline)`
    """
    from zut import files

    columns = None
    ends_with_newline = None

    if not isinstance(file, IOBase):
        if not files.exists(file):
            raise FileNotFoundError(f"CSV file not found: {file}")

    first_line_io = StringIO()
    with nullcontext(file) if isinstance(file, IOBase) else files.open(file, 'r', encoding='utf-8' if encoding == 'utf-8-sig' else encoding, newline='') as fp:
        skip_utf8_bom(fp, encoding)

        first_line_ended = False
        buf_size = 65536
        while True:
            chunk = fp.read(buf_size)
            if not chunk:
                break

            if not first_line_ended:
                pos = chunk.find('\n')
                if pos >= 0:
                    first_line_io.write(chunk[:pos])
                    first_line_ended = True
                    if not need_ends_with_newline:
                        break
                else:
                    first_line_io.write(chunk)

            if need_ends_with_newline:
                ends_with_newline = chunk[-1] == '\n'

    if first_line_io.tell() == 0:
        return columns, delimiter, ends_with_newline

    # Guess the delimiter
    first_line_str = first_line_io.getvalue()
    comma_count = first_line_str.count(',')
    semicolon_count = first_line_str.count(';')

    guessed_delimiter = None
    if semicolon_count > comma_count:
        guessed_delimiter = ';'
    elif comma_count > 0:
        guessed_delimiter = ','

    # Compare with the given delimiter
    if delimiter:
        if guessed_delimiter and guessed_delimiter != delimiter:
            if force_delimiter:
                pass # keep given delimiter
            else:
                if force_delimiter is None:
                    _logger.warning("Use guessed CSV delimiter (\"%s\") instead of given CSV delimiter (\"%s\") for %s", guessed_delimiter, delimiter, file)
                delimiter = guessed_delimiter
    else:
        delimiter = guessed_delimiter

    # Retrieve column names
    first_line_io.seek(0)
    reader = csv.reader(first_line_io, delimiter=delimiter or get_default_csv_delimiter(), quotechar=quotechar, doublequote=True)
    columns = next(reader)

    # Ensure we move back the fp if it was externally built
    if isinstance(file, IOBase):
        file.seek(0)
        skip_utf8_bom(file, encoding)

    return columns, delimiter, ends_with_newline


def escape_csv_value(value, *, delimiter: str|None = None, quotechar = '"', nullval: str|None = None):
    if value is None:    
        return nullval if nullval is not None else ''
    if not isinstance(value, str):
        value = str(value)
    if value == '':
        return f'{quotechar}{quotechar}'
    
    if not delimiter:
        delimiter = get_default_csv_delimiter()

    need_escape = False
    result = ''
    for c in value:
        if c == delimiter:
            result += c
            need_escape = True
        elif c == quotechar:
            result += f'{c}{c}'
            need_escape = True
        elif c == '\n' or c == '\r':
            result += c
            need_escape = True
        else:
            result += c

    if need_escape:
        result = f'{quotechar}{result}{quotechar}'
    else:
        result = result

    return result


def format_csv_value(value, *, delimiter: str|None = None, decimal_separator: str|None = None, no_tz: bool|tzinfo = False, no_microseconds = False, visual = False):
    if decimal_separator is None:
        decimal_separator = get_default_decimal_separator(csv_delimiter=delimiter)
        
    def to_visual_list(values: list|tuple):
        target_str = ''
        for value in values:
            value_formatted = format_value(value, root=False)
            value_str = value_formatted if isinstance(value_formatted, str) else str(value_formatted) if value_formatted else ''
            if '|' in value_str:
                return json.dumps(values, ensure_ascii=False, cls=ExtendedJSONEncoder)
            target_str = (f'{target_str}|' if target_str else '') + value_str
        
        return target_str

    def to_visual_dict(values: dict):
        target_str = ''
        for key, value in values.items():
            key_str = str(key)
            value_formatted = format_value(value, root=False)
            value_str = value_formatted if isinstance(value_formatted, str) else str(value_formatted) if value_formatted else ''
            if '=' in key_str or '|' in key_str or '=' in value_str or '|' in value_str:
                return json.dumps(values, ensure_ascii=False, cls=ExtendedJSONEncoder)
            target_str = (f'{target_str}|' if target_str else '') + (f'{key_str}={value_str}' if value is not None else f'{key_str}')
            
        return target_str
    
    def format_value(value, *, root):    
        if value is None:
            return None

        if no_tz and isinstance(value, str):
            if re.match(r'^\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d{3,6})?(?:Z|[+\-\d]{2,5})?$', value): # recognize datetime ISO strings (typically comming from APIs with JSON-encoded data) and handle them later as datetimes
                value = datetime.fromisoformat(value)

        if isinstance(value, (Enum,Flag)):
            return value.name if visual else value.value
            
        elif isinstance(value, bool):
            if visual:
                return value
            else:
                return 'true' if value else 'false'
        
        elif isinstance(value, int):
            return value
        
        elif isinstance(value, (float,Decimal)):
            if decimal_separator != '.':
                return str(value).replace('.', decimal_separator)        
            return value
        
        elif isinstance(value, (datetime,time)):
            if no_tz:
                if value.tzinfo: # make the datetime naive if it is not already
                    if value.year >= 2999: # avoid astimezone() issue for conversion of datetimes such as 9999-12-31 23:59:59.999999+00:00 or 4000-01-02 23:00:00+00:00
                        value = value.replace(tzinfo=None)
                    else:
                        value = value.astimezone(None if no_tz is True or no_tz == 'localtime' else no_tz).replace(tzinfo=None)
            if no_microseconds:
                return value.replace(microsecond=0)
            return value

        elif isinstance(value, (list,tuple)):
            if visual:
                return to_visual_list(value) if root else json.dumps(value, ensure_ascii=False, cls=ExtendedJSONEncoder)
            else:
                return get_array_literal(value)
        
        elif isinstance(value, dict):
            return to_visual_dict(value) if visual and root else json.dumps(value, ensure_ascii=False, cls=ExtendedJSONEncoder)
                
        else:
            return value

    return format_value(value, root=True)


@overload
def format_csv_row(row: Iterable, *, delimiter: str|None = None, decimal_separator: str|None = None, no_tz: bool|tzinfo = False, no_microseconds = False, visual = False, as_string: Literal[False] = ...) -> list[Any]:
    ...

@overload
def format_csv_row(row: Iterable, *, delimiter: str|None = None, decimal_separator: str|None = None, no_tz: bool|tzinfo = False, no_microseconds = False, visual = False, as_string: Literal[True]) -> str:
    ...

def format_csv_row(row: Iterable, *, delimiter: str|None = None, decimal_separator: str|None = None, no_tz: bool|tzinfo = False, no_microseconds = False, visual = False, as_string = False) -> list[Any]|str:
    if as_string:
        target = ''
        if delimiter is None:
            delimiter = get_default_csv_delimiter()
    else:
        target = []

    first = True
    for value in row:
        formatted_value = format_csv_value(value, delimiter=delimiter, decimal_separator=decimal_separator, no_tz=no_tz, no_microseconds=no_microseconds, visual=visual)
        if as_string:
            if first:
                first = False
            else:
                target += delimiter
            target += escape_csv_value(formatted_value)
        else:
            target.append(formatted_value)
    return target

#endregion


#region JSON
    
class ExtendedJSONEncoder(json.JSONEncoder):
    """
    Adapted from: django.core.serializers.json.DjangoJSONEncoder
    
    Usage example: json.dumps(data, indent=4, cls=ExtendedJSONEncoder)
    """
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def default(self, o):
        if isinstance(o, datetime):
            r = o.isoformat()
            if o.microsecond and o.microsecond % 1000 == 0:
                r = r[:23] + r[26:]
            if r.endswith("+00:00"):
                r = r[:-6] + "Z"
            return r
        elif isinstance(o, date):
            return o.isoformat()
        elif isinstance(o, time):
            if o.tzinfo is not None:
                raise ValueError("JSON can't represent timezone-aware times.")
            r = o.isoformat()
            if o.microsecond and o.microsecond % 1000 == 0:
                r = r[:12]
            return f'T{r}'
        elif isinstance(o, timedelta):
            return duration_iso_string(o)
        elif isinstance(o, (Decimal, UUID)):
            return str(o)
        else:
            try:
                from django.utils.functional import Promise
                if isinstance(o, Promise):
                    return str(o)
            except ModuleNotFoundError:
                pass

            if isinstance(o, (Enum,Flag)):
                return o.value
            elif isinstance(o, bytes):
                return str(o)
            else:
                return super().default(o)

#endregion


#region Locale

def register_locale(name: str = ''):
    """
    Register a locale for the entire application (system default locale if argument `value` is None).
    """
    locale.setlocale(locale.LC_ALL, name)


@contextmanager
def use_locale(name = ''):
    """
    Use a locale temporary (in the following thread-local block/context).

    See: https://stackoverflow.com/a/24070673
    """
    with _locale_lock:
        saved = locale.setlocale(locale.LC_ALL)
        try:
            yield locale.setlocale(locale.LC_ALL, name)
        finally:
            locale.setlocale(locale.LC_ALL, saved)

_locale_lock = threading.Lock()


def get_lang():
    """
    Return current locale lang, for example: `fr_FR`.
    """
    global _lang
    if _lang is None:
        with use_locale():
            _lang = locale.getlocale()[0]
    return _lang

_lang = None

#endregion


#region Time

T_WithTime = TypeVar('T_WithTime', datetime, time)

def parse_tz(tz: tzinfo|str|Literal['localtime']|None = None) -> tzinfo:
    # ZoneInfo: introduced in Python 3.9
    try:
        from zoneinfo import ZoneInfo
    except ImportError:
        ZoneInfo = None

    if tz is None or tz == 'localtime':
        if not ZoneInfo or sys.platform == 'win32':
            # tzlocal: used to parse timezones from strings on Windows (Windows does not maintain a database of timezones and `tzdata` only is not enough)
            try:
                import tzlocal  # type: ignore
            except ModuleNotFoundError:
                tzlocal = None
            if not tzlocal:
                raise ValueError(f"Package `tzlocal` is required on Windows or on Python < 3.9 to retrieve local timezone")
            return tzlocal.get_localzone()
        else:
            return ZoneInfo('localtime')
    elif isinstance(tz, tzinfo):
        return tz
    elif tz == 'UTC':
        return timezone.utc
    elif isinstance(tz, str):
        if not ZoneInfo:
            # pytz: used to parse timezones on Python < 3.9 (no ZoneInfo available)
            try:
                import pytz  # type: ignore
            except ModuleNotFoundError:
                pytz = None
            if not pytz:
                raise ValueError(f"Package `pytz` is required on Python < 3.9 to parse timezones from strings")
            return pytz.timezone(tz)
        if sys.platform == 'win32':
            # tzdata: used to parse timezones from strings through ZoneInfo on Windows (Windows does not maintain a database of timezones)
            try:
                import tzdata
            except ModuleNotFoundError:
                tzdata = None
            if not tzdata:
                raise ValueError(f"Package `tzdata` is required on Windows to parse timezones from strings")
        return ZoneInfo(tz)
    else:
        raise TypeError(f"Invalid timezone type: {tz} ({type(tz).__name__})")
    

def get_tzkey(tz: tzinfo|Literal['localtime']|None = None) -> str:
    if tz is None or tz == 'localtime':
        tz = parse_tz()
    key = getattr(tz, 'key', None)
    if key:
        return key
    raise ValueError(f"{type(tz).__name__} object has no key")


def is_aware(value: T_WithTime):
    # See: https://docs.python.org/3/library/datetime.html#determining-if-an-object-is-aware-or-naive
    if value is None:
        return False
    return value.tzinfo is not None and value.utcoffset() is not None


def make_aware(value: T_WithTime, tz: tzinfo|str|None = None) -> T_WithTime:
    """
    Make a datetime aware in timezone `tz` (use `tz=None` or `tz='localtime'` for the system local timezone).
    """
    if value is None:
        return None
    
    if is_aware(value):
        raise ValueError("make_aware expects a naive datetime, got %s" % value)
    
    tz = parse_tz(tz)
    if hasattr(tz, 'localize'):
        # See: https://stackoverflow.com/a/6411149
        return tz.localize(value) # type: ignore
    else:
        return value.replace(tzinfo=tz)


def is_naive(value: T_WithTime):
    return not is_aware(value)


def make_naive(value: datetime, tz: tzinfo|str|None = None) -> datetime:
    """
    Make a datetime naive and expressed in timezone `tz` (use `tz=None` or `tz='localtime'` for the system local timezone).
    """
    if value is None:
        return None

    if not is_aware(value):
        raise ValueError("make_naive expects an aware datetime, got %s" % value)
    
    value = value.astimezone(None if tz is None or tz == 'localtime' else parse_tz(tz))
    value = value.replace(tzinfo=None)
    return value


def now_aware(tz: tzinfo|str|None = None, *, ms = True):
    """
    Get the current datetime in the timezone `tz` (use `tz=None` or `tz='localtime'` for the system local timezone).
    """
    now = datetime.now().astimezone(None if tz is None or tz == 'localtime' else parse_tz(tz))
    if not ms:
        now = now.replace(microsecond=0)
    return now


def now_naive_utc(*, ms = True):
    """
    Get the current datetime in expressed as a naive UTC.
    """
    now = datetime.now(tz=timezone.utc).replace(tzinfo=None)
    if not ms:
        now = now.replace(microsecond=0)
    return now


def duration_iso_string(duration: timedelta):
    # Adapted from: django.utils.duration.duration_iso_string
    if duration < timedelta(0):
        sign = "-"
        duration *= -1
    else:
        sign = ""

    days, hours, minutes, seconds, microseconds = _get_duration_components(duration)
    ms = ".{:06d}".format(microseconds) if microseconds else ""
    return "{}P{}DT{:02d}H{:02d}M{:02d}{}S".format(
        sign, days, hours, minutes, seconds, ms
    )


def _get_duration_components(duration: timedelta):
    days = duration.days
    seconds = duration.seconds
    microseconds = duration.microseconds

    minutes = seconds // 60
    seconds = seconds % 60

    hours = minutes // 60
    minutes = minutes % 60

    return days, hours, minutes, seconds, microseconds

#endregion


#region URLs

def build_url(result: ParseResult|None = None, *, scheme: str = '', hostname: str|IPv4Address|IPv6Address|None = None, port: int|None = None, username: str|None = None, password: Secret|str|None = None, path: str|None = None, params: str|None = None, query: str|None = None, fragment: str|None = None, noquote = False, hide_password = False):
    if result:
        if scheme == '' and result.scheme:
            scheme = result.scheme
        if hostname is None and result.hostname is not None:
            hostname = unquote(result.hostname)
        if port is None and result.port is not None:
            port = result.port
        if username is None and result.username is not None:
            username = unquote(result.username)
        if password is None and result.password is not None:
            password = unquote(result.password)
        if path is None and result.path is not None:
            path = unquote(result.path)
        if params is None and result.params is not None:
            params = unquote(result.params)
        if query is None and result.query is not None:
            query = unquote(result.query)
        if fragment is None and result.fragment is not None:
            fragment = unquote(result.fragment)

    netloc = build_netloc(hostname=hostname, port=port, username=username, password=password, noquote=noquote, hide_password=hide_password)

    if noquote:
        actual_query = query
    else:
        if isinstance(query, dict):
            actual_query = urlencode(query)
        elif isinstance(query, list):
            named_parts = []
            unnamed_parts = []
            for part in query:
                if isinstance(part, tuple):
                    named_parts.append(part)
                else:
                    unnamed_parts.append(part)
            actual_query = urlencode(named_parts, quote_via=quote_plus)
            actual_query += ('&' if actual_query else '') + '&'.join(quote_plus(part) for part in unnamed_parts)
        else:
            actual_query = query

    return urlunparse((scheme or '', netloc or '', (path or '') if noquote else quote(path or ''), (params or '') if noquote else quote_plus(params or ''), actual_query or '', (fragment or '') if noquote else quote_plus(fragment or '')))


def build_netloc(*, hostname: str|IPv4Address|IPv6Address|None = None, port: int|None = None, username: str|None = None, password: Secret|str|None = None, noquote = False, hide_password = False):
    netloc = ''
    if username or hostname:
        if username:
            netloc += username if noquote else quote_plus(username)
            if password:
                if hide_password:
                    password = '***'
                else:
                    if isinstance(password, Secret):
                        password = password.value
                    if not noquote:
                        password = quote_plus(password) # type: ignore
                netloc += f':{password}'
            netloc += '@'

        if hostname:
            if isinstance(hostname, IPv4Address):
                netloc += hostname.compressed
            elif isinstance(hostname, IPv6Address):
                netloc += f"[{hostname.compressed}]"
            else:
                ipv6 = None
                if ':' in hostname:
                    try:
                        ipv6 = IPv6Address(hostname)
                    except AddressValueError:
                        pass

                if ipv6:
                    netloc += f"[{ipv6.compressed}]"
                else:
                    netloc += hostname if noquote else quote_plus(hostname)

            if port:
                if not (isinstance(port, int) or (isinstance(port, str) and re.match(r'^\d+$', port))):
                    raise ValueError(f"invalid type for port: {type(port)}")
                netloc += f':{port}'
    elif port:
        raise ValueError("Argument 'port' cannot be given without a hostname")
    elif password:
        raise ValueError("Argument 'password' cannot be given without a username")

    return netloc


def hide_url_password(url: str, *, always_password = False):
    r = urlparse(url)
    password = r.password
    if not password and always_password:
        password = '***'
    return build_url(scheme=r.scheme, hostname=r.hostname, port=r.port, username=r.username, password=password, path=r.path, params=r.params, query=r.query, fragment=r.fragment, noquote=True, hide_password=True)

#endregion


#region Network

def get_host_ip() -> str:
    hostname = socket.gethostname()
    return socket.gethostbyname(hostname)


def get_linux_default_gateway_ip(iface: str|None = None):
    with open("/proc/net/route") as fp:
        for line in fp:
            fields = line.strip().split()
            
            if iface and fields[0] != iface:
                continue

            if fields[1] != '00000000' or not int(fields[3], 16) & 2: # if not default route or not RTF_GATEWAY, skip it
                continue

            return socket.inet_ntoa(struct.pack("<L", int(fields[2], 16)))


def resolve_host(host: str, *, timeout: float|None = None, ip_version: int|None = None) -> list[str]:
    """
    Make a DNS resolution with a timeout.
    """
    try:
        # If host is already an ip address, return it
        ip = ip_address(host)
        if not ip_version or ip.version == ip_version:
            return [ip.compressed]
    except ValueError:
        pass
    
    if ip_version is None:
        family = 0
    elif ip_version == 4:
        family = socket.AddressFamily.AF_INET
    elif ip_version == 6:
        family = socket.AddressFamily.AF_INET6
    else:
        raise ValueError(f"Invalid ip version: {ip_version}")

    addresses = []
    exception = None

    def target():
        nonlocal addresses, exception
        try:
            for af, socktype, proto, canonname, sa in socket.getaddrinfo(host, port=0, family=family):
                addresses.append(sa[0])
        except BaseException as err:
            exception = err

    if timeout is not None:
        thread = Thread(target=target, daemon=True)
        thread.start()
        thread.join(timeout=timeout)
        if thread.is_alive():
            raise TimeoutError(f"Name resolution for host \"{host}\" timed out")

    else:
        target()

    if exception:
        err = NameError(str(exception))
        err.name = host
        raise err
        
    return addresses


_wpad_proxy = None
_wpad_proxy_requested = False

def get_wpad_proxy(*, timeout: float = 1.0) -> str|None:
    global _wpad_proxy, _wpad_proxy_requested
    if _wpad_proxy_requested:
        return _wpad_proxy
    
    try:
        wpad_ip = resolve_host('wpad', timeout=1)[0]
    except Exception as err: #timeout or out of range
        _logger.debug("wpad resolution: %s", err)
        _wpad_proxy = None
        _wpad_proxy_requested = True
        return _wpad_proxy
    
    wpad_url = f"http://{wpad_ip}/wpad.dat"
    
    request = Request(wpad_url)
    response: HTTPResponse
    try:
        with urlopen(request, timeout=timeout) as response:
            _logger.debug("WPAD response: %s %s - Content-Type: %s", response.status, response.reason, response.headers.get('Content-Type'))
            body = response.read().decode('utf-8')
    except URLError as err:
        _logger.error(f"Cannot retrieve WPAD: {err.reason}")
        _wpad_proxy = None
        _wpad_proxy_requested = True
        return _wpad_proxy
    except HTTPError as err:
        _logger.error(f"Cannot retrieve WPAD: HTTP {err.status} {err.reason}")
        _wpad_proxy = None
        _wpad_proxy_requested = True
        return _wpad_proxy
    
    _wpad_proxy = None
    for line in body.splitlines():
        line = line.strip()
        if not line or line.startswith('//') or line in {'{', '}', 'function FindProxyForURL(url, host)'}:
            continue
        else:
            m = re.match(r'^return\s*"PROXY\s*(?P<host>[^\s"\:]+)\:(?P<port>\d+)".*', line, re.IGNORECASE)
            if m:
                _wpad_proxy = f"http://{m['host']}:{m['port']}"
                break
    
    _wpad_proxy_requested = True
    return _wpad_proxy


def check_host_port(hostport: tuple[str,int]|Iterable[tuple[str,int]], *, timeout: float = None) -> tuple[str,int]|None:
    """
    Check whether at least one of the given host and port is open.

    If yes, return the first opened (host, port). Otherwise return None.
    """
    if isinstance(hostport, tuple):
        hostport = [hostport]

    opened: list[tuple[str,int]] = []

    def target(host: str, port: int):
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex((host, port))
            if result == 0:
                _logger.debug("Host %s, port %s: opened", host, port)
                opened.append((host, port))
            else:
                _logger.debug("Host %s, port %s: NOT opened", host, port)
            sock.close()
        except Exception as err:
            _logger.debug("Host %s, port %s: %s", host, port, err)

    threads: list[Thread] = []
    for host, port in hostport:
        thread = Thread(target=target, args=[host, port], daemon=True)
        thread.start()
        threads.append(thread)

    # Wait for all threads
    if timeout is not None:
        stop_time = time_s() + timeout
        while time_s() < stop_time:
            if any(t.is_alive() for t in threads):
                sleep_s(0.1)
            else:
                break
    else:
        for thread in threads:
            thread.join()

    # Return
    if opened:
        return opened[0]
    else:
        return None


class JSONApiClient:
    """
    An API client using only Python standard library.
    """

    base_url : str = None
    timeout: float = None
    """ Timeout in seconds. """

    force_trailing_slash: bool = False

    default_headers = {
        'Content-Type': 'application/json; charset=utf-8',
        'Accept': 'application/json; charset=utf-8',
    }

    json_encoder_cls: type[json.JSONEncoder] = ExtendedJSONEncoder
    json_decoder_cls: type[json.JSONDecoder] = json.JSONDecoder
    
    nonjson_error_maxlen = 400

    no_ssl_verify = False


    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs) # necessary to allow this class to be used as a mixin
        self._logger = get_logger(type(self).__module__ + '.' + type(self).__name__)
        self._ssl_context = None
        if self.no_ssl_verify or kwargs.get('no_ssl_verify'):
            self._ssl_context = ssl.create_default_context()
            self._ssl_context.check_hostname = False
            self._ssl_context.verify_mode = ssl.CERT_NONE


    def __enter__(self):
        return self


    def __exit__(self, exc_type = None, exc_value = None, exc_traceback = None):
        pass


    def get(self, endpoint: str = None, *, params: dict = None, headers: MutableMapping[str,str] = None, return_headers = False):
        return self.request(endpoint, method='GET', params=params, headers=headers, return_headers=return_headers)


    def post(self, endpoint: str = None, data = None, *, params: dict = None, headers: MutableMapping[str,str] = None, return_headers = False, content_type: str = None, content_length: int = None, content_filename: str = None):
        return self.request(endpoint, data, method='POST', params=params, headers=headers, return_headers=return_headers, content_type=content_type, content_length=content_length, content_filename=content_filename)
    

    def request(self, endpoint: str = None, data = None, *, method = None, params: dict = None, headers: MutableMapping[str,str] = None, return_headers = False, content_type: str = None, content_length: int = None, content_filename: str = None):
        url = self.prepare_url(endpoint, params=params)

        all_headers = self.get_request_headers(url)
        if headers:
            for key, value in headers.items():
                all_headers[key] = value
                if key == 'Content-Type' and not content_type:
                    content_type = value
                elif key == 'Content-Length' and content_length is None:
                    content_length = int(value) if isinstance(value, str) else value
                elif key == 'Content-Disposition' and not content_filename:
                    m = re.search(r'attachment\s*;\s*filename\s*=\s*(.+)', value)
                    if m:
                        content_filename = m[1].strip()

        if content_type:
            all_headers['Content-Type'] = content_type
        if content_length is not None:
            all_headers['Content-Length'] = content_length
        if content_filename:
            all_headers['Content-Disposition'] = f"attachment; filename={content_filename}"
                
        if data is not None:
            if not method:
                method = 'POST'

            if isinstance(data, IOBase) or (content_type and not 'application/json' in content_type):
                # keep data as is: this is supposed to be an uploaded file
                if not content_type:
                    content_type = 'application/octet-stream'
            else:
                data = json.dumps(data, ensure_ascii=False, cls=self.json_encoder_cls).encode('utf-8')
            
            self._logger.debug('%s %s', method, url)
            request = Request(url,
                method=method,
                headers=all_headers,
                data=data,
            )
        else:
            if not method:
                method = 'GET'
            
            self._logger.debug('%s %s', method, url)
            request = Request(url,
                method=method,
                headers=all_headers,
            )

        response_headers = {}
        try:
            response: HTTPResponse
            with urlopen(request, timeout=self.timeout, context=self._ssl_context) as response:
                response_headers = response.headers
                if self._logger.isEnabledFor(logging.DEBUG):
                    content_type = response.headers.get('content-type', '-')
                    self._logger.debug('%s %s %s %s', response.status, url, response.length, content_type)
                decoded_response = self.decode_response(response)
            
            if return_headers:
                return decoded_response, response.headers
            else:
                return decoded_response
            
        except HTTPError as error:
            with error:
                http_data = self.decode_response(error)
            built_error = self.build_client_error(error, http_data)
        except URLError as error:
            built_error = self.build_client_error(error, None)

        if isinstance(built_error, Exception):
            raise built_error from None
        else:
            if return_headers:
                return built_error, response_headers
            else:
                return built_error


    def prepare_url(self, endpoint: str, *, params: dict = None, base_url: str = None):
        if endpoint is None:
            endpoint = ''

        if not base_url and self.base_url:
            base_url = self.base_url

        if '://' in endpoint or not base_url:
            url = endpoint
            
        else:            
            if endpoint.startswith('/'):
                if base_url.endswith('/'):                    
                    endpoint = endpoint[1:]
            else:
                if not base_url.endswith('/') and endpoint:
                    endpoint = f'/{endpoint}'
            
            if self.force_trailing_slash and not endpoint.endswith('/'):
                endpoint = f'{endpoint}/'

            url = f'{base_url}{endpoint}'

        if params:
            url += "?" + urlencode(params)
        
        return url
    

    def get_request_headers(self, url: str) -> MutableMapping[str,str]:
        headers = {**self.default_headers}
        return headers


    def decode_response(self, response: HTTPResponse):
        rawdata = response.read()
        try:
            strdata = rawdata.decode('utf-8')
        except UnicodeDecodeError:
            strdata = str(rawdata)
            if self.nonjson_error_maxlen is not None and len(strdata) > self.nonjson_error_maxlen:
                strdata = strdata[0:self.nonjson_error_maxlen] + '…'
            return f"[non-utf-8] {strdata}"
        
        try:
            jsondata = json.loads(strdata, cls=self.json_decoder_cls)
        except json.JSONDecodeError:
            if self.nonjson_error_maxlen is not None and len(strdata) > self.nonjson_error_maxlen:
                strdata = strdata[0:self.nonjson_error_maxlen] + '…'
            return f"[non-json] {strdata}"
        
        return jsondata


    def build_client_error(self, error: URLError, http_data):
        if isinstance(error, HTTPError):
            return ApiClientError(error.reason, code=error.status, code_nature='status', data=http_data)
        else:
            return ApiClientError(error.reason, code=error.errno, code_nature='errno', data=http_data)
        

class ApiClientError(Exception):
    def __init__(self, message: str, *, code: int = None, code_nature = None, data = None):
        self.raw_message = message
        self.code = code
        self.code_nature = code_nature
        self.data = data

        super().__init__(self.raw_to_message())


    def raw_to_message(self):
        message = self.raw_message

        if self.code:
            message = (message + ' ' if message else '') + f"[{self.code_nature or 'code'}: {self.code}]"
        
        if self.data:
            if isinstance(self.data, dict):
                for key, value in self.data.items():
                    message = (message + '\n' if message else '') + f"{key}: {value}"
            else:
                message = (message + '\n' if message else '') + str(self.data)

        return message

#endregion


#region Tables

class Header:
    _type_ = type

    not_null: bool|None
    """ Indicate whether this column is has a 'not null' constraint. """

    primary_key: bool|None
    """ Indicate whether this column is part of the primary key. """

    unique: bool|list[tuple[str]]|None
    """ True for a single-column identity Key, a list of column tuples for multi-column identity key(s). """

    identity: bool|str|None
    """ Indicate whether this is an auto-generated identity column. """

    type: type|None
    """ Python type of the data. """
    
    precision: int|None
    """ Precision of the SQL type. """
    
    scale: int|None
    """ Scale of the SQL type. """

    default: Any
    """ Default value. """
    
    sql_type: str|None
    """ SQL full type name (including precision and scale if any). """
    
    sql_type_suffix: str|None
    """ Suffix for the SQL type (example: COLLATION). """

    DEFAULT_NOW = 'CURRENT_TIMESTAMP'  # ANSI/ISO SQL

    def __init__(self,
                 name: str|None = None, *,
                 key: Any|None = None,
                 type: type|None = None,
                 precision: int|None = None,
                 scale: int|None = None,
                 default: Any|None = None,
                 sql_type: str|None = None,
                 sql_type_suffix: str|None = None,
                 primary_key: bool|None = None,
                 unique: bool|list[tuple[str]]|None = None,
                 identity: bool|str|None = None,
                 not_null: bool|None = None):
        """
        Create a header for tabular data.
        - `key`: how to obtain the value from a dict object (may be a key or a list of keys for recursive get).
        - `name`: name of the header (defaults to stringified key).
        """
        if key is None:
            if name is None:
                raise ValueError(f"Key and name both null")
            key = name

        if isinstance(key, list):
            if len(key) == 0:
                raise TypeError(f"key: cannot be an empty list")
            self.key = key
        else:
            self.key = [key]

        if name is None:
            self.name = '.'.join(str(part) for part in self.key)
        elif not isinstance(name, str):
            self.name = str(name)
        else:
            self.name = name

        self.type = type
        self.precision = precision
        self.scale = scale
        self.default = default

        self.sql_type = sql_type # must be set AFTER precision and scale because the setter might override them
        self.sql_type_suffix = sql_type_suffix

        self.not_null = not_null
        self.primary_key = primary_key
        self.unique = unique
        self.identity = identity
        # ROADMAP: add `index`

    def merge(self, other: Header):
        if self.type is None:
            self.type = other.type
        if self.precision is None:
            self.precision = other.precision
        if self.scale is None:
            self.scale = other.scale

        if self.sql_type is None:
            self.sql_type = other.sql_type
        if self.sql_type_suffix is None:
            self.sql_type_suffix = other.sql_type_suffix
            
        if self.not_null is None:
            self.not_null = other.not_null
        if self.primary_key is None:
            self.primary_key = other.primary_key
        if self.unique is None:
            self.unique = other.unique
        if self.identity is None:
            self.identity = other.identity
    
    def __repr__(self):
        return self.name
    
    @property
    def type(self):        
        if not self._type and self.sql_type: # Set type from sql type
            m = re.match(r'^(?P<base>.+)\((?P<precision>\d+)(?:,(?P<scale>\d+))?\)$', self.sql_type)
            if m:
                sql_basetype = m['base'].lower()
            else:
                sql_basetype = self.sql_type.lower()

            if sql_basetype.endswith('[]'):
                self._type = list
            elif 'char' in sql_basetype or 'text' in sql_basetype:
                self._type = str
            elif sql_basetype.startswith('int') or sql_basetype in {'bigint', 'smallint', 'tinyint'}:
                self._type = int
            elif sql_basetype.startswith('bool') or sql_basetype == 'bit':
                self._type = bool
            elif sql_basetype.startswith(('float','double','real')):
                self._type = float
            elif sql_basetype in {'numeric', 'decimal'}:
                self._type = Decimal
            elif sql_basetype.startswith(('timestamp','datetime')):
                self._type = datetime
            elif sql_basetype == 'date':
                self._type = date
            elif sql_basetype == 'uuid':
                self._type = UUID
        return self._type

    @type.setter
    def type(self, value: type):
        self._type = value

    @property
    def sql_type(self):
        return self._sql_type

    @sql_type.setter
    def sql_type(self, value: str):
        if value is None:
            self._sql_type = None
        else:
            m = re.match(r'^(?P<base>.+)\((?P<precision>\d+)(?:,(?P<scale>\d+))?\)$', value)
            if m:
                self.precision = int(m['precision']) if m['precision'] is not None else None
                self.scale = int(m['scale']) if m['scale'] is not None else None
            self._sql_type = value

    @property
    def not_null(self):
        return self._not_null

    @not_null.setter
    def not_null(self, value: bool|None):
        if value is None or isinstance(value, bool):
            self._not_null = value
        elif value == 0:
            self._not_null = False
        elif value == 1:
            self._not_null = True
        else:
            raise TypeError(f"not_null: {type(value)}")
    
    @property
    def primary_key(self):
        return self._primary_key

    @primary_key.setter
    def primary_key(self, value: bool|None):
        if value is None or isinstance(value, bool):
            self._primary_key = value
        elif value == 0:
            self._primary_key = False
        elif value == 1:
            self._primary_key = True
        else:
            raise TypeError(f"primary_key: {type(value)}")
    
    @property
    def unique(self):
        return self._unique

    @unique.setter
    def unique(self, unique: bool|list[tuple[str]]|None):
        if unique is None or isinstance(unique, bool):
            self._unique = unique
        elif isinstance(unique, str):
            if not unique:
                self._unique = False
            elif unique == self.name:
                self._unique = True
            else: # encoded string
                decoded = []
                for i, encoded_key in enumerate(unique.split(';')):
                    decoded_key = encoded_key.split(',')
                    decoded.append(decoded_key)
                self.unique = decoded
        elif isinstance(unique, (list,tuple)):
            self._unique = []
            for i, key in enumerate(unique):
                if isinstance(key, (list,tuple)):
                    for j, column in enumerate(key):
                        if not isinstance(column, str):
                            raise TypeError(f"unique[{i}][{j}]: must be a column name, got {type(column).__name__}")        
                    if not self.name in key:
                        raise ValueError(f"unique[{i}] {self._unique} does not contain header name: {self.name}")
                    self._unique.append(key if isinstance(key, tuple) else tuple(key))
                else:
                    raise TypeError(f"unique[{i}]: must be a tuple of columns, got {type(key).__name__}")
        else:
            raise TypeError(f"unique: {type(unique).__name__}")
    
    @property
    def identity(self):
        return self._identity

    @identity.setter
    def identity(self, value: bool|None):
        if value is None or isinstance(value, bool):
            self._identity = value
        elif value == 0:
            self._identity = False
        elif value == 1:
            self._identity = True
        else:
            raise TypeError(f"identity: {type(value)}")
    
    @property
    def path(self):
        return '.'.join(str(part) for part in self.key)
    
    @property
    def toplevel_key(self):
        return self.key[0]
    
    def get_value(self, obj: dict):
        def recurse(obj: dict, remaining_keys: list[Any], parent_path: str):
            if obj is None:
                return None
            elif not remaining_keys:
                return obj
            else:                
                key = remaining_keys[0]
                path = parent_path + ('.' if parent_path else '') + str(key)
                remaining_keys = remaining_keys[1:]
                if not isinstance(obj, dict):
                    raise NotADirectoryError(path)
                else:
                    try:
                        sub = obj[key]
                    except KeyError:
                        raise KeyError(path)
                return recurse(sub, remaining_keys, path)
            
        return recurse(obj, self.key, '')


class RowMixinProvider(Protocol):
    @property
    def columns(self) -> tuple[str]:
        ...

    @property
    def headers(self) -> list[Header]: # optional (if not given, calling headers on the RowMixin object would fail, but the other features, which are based on `columns`, would succeed)
        ...
    
    def get_column_index(self, name: str) -> int: # optional optimisation
        """ Return index of the column with the given name, or ValueError if the value is not present """
        ...


class RowMixin:
    """ A mixin for rows with reference to columns. """
    @property
    def provider(self):
        return self._provider
    
    @provider.setter
    def provider(self, value: RowMixinProvider|list[Header|str]|tuple[Header|str]):
        self._provider = value

    @cached_property
    def columns(self) -> tuple[str]:
        if isinstance(self.provider, (list,tuple)):
            return tuple(header.name if isinstance(header, Header) else header for header in self.provider)
        else:
            return self.provider.columns

    @cached_property
    def headers(self):
        if isinstance(self.provider, (list,tuple)):
            return tuple(header if isinstance(header, Header) else Header(header) for header in self.provider)
        else:
            return self.provider.headers
    
    def __len__(self):
        return len(self.columns)
        
    def as_dict(self):
        return {column: self[i] for i, column in enumerate(self.columns)}
    
    def __getitem__(self, key: int|str): # For use as `row['mycolumn']`
        if isinstance(key, str):
            get_column_index = getattr(self.provider, 'get_column_index', None)
            if get_column_index:
                index = get_column_index(key)
            else:
                index = self.columns.index(key)
            return self[index]
        return super().__getitem__(key)

    def __getattribute__(self, name: str): # For use as `row.mycolumn`
        try:
            return super().__getattribute__(name)
        except AttributeError:
            try:
                return self[name]
            except ValueError:
                pass
            raise


class ListRow(RowMixin, list):
    """ Mutable values with reference to columns. """
    def __init__(self, values: Iterable):
        super().__init__(values)
    
    def __setitem__(self, key: int|str, value): # For use as `row['mycolumn'] =`
        if isinstance(key, str):
            get_column_index = getattr(self.provider, 'get_column_index', None)
            if get_column_index:
                index = get_column_index(key)
            else:
                index = self.columns.index(key)
            self[index] = value
            return
        super().__setitem__(key, value)


class TupleRow(RowMixin, tuple):
    """ Immutable values with reference to columns. """
    def __init__(self, values: Iterable):
        super().__init__() # values passed automatically because we are a subclass of tuple

#endregion


#region Dump

def dump(data, *,
         dst: str|os.PathLike|TextIO,
         tabular: bool|None = None,
         append = False,
         archivate: bool|str|os.PathLike|None = None,
         title: str|bool|None = None,
         interval: float|int|bool|None = None,
         dst_name: str|bool = True,
         dir: str|os.PathLike|None = None,
         # Other TabularDumper options
         headers: Iterable[str|Header]|None = None,
         delay: bool = True,         
         defaults: dict[str,Any]|None = None,
         optional: str|Sequence[str]|Literal['*',True]|None = None,
         add_columns: bool|None = None,
         no_tz: bool|tzinfo|str|None = None,
         # TabDumper and CsvDumper options
         encoding = 'utf-8-sig',
         # CsvDumper options
         delimiter: str|None = None,
         decimal_separator: str|None = None,
         quotechar = '"',
         nullval: str|None = None,
         excel: bool|None = None,
         # Destination mask values and other options
         **kwargs):
    """
    Dump an object (if `data` is not an iterable) or a an iterable of dicts or iterables (if `data` is an iterable).

    NOTE: `delay` is True by default for `dump` because we expect the input data to be already produced entirely so we can take time to analyze all the dict keys to determine headers,
    whereas `delay` is False by default for `dump_tabular` or `CsvDumper` because we expect appended data to be produced on the fly so we favor immediate processing.
    """
    extended_kwargs = {
        **kwargs,
        'headers': headers,
        'append': append,
        'archivate': archivate,
        'title': title,
        'interval': interval,
        'dst_name': dst_name,
        'dir': dir,
        'delay': delay,
        'defaults': defaults,
        'optional': optional,
        'add_columns': add_columns,
        'no_tz': no_tz,
        'encoding': encoding,
        'delimiter': delimiter,
        'decimal_separator': decimal_separator,
        'quotechar': quotechar,
        'nullval': nullval,
        'excel': excel,
    }

    if tabular is None:
        tabular = headers is not None or dst == 'tab' or dst == 'csv' or (isinstance(dst, (str,os.PathLike)) and is_tabular_path(dst))
    
    if tabular:
        if data is None or isinstance(data, (str,dict)) or not hasattr(data, '__iter__'):
            raise TypeError(f"data: {type(data).__name__} (not iterable)")

        with tabular_dumper(dst, **extended_kwargs) as t:
            for elem in data:
                t.dump(elem)
    
    else:
        dump_object(data, dst=dst, **extended_kwargs)


def is_tabular_path(path: str|os.PathLike):
    from zut import files
    from zut.excel import is_excel_path
    
    if path is None:
        return False

    if isinstance(path, os.PathLike):
        path = str(path)
    elif not isinstance(path, str):
        raise TypeError(f"path: {type(path).__name__}")
    
    _, ext = files.splitext(path)
    return ext.lower() in {'.csv', '.xlsx'} or is_excel_path(path)


def dump_object(data,
                dst: str|os.PathLike|TextIO|None = None,
                *,
                append = False,
                archivate: bool|str|os.PathLike|None = None,
                title: str|bool|None = None,              
                interval: float|int|bool|None = None,
                dst_name: str|bool = True,
                dir: str|os.PathLike|None = None,
                indent: int = 2,
                sort_keys: bool = False,
                **kwargs):
    """
    Export any data object, in JSON format, or using pretty print.
    """
    from zut import files
    
    if dst == os.devnull: # Do nothing
        return
    
    if isinstance(data, GeneratorType):
        data = [elem for elem in data]

    use_pprint = False
    if dst is None or dst == 'pprint': # Display on stdout using pretty print
        use_pprint = True
        dst = sys.stdout
        if dst_name is True:
            dst_name = '<stdout>'
    elif dst == 'json': # Display on stdout using JSON
        dst = sys.stdout
        if dst_name is True:
            dst_name = '<stdout>'
    elif isinstance(dst, (str,os.PathLike)):
        dst = files.in_dir(dst, dir=dir, title=title, **kwargs)
        if archivate:
            files.archivate(dst, archive_dir=archivate, missing_ok=True)

        if not append:
            files.remove(dst, missing_ok=True)
        
        parent = files.dirname(dst)
        if parent:
            files.makedirs(parent, exist_ok=True)

        if dst_name is True:
            dst_name = dst
    else:
        raise TypeError(f"dst: {type(dst).__name__}")

    if interval is None:
        interval = False
    elif interval is True:
        interval = 1.0 #ROADMAP: auto growing from 1.0 to 60.0
    t0 = time_ns() if interval is not False else None
    _logger.log(logging.INFO if title else logging.DEBUG, f"Dump{f' {title}' if title and title is not True else ''}{f' to {dst_name}' if dst_name else ''} …")

    if use_pprint:        
        pprint(data, sort_dicts=sort_keys, stream=dst) # type: ignore
    
    else:
        with nullcontext(dst) if isinstance(dst, IOBase) else files.open(dst, 'w', encoding='utf-8') as fp: # type: ignore
            json.dump(data, fp=fp, indent=indent, cls=ExtendedJSONEncoder, ensure_ascii=False, sort_keys=sort_keys)
            fp.write('\n')

    message = f"Dumped{f' {title}' if title and title is not True else ''}{f' to {dst_name}' if dst_name else ''}"
    if t0 is not None:
        seconds = (time_ns() - t0) / 1E9
        message += f" in {seconds:,.3f} seconds" if seconds < 60 else f" in {timedelta(seconds=int(seconds))}"
    _logger.log(logging.INFO if title else logging.DEBUG, message)


def tabular_dumper(dst: str|os.PathLike|TextIO|Db|Connection|None = None,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[str|Header]|None = None,
                 append = False,
                 archivate: bool|str|os.PathLike|None = None,
                 title: str|bool|None = None,
                 interval: float|int|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|os.PathLike|None = None,
                 delay: bool = False,
                 defaults: dict[str,Any]|None = None,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,                 
                 add_columns: bool|None = None,
                 no_tz: bool|tzinfo|str|None = None,
                 # TabDumper and CsvDumper options
                 encoding = 'utf-8-sig',
                 # CsvDumper options
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,
                 quotechar = '"',
                 nullval: str|None = None,
                 excel: bool|None = None,
                 # DbDumper options
                 table: str|tuple|None = None,
                 add_autoincrement_pk: bool|str = False,
                 # Destination mask values and other options
                 **kwargs) -> TabularDumper:
    """
    Export tabular data, in CSV format, or using `tabulate`.

    Options:

    - `delay`: if True, wait the end of the context to determine headers (based on appended dictionnaries) and start outputing. This only applies if we're not appending to an existing file.

    - `optional`: list of header names that are expected but might not be given as keys of the first appended dict row.

    - `no_tz`: transform all aware (= with timezone info) datetimes into naive (= timezone info removed), expressing them in the given timezone (or in the local timezone if the value is True or 'localtime').
      By default, this option is set for excel exports (via ExcelDumper or CsvDumper with excel option).

    NOTE: empty strings are written with double quotes ("") in order to distinguish them from NULL values (written as an
    unquoted string). This is compatible with PostgreSQL's COPY FROM command.
    """
    from zut.db import Db, DbDumper, get_db_class
    from zut.excel import ExcelDumper, is_excel_path

    extended_kwargs = {
        'headers': headers,
        'append': append,
        'archivate': archivate,
        'title': title,
        'interval': interval,
        'dst_name': dst_name,
        'dir': dir,
        'delay': delay,
        'defaults': defaults,
        'optional': optional,
        'add_columns': add_columns,
        'no_tz': no_tz,
        'encoding': encoding,
        'delimiter': delimiter,
        'decimal_separator': decimal_separator,
        'quotechar': quotechar,
        'nullval': nullval,
        'excel': excel,
        'table': table,
        'add_autoincrement_pk': add_autoincrement_pk,
        **kwargs
    }

    if dst == os.devnull: # Do nothing
        return NoopDumper()
    
    elif dst is None or dst == 'tab' or dst == 'csv': # Display on stdout using tabulate or CSV
        if (dst is None or dst == 'tab') and not TabDumper.missing_dependency:
            return TabDumper(sys.stdout, **extended_kwargs)
        else:
            return CsvDumper(sys.stdout, **extended_kwargs)
        
    elif is_excel_path(dst, accept_table_suffix=True):
        return ExcelDumper(dst, **extended_kwargs)
    
    else:
        if isinstance(dst, Db):
            return dst.dumper(**extended_kwargs)
        else:
            if get_db_class(dst):
                return DbDumper(dst, **extended_kwargs)

        # By default: CSV
        return CsvDumper(dst, **extended_kwargs)


class TabularDumper(Generic[T]):
    missing_dependency: str|None = None
    foreign_id_keys = {'id', 'pk', 'ref'}
    foreign_name_keys = {'name', 'title', 'text', '__str__'}
    dst: T
    
    def __init__(self,
                 dst: T,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[Header|Any]|None = None,
                 append = False,
                 archivate: bool|str|os.PathLike|None = None,
                 title: str|bool|None = None,
                 interval: float|int|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|os.PathLike|Literal[False]|None = None,
                 delay: bool = False,
                 defaults: dict[str,Any]|None = None,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,
                 add_columns: bool|None = None,
                 no_tz: bool|tzinfo|str|None = None,
                 # Destination mask values
                 **kwargs):
        
        if self.missing_dependency:
            raise ValueError(f"Cannot use {type(self).__name__} (missing {self.missing_dependency} dependency)")

        self._logger = get_logger(f'{__name__}.{self.__class__.__qualname__}')
        
        self.defaults = defaults
        self.defaults_to_append = None
        
        self._headers: list[Header|None]|None = None
        self._headers_mask = None
        self._opened = False
        if headers is not None:
            if '*' in headers:
                self._headers_mask = headers
            else:
                self.headers = headers

        self.append = append
        self.archivate = archivate # managed by each subclass
        
        self.delayed: list[Iterable|dict] = [] if delay else None

        self.optional = ('*' if delay else []) if optional is None else '*' if optional == '*' or optional is True else [optional] if isinstance(optional, str) else optional        
        self.add_columns = add_columns
        
        self.count = 0
        """ Number of rows added to the dumper. """

        self.exported_count = 0
        """ Number of rows actually exported. """

        if isinstance(dst, IOBase):
            self.dst = dst
            self.dst_name = getattr(dst, 'name', f'<{type(dst).__name__}>') if dst_name is True else dst_name
        elif isinstance(dst, (str,os.PathLike)):
            if dir is not False:
                from zut import files
                self.dst = files.in_dir(dst, dir, title=title, **kwargs)
            else:
                self.dst = Path(dst) if isinstance(dst, str) else dst
            self.dst_name = self.dst if dst_name is True else dst_name
        else:
            self.dst = dst
            self.dst_name = str(self.dst) if dst_name is True else dst_name

        if isinstance(no_tz, str):
            no_tz = no_tz if no_tz == 'localtime' else parse_tz(no_tz)
        self.no_tz = no_tz

        self.title = title
        if interval is None:
            interval = False
        elif interval is True:
            interval = 1.0 #ROADMAP: auto growing from 1.0 to 60.0
        self._interval = interval
        self._t0 = None        
        self._next_t = None
        self._closed = False

        # Manage existing headers
        self._combined_headers: list[Header]|None = None
        """ All headers, including those collected from dict rows, starting with previously existing headers, in order. Stays None if target headers exactly match existing headers. """
        self._combined_indexes: list[int] = None
        """ Indexes of row values (for rows appended as iterables) within combined headers. """
        self._existing_headers: list[Header]|None = None

    def __enter__(self):
        if self._interval is not False:
            self._t0 = time_ns()                
            self._next_t = self._t0 + 1E9 * self._interval
        self._logger.log(logging.INFO if self.title else logging.DEBUG, f"Dump{f' {self.title}' if self.title and self.title is not True else ''}{f' to {self.dst_name}' if self.dst_name else ''} …")

        if self._headers is not None:
            self._ensure_opened()

        return self
    
    def __exit__(self, exc_type = None, exc_value = None, exc_traceback = None):
        self.close()

    @property
    def headers(self):
        return self._headers
    
    @headers.setter
    def headers(self, headers: Iterable[Header|Any]):
        if self._headers is not None:
            raise ValueError("headers already set")
        self._headers = [Header(header) if not isinstance(header, Header) else header for header in headers]
        if self.defaults:
            self.defaults_to_append = []
            for name in self.defaults:
                if not any(header.name == name for header in self._headers):
                    self.defaults_to_append.append(name)
                    self._headers.append(Header(name))

    def open(self) -> list[Header]|None:
        """
        Called at first exported row, before headers are analyzed.
        
        Return list of existing headers, or None if file/headers must be created.
        """
        return None
    
    def export_headers(self, headers: list[Header]):
        """
        Called at first exported row, if there are no pre-existing headers. 
        """
        pass
    
    def new_headers(self, headers: list[Header]) -> bool|None:
        """
        Called at first exported row, if there are headers that did not exist in pre-existing headers, and for dict rows
        having additional keys (if `additional_headers` is True).

        Return `None` to prevent adding any new header, `False` to allow new columns but without a header name, and
        `True` to allow updating header names.
        """
        return False

    def export(self, row: list):
        """
        Called for each exported row. Row has been prepared and is a list, with values already converted if necessary.
        Rows are already reordered to match order of existing rows (if any), i.e. they match `self._combined_headers` (if defined).
        """
        pass

    def flush(self):
        if self.delayed:
            if self._headers is None:
                self.headers = self._get_headers_from_all_dict_rows(self.delayed)

            self._ensure_opened()
            for row in self.delayed:
                self._convert_and_export_row(row)
            
            self.delayed = []

    def close(self):
        """
        Called at exit.
        """
        if self._closed:
            return

        self.flush()

        message = f"{self.count:,}{f' {self.title}' if self.title is not True else ''} rows exported{f' to {self.dst_name}' if self.dst_name else ''}"
        if self._t0 is not None:
            seconds = (time_ns() - self._t0) / 1E9
            message += f" in {seconds:,.3f} seconds" if seconds < 60 else f" in {timedelta(seconds=int(seconds))}"
        self._logger.log(logging.INFO if self.title else logging.DEBUG, message)
        
        self._closed = True

    def dump(self, row: Iterable|dict|RowMixin):
        if not isinstance(row, (list,tuple,dict,RowMixin)):
            if row is None or isinstance(row, (str,os.PathLike)) or not (hasattr(row, '__iter__') or (hasattr(row, '__len__') and hasattr(row, '__getitem__'))):
                raise TypeError(f"row: {type(row).__name__}")

        self.count += 1

        if self.delayed is not None:
            self.delayed.append(row)
        else:
            if self.count == 1:
                if self._headers is None and isinstance(row, (dict,RowMixin)):
                    self.headers = self._get_headers_from_first_dict_row(row)                
                self._ensure_opened()
            self._convert_and_export_row(row)

        if self._next_t is not None:
            t = time_ns()
            if t >= self._next_t:
                _logger.info(f"{self.count:,}{f' {self.title}' if self.title and self.title is not True else ''} rows dumped …")
                self._next_t = t + 1E9 * self._interval

    def _ensure_opened(self):
        if self._opened:
            return
    
        self._existing_headers = self.open()
        if self._headers is None:
            return

        if self._existing_headers is None:
            self.export_headers(self._headers)        
        else:
            header_names = [header.name for header in self._headers]
            existing_names = [header.name for header in self._existing_headers]

            if header_names != existing_names:
                self._combined_headers = self._existing_headers if self._existing_headers is not None else []
                self._combined_indexes = []
                new_headers: dict[int,Header] = {}               

                for i, header in enumerate(self._headers):
                    try:
                        index = existing_names.index(header.name)
                        self._combined_headers[index] = header
                        self._combined_indexes.append(index)
                    except ValueError:
                        new_headers[i] = header
                        self._combined_indexes.append(None)

                if new_headers:
                    names = ', '.join(f'"{header.name}"' for header in new_headers.values())

                    if self.add_columns is False:
                        self._logger.log(logging.DEBUG, f"New header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
                    else:
                        result = self.new_headers(list(new_headers.values()))
                        if result is None:
                            self._logger.warning(f"New header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
                        else:
                            next_index = len(self._existing_headers)

                            if result:
                                self._logger.log(logging.DEBUG if self.add_columns is True else logging.WARNING, f"New header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} added")
                            else:
                                col_min = next_index + 1
                                col_max = col_min + len(new_headers) - 1
                                self._logger.log(logging.DEBUG if self.add_columns is True else logging.WARNING, f"New header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} {col_min}{'-'+str(col_max) if len(new_headers) > 1 else ''} added without a column name")
                            
                            for i, header in new_headers.items():
                                self._combined_headers.append(header)
                                self._combined_indexes[i] = next_index
                                next_index += 1

        self._opened = True

    def _convert_and_export_row(self, row: Iterable|dict|RowMixin):
        self.exported_count += 1

        if isinstance(row, RowMixin):
            if self._headers is not None:
                combined_row = self._dict_to_combined_row(row.as_dict())
            else:
                combined_row = self._iterable_to_combined_row(row)
        elif isinstance(row, dict):
            combined_row = self._dict_to_combined_row(row)
        else:            
            combined_row = self._iterable_to_combined_row(row)

        converted_row = [self._convert_value(value) for value in combined_row]
        self.export(converted_row)

    def _iterable_to_combined_row(self, row: Iterable):
        if self.defaults_to_append:
            for name in self.defaults_to_append:
                row += [self.defaults[name]]
        
        if self._headers is not None:
            expected = len(self._headers)
            actual = len(row)
            if actual < expected:
                self._logger.warning(f"Row {self.exported_count}: only {actual} column{'s' if actual > 1 else ''} (expected {expected} column{'s' if expected > 1 else ''}): null values will be added")
                row += [None] * (expected - actual)
            elif actual > expected:                    
                self._logger.warning(f"Row {self.exported_count}: too many column{'s' if actual > 1 else ''}: {actual} (expected {expected} column{'s' if expected > 1 else ''}): additional values will be discarded")
                row = row[:expected]
                
        if self._combined_indexes is None:
            return row
        
        combined_row = [None] * len(self._combined_headers)
        for i, value in enumerate(row):
            index = self._combined_indexes[i]
            combined_row[index] = value
        
        return combined_row

    def _dict_to_combined_row(self, obj: dict):
        if self._headers is None:
            raise ValueError("Cannot use dict rows when there are no headers")
        
        combined_headers = self._combined_headers if self._combined_headers is not None else self._headers

        combined_row = []
        used_toplevel_keys = set()
        missing_paths = []
        for header in combined_headers:
            used_toplevel_keys.add(header.toplevel_key)
            try:
                value = header.get_value(obj)
            except NotADirectoryError:
                value = obj.get(header.toplevel_key)
                self._logger.warning(f"Row {self.exported_count}: column \"{header.path}\" was not applied to a dictionnary")
            except KeyError as err:
                if self.defaults is not None and header.name in self.defaults:
                    value = self.defaults[header.name]
                else:
                    path = err.args[0]
                    if self.optional != '*' and not path in self.optional and header in self._headers:
                        missing_paths.append(path)
                    value = None
            combined_row.append(value)

        new_keys = [key for key in obj.keys() if not key in used_toplevel_keys]
        if new_keys:
            new_headers = [Header(key=key) for key in new_keys]
            names = ', '.join(f'"{header.name}"' for header in new_headers)

            if self.add_columns is False:
                self._logger.log(logging.DEBUG, f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
            else:
                result = self.new_headers(new_headers)
                if result is None:
                    self._logger.warning(f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
                else:
                    if self._combined_headers is None:
                        self._combined_headers = [header for header in self._headers]

                    if result:
                        self._logger.log(logging.DEBUG if self.add_columns is True else logging.WARNING, f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} added")
                    else:
                        col_min = len(self._combined_headers) + 1
                        col_max = col_min + len(new_headers) - 1
                        self._logger.log(logging.DEBUG if self.add_columns is True else logging.WARNING, f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} {col_min}{'-'+str(col_max) if len(new_headers) > 1 else ''} added without a column name")
                        
                    for i, key in enumerate(new_keys):
                        header = new_headers[i]
                        self._combined_headers.append(header)                    
                        combined_row.append(obj.get(key))

        if missing_paths:
            missing_paths_str = ', '.join(f'"{k}"' for k in missing_paths)
            self._logger.warning(f"Row {self.exported_count}: dict misses key{'s' if len(missing_paths) > 1 else ''} {missing_paths_str}")

        return combined_row

    def _convert_value(self, value: Any):
        return format_csv_value(value, decimal_separator='.', no_tz=self.no_tz, visual=False)
    
    def _get_headers_from_first_dict_row(self, row: dict|RowMixin):
        return self._get_headers_from_all_dict_rows([row])

    def _get_headers_from_all_dict_rows(self, rows: Iterable[Iterable|dict|RowMixin]):
        headers: list[Header] = []
        header_keys = []
        remaining_optionals = [*self.optional] if isinstance(self.optional, list) else None

        def insert_header(header: Header, following_keys: list[str]):
            # Try to keep the global order of headers: insert just before the first existing that we know is after the given header
            pos = len(headers)
            for following_key in following_keys:
                try:
                    pos = header_keys.index([following_key])
                    break
                except ValueError:
                    continue

            headers.insert(pos, header)

            # Update other caches
            header_keys.insert(pos, header.key)
            if remaining_optionals:
                try:
                    remaining_optionals.remove(header.name)
                except:
                    pass
            
        for row in rows:
            if isinstance(row, RowMixin):
                row = row.as_dict()
            elif not isinstance(row, dict):
                continue

            dict_keys = list(row.keys())
            for i, dict_key in enumerate(dict_keys):
                if [dict_key] in header_keys:
                    continue
                
                value = row[dict_key]
                if isinstance(value, dict):
                    dict_headers = self._get_dict_value_foreign_headers(dict_key, value.keys())
                    for header in dict_headers:
                        if not header.key in header_keys:
                            insert_header(header, dict_keys[i+1:])
                else:
                    insert_header(Header(dict_key), dict_keys[i+1:])

        if remaining_optionals:
            for optional in remaining_optionals:
                headers.append(Header(name=optional))

        if self._headers_mask:
            reorganized_headers = []
            reorganized_header_names = set()
            for header in self._headers_mask:
                if header == '*':
                    for header in headers:
                        if not header.name in reorganized_header_names:
                            reorganized_headers.append(header)
                            reorganized_header_names.add(header.name)
                else:
                    if not isinstance(header, Header):
                        header = Header(header)
                    reorganized_headers.append(header)
                    reorganized_header_names.add(header.name)
            headers = reorganized_headers
        
        return headers

    @classmethod
    def _get_dict_value_foreign_headers(cls, foreign_name: str, dict_keys):
        """
        Inline the keys of a dictionnary value as top-level header keys, if it is considered as a foreign key:
        - First key is either 'id', 'pk', or 'ref'.
        - optionaly, second key is either 'name', 'title', 'text' or '__str__'.
        - If there are other keys, an additional column named as the original one is kept.
        """
        if not isinstance(foreign_name, str):
            yield Header(foreign_name)
            return
        
        it = iter(dict_keys)

        try:
            id_dict_key = next(it)
        except StopIteration:
            yield Header(foreign_name)
            return
        
        if id_dict_key in cls.foreign_id_keys:
            yield Header(f'{foreign_name}_{id_dict_key}', key=[foreign_name, id_dict_key])
        else:
            yield Header(foreign_name)
            return
        
        try:
            name_dict_key = next(it)
        except StopIteration:
            return
                
        remaining = False
        if name_dict_key in cls.foreign_name_keys:
            yield Header(f'{foreign_name}_{name_dict_key}', key=[foreign_name, name_dict_key])
        else:
            remaining = True

        if not remaining:
            try:
                next(it)
                remaining = True
            except StopIteration:
                pass

        if remaining:
            yield Header(foreign_name)


class NoopDumper(TabularDumper[str]):   
    def __init__(self, *args, **kwargs):
        super().__init__(os.devnull)


class TabDumper(TabularDumper[Union[str,os.PathLike,TextIO]]):
    try:
        from tabulate import tabulate
        missing_dependency = None
    except ModuleNotFoundError:
        missing_dependency = "Missing tabulate dependency"

    def __init__(self,
                 dst: str|os.PathLike|TextIO|None = None,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[Header|Any]|None = None,
                 append = False,
                 archivate: bool|str|os.PathLike|None = None,
                 title: str|bool|None = None,
                 interval: float|int|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|os.PathLike = None,
                 delay = False,
                 defaults: dict[str,Any] = None,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,
                 add_columns: bool|None = None,
                 no_tz: bool|tzinfo|str = False,
                 # TabDumper and CsvDumper options
                 encoding = 'utf-8-sig',
                 # Destination mask values
                 **kwargs):
        
        if dst is None:
            dst = sys.stdout
        
        super().__init__(dst, headers=headers, append=append, archivate=archivate, title=title, interval=interval, dst_name=dst_name, dir=dir, delay=delay, defaults=defaults, optional=optional, add_columns=add_columns, no_tz=no_tz, **kwargs)

        self.encoding = 'utf-8' if encoding is None else encoding

        self._rows: list[Iterable] = []

    def close(self):
        super().close()

        from tabulate import tabulate
        if self._headers:
            result = tabulate(self._rows, headers=self._headers)
        else:
            result = tabulate(self._rows)

        file = None
        try:
            if isinstance(self.dst, IOBase):
                file = self.dst
            else:
                from zut import files
                
                if self.archivate:
                    files.archivate(self.dst, archive_dir=self.archivate, missing_ok=True)

                if not self.append:
                    files.remove(self.dst, missing_ok=True)

                parent = files.dirname(self.dst)
                if parent and not files.exists(parent):
                    files.makedirs(parent)

                file = files.open(self.dst, 'a', encoding=self.encoding, newline='')
                skip_utf8_bom(file, self.encoding)

            file.write(result)
            file.write('\n')
            file.flush()
        finally:
            if file and not isinstance(self.dst, IOBase):
                file.close()

    def export(self, row: list):
        self._rows.append(row)

    def _convert_value(self, value: Any):
        return format_csv_value(value, decimal_separator='.', no_tz=self.no_tz, visual=True)


class CsvDumper(TabularDumper[Union[str,os.PathLike,TextIO]]):
    def __init__(self,
                 dst: str|os.PathLike|TextIO|None = None,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[str|Header]|None = None,
                 append = False,
                 archivate: bool|str|os.PathLike|None = None,
                 title: str|bool|None = None,
                 interval: float|int|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|os.PathLike|None = None,
                 delay = False,
                 defaults: dict[str,Any]|None = None,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,
                 add_columns: bool|None = None,
                 no_tz: bool|tzinfo|str|None = None,
                 # TabDumper and CsvDumper options
                 encoding = 'utf-8-sig',
                 # CsvDumper options
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,                 
                 quotechar = '"',
                 nullval: str|None = None,
                 excel: bool|None = None,
                 # Destination mask values
                 **kwargs):
        
        if excel is None:
            excel = os.environ.get('CSV_EXCEL', '').lower() in {'1', 'yes', 'true', 'on'}
        
        if no_tz is None:
            no_tz = True if excel else False

        if dst is None:
            dst = sys.stdout
        
        super().__init__(dst, headers=headers, append=append, archivate=archivate, title=title, interval=interval, dst_name=dst_name, dir=dir, delay=delay, defaults=defaults, optional=optional, add_columns=add_columns, no_tz=no_tz, **kwargs)

        self.encoding = encoding
        self.delimiter = delimiter
        self.decimal_separator = decimal_separator
        self.quotechar = quotechar
        self.nullval = nullval
        self.excel = excel

        if isinstance(self.dst, IOBase):
            self.file = self.dst
        elif isinstance(self.dst, (str,os.PathLike)):
            self.file = None # will be opened when necessary
        else:
            raise TypeError(f"dst: {type(self.dst).__name__}")

        # Manage existing headers
        self._need_newline = False

    def open(self):
        if not self.file:
            from zut import files
            
            if self.archivate:
                files.archivate(self.dst, archive_dir=self.archivate, missing_ok=True)

            if not self.append:
                files.remove(self.dst, missing_ok=True)

            parent = files.dirname(self.dst)
            if parent and not files.exists(parent):
                files.makedirs(parent)

            if files.exists(self.dst):
                columns, delimiter, ends_with_newline = examine_csv_file(self.dst, encoding=self.encoding, delimiter=self.delimiter, quotechar=self.quotechar, force_delimiter=False, need_ends_with_newline=True)
                if columns is not None:
                    existing_headers = [Header(column) for column in columns]
                    if not ends_with_newline:
                        self._need_newline = True
                else:
                    existing_headers = None
                if not self.delimiter:
                    self.delimiter = delimiter or get_default_csv_delimiter()
            else:
                existing_headers = None
            
            self.file = files.open(self.dst, 'a', encoding=self.encoding, newline='')
            skip_utf8_bom(self.file, self.encoding)

        else:
            existing_headers = None 

        if not self.delimiter:
            self.delimiter = get_default_csv_delimiter()
        if not self.decimal_separator:
            self.decimal_separator = get_default_decimal_separator(csv_delimiter=self.delimiter)

        return existing_headers

    def export_headers(self, headers: list[Header]):
        self.export([header.name for header in headers])

    def close(self):
        super().close()

        if self.file and not isinstance(self.dst, IOBase):
            self.file.close()
            self.file = None

    def export(self, row: list):        
        if self._need_newline:
            # CSV newline is standardized for all platforms to '\r\n' as recommended by RFC 4180 (see https://www.rfc-editor.org/rfc/rfc4180#section-2)
            # NOTE: when writing to piped stdout or stderr on Windows, we need to use '\n' (which will be converted to '\r\n' automatically) otherwise an additional empty newline is added
            if sys.platform == 'win32' and (self.dst == sys.stdout or self.dst == sys.stderr) and not self.dst.isatty():
                self.file.write('\n') 
            else:
                self.file.write('\r\n') 

        for i, value in enumerate(row):
            if i > 0:
                self.file.write(self.delimiter)
            self.file.write(escape_csv_value(value, delimiter=self.delimiter, quotechar=self.quotechar, nullval=self.nullval))

        self.file.flush()

        self._need_newline = True

    def _convert_value(self, value):
        return format_csv_value(value, delimiter=self.delimiter, decimal_separator=self.decimal_separator, no_tz=self.no_tz, no_microseconds=self.excel)

#endregion


#region Load

def load_tabular(src: str|os.PathLike|TextIO, *,
             headers: Iterable[str|Header]|None = None,
             title: str|bool|None = None,             
             interval: float|int|bool|None = None,
             src_name: str|bool = True,
             dir: str|os.PathLike|Literal[False]|None = None,
             # CSV parameters
             encoding = 'utf-8',
             delimiter: str|None = None,
             decimal_separator: str|None = None,
             quotechar = '"',
             nullval: str|None = None,
             no_headers: bool|None = None,
             **kwargs):
    """
    See also db.load_from_csv() for direct import in database.
    """
    
    data: list[ListRow] = []
    with tabular_loader(src, headers=headers, title=title, interval=interval, src_name=src_name, dir=dir, encoding=encoding, delimiter=delimiter, decimal_separator=decimal_separator, quotechar=quotechar, nullval=nullval, no_headers=no_headers, **kwargs) as table:
        for row in table:
            data.append(row)
    return data


def tabular_loader(src: str|os.PathLike|TextIO, *,
             headers: Iterable[str|Header]|None = None,
             title: str|bool|None = None,
             interval: float|int|bool|None = None,
             src_name: str|bool = True,
             dir: str|os.PathLike|Literal[False]|None = None,
             # CSV parameters
             encoding = 'utf-8',
             delimiter: str|None = None,
             decimal_separator: str|None = None,
             quotechar = '"',
             nullval: str|None = None,
             no_headers: bool|None = None,
             **kwargs):

    # ROADMAP: Excel loader
    return CsvLoader(src=src, headers=headers, title=title, interval=interval, src_name=src_name, dir=dir, encoding=encoding, delimiter=delimiter, decimal_separator=decimal_separator, quotechar=quotechar, nullval=nullval, no_headers=no_headers, **kwargs)
   

class CsvLoader:
    def __init__(self, src: str|os.PathLike|TextIO, *,
                 headers: Iterable[str|Header]|None = None,
                 title: str|bool|None = None,
                 interval: float|int|bool|None = None,
                 src_name: str|bool = True,
                 dir: str|os.PathLike|Literal[False]|None = None,
                 add_columns: bool|None = None,
                 # CSV parameters
                 encoding = 'utf-8',
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,
                 quotechar = '"',
                 nullval: str|None = None,
                 no_headers: bool = False,
                 **kwargs):
        
        self.headers: list[Header]|None = None
        self._columns: tuple[str]|None = None
        self._column_indexes: dict[str, int]|None = None

        self.add_columns = add_columns
        if headers is not None:
            self.headers = []
            for header in headers:
                if header == '*':
                    self.add_columns = True
                else:
                    self.headers.append(Header(header) if not isinstance(header, Header) else header)
        
        if isinstance(src, IOBase):
            self.src = src
            self.src_name = getattr(src, 'name', f'<{type(src).__name__}>') if src_name is True else src_name
            self.file = src
        elif isinstance(src, (str,os.PathLike)):
            if dir is not False:
                from zut import files
                self.src = files.in_dir(src, dir, title=title, **kwargs)
            else:
                self.src = Path(src) if isinstance(src, str) else src
            self.src_name = self.src if src_name is True else src_name
            self.file = None # will be opened when necessary
        else:
            raise TypeError(f"src: {type(src).__name__}")

        self.count = 0
        
        self._logger = get_logger(f'{__name__}.{self.__class__.__qualname__}')
        self.title = title
        if interval is None:
            interval = False
        elif interval is True:
            interval = 1.0 #ROADMAP: auto growing from 1.0 to 60.0
        self.interval = interval
        self._t0 = None

        self.encoding = encoding
        self.delimiter = delimiter
        self.decimal_separator = decimal_separator
        self.quotechar = quotechar
        self.nullval = nullval
        self.no_headers = no_headers

        # Set during prepare()
        self.src_columns: list[str]|None = None
        self._src_values_reorder: list[tuple[int|None,Header|None]]|None = None
        self._reader: _csv.Reader|None = None
            
    @property
    def columns(self) -> tuple[str]:
        if self._columns is None:
            self._columns = tuple(header.name for header in self.headers) # type: ignore
        return self._columns # type: ignore

    def get_column_index(self, name: str) -> int:
        if self._column_indexes is None:
            self._column_indexes = {n: i for i, n in enumerate(self.columns)}
        return self._column_indexes[name]

    def __enter__(self):
        return self
    
    def __exit__(self, exc_type = None, exc_value = None, exc_traceback = None):
        self.close()

    def prepare(self):
        # NOTE: QUOTE_STRINGS is not interpreted correctly by csv reader in Python 3.12 - https://github.com/python/cpython/issues/116633
        from zut import files

        if not self.file:
            self.file = files.open(self.src, 'r', encoding=self.encoding, newline='')

        _, delimiter, _ = examine_csv_file(self.file, encoding=self.encoding, delimiter=self.delimiter, quotechar=self.quotechar, force_delimiter=False)
        if not self.delimiter:
            self.delimiter = delimiter or get_default_csv_delimiter()
        
        if sys.version_info >= (3, 13) and self.nullval is None: # NOTE: QUOTE_NOTNULL does not seem to work correctly on Python 3.12
            self._reader = csv.reader(self.file, delimiter=self.delimiter, quotechar=self.quotechar, doublequote=True, quoting=csv.QUOTE_NOTNULL)
        else:
            self._reader = csv.reader(self.file, delimiter=self.delimiter, quotechar=self.quotechar, doublequote=True)
        
        if not self.no_headers:
            self.src_columns = next(self._reader)

        if self.headers is None:
            if self.no_headers:
                raise ValueError("Headers must be explicitely given if they are not in source")
            else:
                self.headers = [Header(name) for name in self.src_columns]
                self._columns = None # invalidate columns
                self._column_indexes = None # invalidate columns
        else:
            if self.no_headers:
                self.src_columns = [header.name for header in self.headers]
            else:
                warn_not_found = []
                for header in self.headers:
                    if not header.name in self.src_columns:
                        warn_not_found.append(f'"{header}"')
                if warn_not_found:
                    self._logger.warning(f"Header{'s' if len(warn_not_found) > 1 else ''} {', '.join(warn_not_found)} not found in source")

                warn_additional = []
                reorder = [(None,None)] * len(self.src_columns)
                reorder_required = False
                for src_i, column in enumerate(self.src_columns):
                    index = None
                    header = None
                    for i, a_header in enumerate(self.headers):
                        if a_header.name == column:
                            index = i
                            header = a_header
                            break
                    if index is not None:
                        reorder[src_i] = index, header
                        if index != src_i or (header.type and header.type != str):
                            reorder_required = True
                    else:
                        if not self.add_columns:
                            warn_additional.append(f'"{column}"')
                            reorder_required = True
                        else:
                            header = Header(column)
                            self.headers.append(header)
                            self._columns = None # invalidate columns
                            self._column_indexes = None # invalidate columns

                            index = len(self.headers) - 1
                            reorder[src_i] = index, header
                            if index != src_i:
                                reorder_required = True

                if reorder_required:
                    self._src_values_reorder = reorder
                
                if warn_additional:
                    self._logger.warning(f"Additional column{'s' if len(warn_additional) > 1 else ''} {', '.join(warn_additional)} found in source")
    
    def close(self):
        message = f"{self.count:,}{f' {self.title}' if self.title and self.title is not True else ''} rows imported{f' from {self.src_name}' if self.src_name else ''}"
        if self._t0 is not None:
            seconds = (time_ns() - self._t0) / 1E9
            message += f" in {seconds:,.3f} seconds" if seconds < 60 else f" in {timedelta(seconds=int(seconds))}"
        self._logger.log(logging.INFO if self.title else logging.DEBUG, message)
    
        if self.file and not isinstance(self.src, IOBase):
            self.file.close()
            self.file = None

    def __iter__(self):
        return self

    def __next__(self):
        if not self._reader:
            self.prepare()
        
        src_values = next(self._reader)
        self.count += 1

        if self.count == 1:
            if self.interval is not False:
                self._t0 = time_ns()
            self._logger.log(logging.INFO if self.title else logging.DEBUG, f"Load{f' {self.title}' if self.title and self.title is not True else ''}{f' from {self.src_name}' if self.src_name else ''} …")
    
        if len(src_values) > len(self.src_columns):
            start_column_num = len(self.src_columns)
            end_column_num = len(src_values) - 1
            self._logger.warning(f"Row {self.count}: unexpected additional column{'s' if end_column_num > start_column_num else ''} {start_column_num}{f'-{end_column_num}' if end_column_num > start_column_num else ''}")

        elif len(src_values) < len(self.src_columns):
            start_column_num = len(src_values)
            end_column_num = len(self.src_columns) - 1
            self._logger.warning(f"Row {self.count}: missing column{'s' if end_column_num > start_column_num else ''} {start_column_num}{f'-{end_column_num}' if end_column_num > start_column_num else ''}")
            while len(src_values) < len(self.src_columns):
                src_values.append(None)

        if self._src_values_reorder:
            values = [None] * len(self.headers)
            for src_i, value in enumerate(src_values):
                if value == self.nullval:
                    value = None
                if src_i >= len(self._src_values_reorder):
                    values.append(value)
                else:
                    index, header = self._src_values_reorder[src_i]
                    if index is not None:
                        if header and header.type:
                            value = convert(value, header.type)
                        values[index] = value
        else:
            if self.nullval is not None:
                values = [None if value == self.nullval else value for value in src_values]
            else:
                values = src_values

        row = ListRow(values)
        row.provider = self
        return row


#endregion


#region Process

@overload
def run_process(cmd: str|os.PathLike|bytes|list[str|os.PathLike|bytes], *, encoding: Literal['bytes'], capture_output: bool|Literal['rstrip','strip']|None = None, check: int|Sequence[int]|bool = False, sudo = False, shell = False, env: dict[str,Any]|None = None, stdout: Literal['disable','raise','warning','error']|None = None, stderr: Literal['disable','raise','warning','error']|None = None, input: str|None = None, logger: logging.Logger|None = None) -> subprocess.CompletedProcess[bytes]:
    ...

@overload
def run_process(cmd: str|os.PathLike|bytes|list[str|os.PathLike|bytes], *, encoding: str|Literal['unknown','bytes']|None = None, capture_output: bool|Literal['rstrip','strip']|None = None, check: int|Sequence[int]|bool = False, sudo = False, shell = False, env: dict[str,Any]|None = None, stdout: Literal['disable','raise','warning','error']|None = None, stderr: Literal['disable','raise','warning','error']|None = None, input: str|None = None, logger: logging.Logger|None = None) -> subprocess.CompletedProcess[str]:
    ...

def run_process(cmd: str|os.PathLike|bytes|list[str|os.PathLike|bytes], *, encoding: str|Literal['unknown','bytes']|None = None, capture_output: bool|Literal['rstrip','strip']|None = None, check: int|Sequence[int]|bool = False, sudo = False, shell = False, env: dict[str,Any]|None = None, stdout: Literal['disable','raise','warning','error']|None = None, stderr: Literal['disable','raise','warning','error']|None = None, input: str|None = None, logger: logging.Logger|None = None) -> subprocess.CompletedProcess:
    if sudo:
        if not is_sudo_available():
            raise SudoNotAvailable()
        if isinstance(cmd, (str,os.PathLike,bytes)):
            cmd = f'sudo {cmd}'
        else:
            cmd = ['sudo', *cmd]

    if capture_output is None:
        capture_output = (stdout and stdout != 'disable') or (stderr and stderr != 'disable')

    cp = subprocess.run(cmd,
                        capture_output=True if capture_output else False,
                        text=encoding not in {'unknown', 'bytes'},
                        encoding=encoding if encoding not in {'unknown', 'bytes'} else None,
                        shell=shell,
                        env=env,
                        stdout=subprocess.DEVNULL if stdout == 'disable' else None,
                        stderr=subprocess.DEVNULL if stderr == 'disable' else None,
                        input=input)
    
    if encoding == 'unknown':
        def parse_unknown_encoding(output: bytes):
            if output is None:
                return None
            try:
                return output.decode('utf-8')
            except UnicodeDecodeError:
                return output.decode('cp1252')
        
        cp.stdout = parse_unknown_encoding(cp.stdout)
        cp.stderr = parse_unknown_encoding(cp.stderr)
    
    return verify_run_process(cp, strip=capture_output if capture_output in ['strip', 'rstrip'] else False, check=check, stdout=stdout, stderr=stderr, logger=logger)


def verify_run_process(cp: subprocess.CompletedProcess, *, strip: bool|Literal['strip','rstrip'] = False, check: int|Sequence[int]|bool = False, stdout: Literal['disable','raise','warning','error'] = None, stderr: Literal['disable','raise','warning','error'] = None, logger: logging.Logger = None) -> subprocess.CompletedProcess:
    if strip:
        cp.stdout = _strip_data(cp.stdout, strip)
        cp.stderr = _strip_data(cp.stderr, strip)
    
    invalid_returncode = False
    if check:
        if check is True:
            check = 0
        invalid_returncode = not (cp.returncode in check if not isinstance(check, int) else cp.returncode == check)

    invalid_stdout = stdout == 'raise' and cp.stdout
    invalid_stderr = stderr == 'raise' and cp.stderr

    if cp.stdout:
        level = None
        if stdout == 'warning':
            level = logging.WARNING
        elif stdout == 'error':
            level = logging.ERROR
        if level:
            (logger or _logger).log(level, f"{Color.PURPLE}[stdout]{Color.RESET} %s", stdout)
            
    if cp.stderr:
        level = None
        if stderr == 'warning':
            level = logging.WARNING
        elif stderr == 'error':
            level = logging.ERROR
        if level:
            (logger or _logger).log(level, f"{Color.PURPLE}[stderr]{Color.RESET} %s", stderr)

    if invalid_returncode or invalid_stdout or invalid_stderr:
        raise RunProcessError(cp.returncode, cp.args, cp.stdout, cp.stderr)    
    return cp


def run_process_callback(cmd: str|os.PathLike|bytes|list[str|os.PathLike|bytes], *, encoding: str|Literal['unknown','bytes']|None = None, shell = False, env: dict[str,Any]|None = None, on_stdout: Callable[[str|bytes],None]|None = None, on_stderr: Callable[[str|bytes],None]|None = None, strip: bool|Literal['strip','rstrip'] = False, strip_stderr: bool|Literal['strip','rstrip']|None = None) -> int:
    """
    Run a process, using `on_stdout` and/or `on_stderr` in other threads when data is available.
    """
    # See: https://stackoverflow.com/a/60777270
    queue = Queue()

    def enqueue_stream(stream: TextIO, source: str):
        for data in iter(stream.readline, ''):
            queue.put((source, data))
        stream.close()

    def enqueue_process(proc: subprocess.Popen):
        returncode = proc.wait()
        queue.put(('process', returncode))
    
    proc = subprocess.Popen(cmd,
                        text=encoding not in {'unknown', 'bytes'},
                        encoding=encoding if encoding not in {'unknown', 'bytes'} else None,
                        shell=shell,
                        env=env,
                        stdout=subprocess.PIPE if on_stdout else subprocess.DEVNULL,
                        stderr=subprocess.PIPE if on_stderr else subprocess.DEVNULL)
    
    if on_stdout:
        Thread(target=enqueue_stream, args=[proc.stdout, 'stdout'], daemon=True).start()
    if on_stderr:
        Thread(target=enqueue_stream, args=[proc.stderr, 'stderr'], daemon=True).start()
    Thread(target=enqueue_process, args=[proc], daemon=True).start()

    if strip_stderr is None:
        strip_stderr = strip
    
    while True:
        source, data = queue.get()
        if source == 'stdout':
            on_stdout(_strip_data(data, strip))
        elif source == 'stderr':
            on_stderr(_strip_data(data, strip_stderr))
        else: # process
            return data # returncode


def _strip_data(data, strip: bool|Literal['strip','rstrip']):
    if not strip:
        return data
    
    if isinstance(data, str):
        if strip == 'rstrip':
            return data.rstrip('\r\n')
        else:
            return data.strip()
    else:
        raise TypeError(f"Cannot strip data of type {type(data).__name__}")         


_sudo_available: bool = None

def is_sudo_available() -> bool:
    global _sudo_available
    if _sudo_available is None:
        if not which('sudo'):
            _sudo_available = False
        else:
            try:
                return_code = subprocess.call(['sudo', 'sh', '-c', 'id -u > /dev/null'])
                _sudo_available = return_code == 0
            except BaseException: # e.g. SIGINT / CTRL+C
                _sudo_available = False
    return _sudo_available


class SudoNotAvailable(subprocess.SubprocessError):
    def __init__(self):
        super().__init__("Sudo is not available")


class RunProcessError(subprocess.CalledProcessError):
    def __init__(self, returncode, cmd, stdout, stderr):
        super().__init__(returncode, cmd, stdout, stderr)
        self.maxlen: int|None = 200
        self._message = None

    def with_maxlen(self, maxlen: int|None):
        self.maxlen = maxlen
        return self

    @property
    def message(self):
        if self._message is None:
            self._message = ''

            if self.returncode and self.returncode < 0:
                try:
                    self._message += "died with %r" % Signals(-self.returncode)
                except ValueError:
                    self._message += "died with unknown signal %d" % -self.returncode
            else:
                self._message += "returned exit code %d" % self.returncode

            if self.output:
                info = self.output[0:self.maxlen] + '…' if self.maxlen is not None and len(self.output) > self.maxlen else self.stdout
                self._message += ('\n' if self._message else '') + f"[stdout] {info}"

            if self.stderr:
                info = self.stderr[0:self.maxlen] + '…' if self.maxlen is not None and len(self.stderr) > self.maxlen else self.stderr
                self._message += ('\n' if self._message else '') + f"[stderr] {info}"

            self._message = f"Command '{self.cmd}' {self._message}"

        return self._message

    def __str__(self):
        return self.message

#endregion


#region Colors

class Color:
    RESET = '\033[0m'

    BLACK = '\033[0;30m'
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[0;33m'
    BLUE = '\033[0;34m'
    PURPLE = '\033[0;35m'
    CYAN = '\033[0;36m'
    WHITE = '\033[0;37m'
    GRAY = '\033[0;90m'
    BG_RED = '\033[0;41m'

    # Disable coloring if environment variable NO_COLORS is set to 1 or if stderr is piped/redirected
    NO_COLORS = False
    if os.environ.get('NO_COLORS', '').lower() in {'1', 'yes', 'true', 'on'} or not sys.stderr.isatty():
        NO_COLORS = True
        for _ in dir():
            if isinstance(_, str) and _[0] != '_' and _ not in ['DISABLED']:
                locals()[_] = ''

    # Set Windows console in VT mode
    if not NO_COLORS and sys.platform == 'win32':
        _kernel32 = ctypes.windll.kernel32
        _kernel32.SetConsoleMode(_kernel32.GetStdHandle(-11), 7)
        del _kernel32

#endregion


#region Commands

def add_commands(subparsers: _SubParsersAction[ArgumentParser], package: ModuleType|str):
    """
    Add all sub modules of the given package as commands.
    """
    if isinstance(package, str):
        package = import_module(package)
    elif not isinstance(package, ModuleType):
        raise TypeError(f"Invalid argument 'package': not a module")

    package_path = getattr(package, '__path__', None)
    if not package_path:
        raise TypeError(f"Invalid argument 'package': not a package")

    for module_info in iter_modules(package_path):
        if module_info.name.startswith('_'):
            continue # skip

        add_command(subparsers, f'{package.__name__}.{module_info.name}')

    
def add_command(subparsers: _SubParsersAction[ArgumentParser], handle: Callable|ModuleType|str, *, name: str|None = None, doc: str|None = None, help: str|None = None, add_arguments: Callable[[ArgumentParser]]|None = None, parents: ArgumentParser|Sequence[ArgumentParser]|None = None, **defaults):
    """
    Add the given function or module as a command.
    """
    if isinstance(handle, str):
        handle = import_module(handle)

    if isinstance(handle, ModuleType):
        module = handle        
        command_class = getattr(module, 'Command', None)
        if command_class:
            handle = command_class # will be treated later
        else:    
            handle = getattr(module, 'handle', None) # type: ignore
            if not handle:
                handle = getattr(module, '_handle', None) # type: ignore
                if not handle:
                    handle_name = name if name else module.__name__.split('.')[-1]
                    handle = getattr(module, handle_name, None) # type: ignore
                    if not handle:
                        raise ValueError(f"Cannot use module {module.__name__} as a command: no attribute named \"Command\", \"handle\" , \"_handle\" or \"{handle_name}\"")
    elif callable(handle):
        module = None
    else:
        raise TypeError(f"Invalid argument 'handle': not a module or a callable")
    
    if isinstance(handle, type): # Command class (e.g. Django management command)
        command_class = handle.__name__.lower()
        command_instance = handle()
        handle = getattr(command_instance, 'handle')
        if not name:
            if command_class.endswith('subcommand') and command_class != 'subcommand':
                name = command_class.removesuffix('subcommand')
            elif command_class.endswith('command') and command_class != 'command':
                name = command_class.removesuffix('command')  
        if not add_arguments:
            add_arguments = getattr(command_instance, 'add_arguments', None)
        if not help:
            help = getattr(command_instance, 'help', None)
        if not doc:
            doc = command_instance.__doc__

    if not name:
        name = getattr(handle, 'name', None)
        if not name:
            if module:
                name = module.__name__.split('.')[-1]
            else:
                name = handle.__name__ # type: ignore

    if not doc:
        doc = getattr(handle, 'doc', None)
        if doc:
            if callable(doc):
                doc = doc()
        else:
            doc = handle.__doc__
            if not doc:
                if module:
                    doc = module.__doc__
                if not doc:
                    doc = help

    if not help:
        help = getattr(handle, 'help', doc)
    
    if not add_arguments:
        add_arguments = getattr(handle, 'add_arguments', None)
        if not add_arguments and module:
            add_arguments = getattr(module, 'add_arguments', None)

    if parents is None:
        parents = []
    elif isinstance(parents, ArgumentParser):
        parents = [parents]

    cmdparser = subparsers.add_parser(name, help=get_help_text(help) if help else None, description=get_description_text(doc) if doc else None, formatter_class=RawTextHelpFormatter, parents=parents)

    if add_arguments:
        add_arguments(cmdparser)

    cmdparser.set_defaults(handle=handle, **defaults)

    return cmdparser


def get_help_text(doc: str):
    if doc is None:
        return None
    
    doc = doc.strip()
    try:
        return doc[0:doc.index('\n')].strip()
    except:
        return doc
    

def get_description_text(doc: str|None):
    if doc is None:
        return None
    
    return dedent(doc)


def get_exit_code(code: Any) -> int:
    if not isinstance(code, int):
        code = 0 if code is None or code is True else 1
    return code


def run_command(handle: ArgumentParser|Callable|dict[str,Any], args: dict|list[str]|None = None, *, default: str|None = None,
                context_builder: Callable[[dict[str,Any],list[str]],AbstractContextManager|Iterable[AbstractContextManager]|None]|None = None,
                additional_args_builders: dict[str,Callable[[],Any]|Callable[[dict[str,Any]],Any]]|None = None):
    """
    Run a command.
    - `handle` may be:
        - An `ArgumentParser` objects: in this case, `run_command` will take care of calling `parse_args` on it.
        - A handle function already parsed before calling this `run_command`.
        - A dictionnary: this is intended for running Django subcommands directly from the keywork arguments of the Django command's `handle(**option)` method.
          Django-specific options (verbosity, settings, pythonpath, traceback, no_color, force_color and skip_checks) are removed.
    - `context_builder`: `(args: dict, handle_parameters: list[str]) -> context manager, list of context managers or None`
    """
    if isinstance(args, list):
        argv = args
        args = {}
    else:
        if args is None:
            args = {}
        argv = sys.argv[1:]
    
    handle_func: Callable|None = None
    remaining_argv = []
    parser: ArgumentParser|None = None
    if isinstance(handle, dict): # Intended for running Django subcommands
        for key, value in handle.items():
            if key == 'handle':
                handle_func = value
            elif key not in {'verbosity', 'settings', 'pythonpath', 'traceback', 'no_color', 'force_color', 'skip_checks'}:
                args[key] = value
    elif isinstance(handle, ArgumentParser):
        parser = handle
        known_args, remaining_argv = parser.parse_known_args(argv)
        known_args = vars(known_args)
        args = {**args, **known_args}
        handle_func = args.pop('handle', None)
    else:
        handle_func = handle
    
    if not handle_func:
        if default and parser:
            known_args = vars(parser.parse_args([default, *argv]))
            args = {**args, **known_args}
            handle_func = args.pop('handle', None)
            if not handle_func:
                _logger.error("Default command handle not found")
                return 1                
        else:
            _logger.error("Missing command")
            return 1
    elif remaining_argv:
        _logger.error(f"Unrecognized arguments: {', '.join(remaining_argv)}")
        return 1

    try:
        r = 1
        with ExitStack() as exit_stack:
            if context_builder or additional_args_builders:
                handle_parameters = list(inspect.signature(handle_func).parameters.keys())
                
                if context_builder:
                    context_managers = context_builder(args, handle_parameters)
                    if context_managers is not None:
                        if isinstance(context_managers, AbstractContextManager):
                            context_managers = [context_managers]
                        for manager in context_managers:
                            exit_stack.enter_context(manager)
                
                if additional_args_builders:
                    for arg in handle_parameters:
                        if not arg in args:
                            builder = additional_args_builders.get(arg)
                            if builder:
                                builder_params_count = len(inspect.signature(builder).parameters)
                                additional_arg = builder(args) if builder_params_count > 0 else builder() # type: ignore
                                if isinstance(additional_arg, AbstractContextManager):
                                    additional_arg = exit_stack.enter_context(additional_arg)
                                args[arg] = additional_arg

            r = handle_func(**args)
        return get_exit_code(r)
    except SimpleError as err:
        _logger.error(str(err))
        return 1
    except KeyboardInterrupt:
        _logger.error("Interrupted")
        return 1
    except BaseException as err:
        message = str(err)
        _logger.exception(f"{type(err).__name__}{f': {message}' if message else ''}")
        return 1


def exec_command(handle: ArgumentParser|Callable|dict[str,Any], args: dict|list[str]|None = None, *, default: str|None = None,
                 context_builder: Callable[[dict[str,Any],list[str]],AbstractContextManager|Iterable[AbstractContextManager]|None]|None = None,
                 additional_args_builders: dict[str,Callable[[],Any]|Callable[[dict[str,Any]],Any]]|None = None):
    """
    - `context_builder`: `(args: dict, handle_parameters: list[str]) -> context manager, list of context managers or None`
    """
    r = run_command(handle, args, default=default, context_builder=context_builder, additional_args_builders=additional_args_builders)
    sys.exit(r)


class SimpleError(ValueError):
    """
    An error that should result to only an error message being printed on the console, without a stack trace.
    """
    def __init__(self, msg: str, *args, **kwargs):
        if args or kwargs:
            msg = msg.format(*args, **kwargs)
        super().__init__(msg)

#endregion


#region Config

_loaded_dotenv_paths = set()

def load_dotenv(path: os.PathLike|str|None = None, *, encoding = 'utf-8', override = False, parents = False):
    """
    Load `.env` from the given or current directory (or the given file if any) to environment variables.    
    If the given file is a directory, search `.env` in this directory.
    If `parents` is True, also search if parent directories until a `.env` file is found.

    Usage example:

    ```
    # Load configuration files
    load_dotenv() # load `.env` in the current working directory
    load_dotenv(os.path.dirname(__file__), parents=True) # load `.env` in the Python module installation directory or its parents
    load_dotenv(f'C:\\ProgramData\\my-app\\my-app.env' if sys.platform == 'win32' else f'/etc/my-app/my-app.env') # load `.env` in the system configuration directory
    ```
    """
    if not path:
        path = find_to_root('.env') if parents else '.env'
    elif os.path.isdir(path):
        path = find_to_root('.env', path) if parents else os.path.join(path, '.env')
    elif not os.path.isfile(path) and parents:
        path = find_to_root(os.path.basename(path), os.path.dirname(path))
        if not path: # not found
            return None
    
    if not os.path.isfile(path):
        return None # does not exist
    
    if path in _loaded_dotenv_paths and not override:
        return None # already loaded
        
    _logger.debug("Load dotenv file: %s", path)
    with open(path, 'r', encoding=encoding, newline=None) as fp:
        skip_utf8_bom(fp, encoding=encoding)
        for name, value in parse_properties(fp.read()):
            if not override:
                if name in os.environ:
                    continue
            os.environ[name] = value

    _loaded_dotenv_paths.add(path)
    return path


def find_to_root(name: str, start_dir: str|os.PathLike|None = None):
    """
    Find the given file name from the given start directory (or current working directory if none given), up to the root.

    Return None if not found.
    """    
    if start_dir:            
        if not os.path.exists(start_dir):
            raise IOError('Starting directory not found')
        elif not os.path.isdir(start_dir):
            start_dir = os.path.dirname(start_dir)
    else:
        start_dir = os.getcwd()

    last_dir = None
    current_dir = os.path.abspath(start_dir)
    while last_dir != current_dir:
        path = os.path.join(current_dir, name)
        if os.path.exists(path):
            return path
        parent_dir = os.path.abspath(os.path.join(current_dir, os.path.pardir))
        last_dir, current_dir = current_dir, parent_dir

    return None


def parse_properties(content: str) -> Iterator[tuple[str,str]]:
    """
    Parse properties/ini/env file content.
    """
    def find_nonspace_on_same_line(start: int):
        pos = start
        while pos < len(content):
            c = content[pos]
            if c == '\n' or not c.isspace():
                return pos
            else:
                pos += 1
        return None

    def find_closing_quote(start: int):
        """ Return the unquoted value and the next position """
        pos = content.find('"', start)
        if pos == -1:
            return content[start:], None
        elif pos+1 < len(content) and content[pos+1] == '"': # escaped
            begining_content = content[start:pos+1]
            remaining_content, remaining_pos = find_closing_quote(pos+2)
            return begining_content + remaining_content, remaining_pos
        else:
            return content[start:pos], pos

    name = None
    value = '' # value being build (or name being build if variable `name` is None)
    i = find_nonspace_on_same_line(0)
    while i is not None and i < len(content):
        c = content[i]
        if c == '"':
            unquoted, end = find_closing_quote(i+1)
            value += unquoted
            if end is None:
                return
            i = end + 1
        elif c == '=' and name is None:
            name = value
            value = ''
            i += 1
        elif c == '\n':
            if name or value:
                yield (name, value) if name is not None else (value, '')
            name = None
            value = ''
            i += 1            
        elif c == '#': # start of comment
            if name or value:
                yield (name, value) if name is not None else (value, '')
            name = None
            value = ''
            pos = content.find('\n', i+1)
            if pos == -1:
                return
            else:
                i = pos + 1
        elif c.isspace(): # start of whitespace
            end = find_nonspace_on_same_line(i+1)
            if value:
                if end is None or content[end] in ({'#', '\n', '='} if name is None else {'#', '\n'}):
                    pass # strip end
                else:
                    value += content[i:end]
            i = end
        else:
            value += c
            i += 1

    if name or value:
        yield (name, value) if name is not None else (value, '')


def get_secret(name: str, default: str|type[Exception]|None = None) -> Secret:
    """
    Read a secret value from an environment variable if given, or from the file indicated by environment variable
    `{name}_FILE` if given (use GPG pass if starts with "pass:"), or from the file `/run/secrets/{name}` (usefull for Docker containers).
    """
    return Secret(name, default)
    

class Secret:
    """ A version of get_secret() that allow delayed evaluation. """
    def __init__(self, name: str, default: str|type[Exception]|None = None):
        self.name = name
        self.default = default
        self._evaluated = False
        self._value = None

    @property
    def evaluated(self):
        return self._evaluated

    @property
    def value(self):
        if self._evaluated:
            return self._value
        
        _logger.debug("Evaluate secret %s", self.name)
        self._value = self._evaluate()
        self._evaluated = True
        return self._value

    @property
    def value_or_fail(self):
        if self.value is None:
            raise ValueError(f"Secret '{self.name}' not found")
        return self.value
    
    def _evaluate(self):
        from zut import files

        value = os.environ.get(self.name)
        if value is not None:
            return value
        
        file = os.environ.get(f'{self.name.upper()}_FILE')
        if file is not None:
            m = re.match(r'^pass:(.+)$', file)
            if m:
                from zut.gpg import get_pass
                pass_name = m[1]
                result = get_pass(pass_name)
                if result is None:
                    if isinstance(self.default, type):
                        raise self.default(f"Password '{pass_name}' not found")
                    return self.default
                return result
            else:
                with files.open(file, 'r', encoding='utf-8-sig') as fp:
                    return fp.read().rstrip('\r\n')

        file = f'/run/secrets/{self.name.lower()}'
        if files.exists(file):
            with files.open(file, 'r', encoding='utf-8-sig') as fp:
                return fp.read().rstrip('\r\n')
            
        if isinstance(self.default, type):
            raise self.default(f"Secret '{self.name}' not found")
        return self.default
    
    def __str__(self):
        return f"Secret({self.name})"
    
    def __repr__(self):
        return f"Secret({self.name})"


_in_docker_container: bool|None = None

def in_docker_container():
    """
    Indicate whether the application is running in a Docker container.
    """
    global _in_docker_container
    if _in_docker_container is None:
        _in_docker_container = os.path.exists('/.dockerenv')
    return _in_docker_container

#endregion


#region Logging

def get_logger(name: str|type):
    if not isinstance(name, str):
        if isinstance(name, type):
            name = f'{name.__module__}.{name.__qualname__}'
        elif hasattr(name, '__class__'):
            name = f'{name.__class__.__module__}.{name.__class__.__qualname__}'
        else:
            raise TypeError(f"Invalid type for argument 'name': {type(name).__name__}")

    try:
        from celery.utils.log import get_task_logger  # type: ignore
        return get_task_logger(name)
    except ModuleNotFoundError:
        return logging.getLogger(name)


def configure_logging(level: str|int|None = None, *, file_level: str|int|None = None, verbose_level: str|int|None = None, verbose_loggers: Sequence[str]|None = None, name_info = True, counter = True, exit_handler = True):
    config = get_logging_config(level=level, file_level=file_level, verbose_level=verbose_level, verbose_loggers=verbose_loggers, name_info=name_info, counter=counter, exit_handler=exit_handler)
    logging.config.dictConfig(config)


def get_logging_config(level: str|int|None = None, *, file_level: str|int|None = None, verbose_level: str|int|None = None, verbose_loggers: Sequence[str]|None = None, name_info = True, counter = True, exit_handler = True):
    if not isinstance(level, str):
        if isinstance(level, int):
            level = logging.getLevelName(level)
        else:
            level = os.environ.get('LOG_LEVEL', '').upper() or 'INFO'
    
    # Ensure specific verbose subsystems do not send DEBUG messages, even if LOG_LEVEL is DEBUG, except if we explicitely request it
    if not isinstance(verbose_level, str):
        if isinstance(verbose_level, int):
            verbose_level = logging.getLevelName(verbose_level)
        else:
            verbose_level = os.environ.get('LOG_VERBOSE_LEVEL', '').upper()
    
    config = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'default': {
                'format': '%(levelname)s [%(name)s] %(message)s' if name_info or level == 'DEBUG' else '%(levelname)s: %(message)s',
            },
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'level': level,
                'formatter': 'default',
            },
        },
        'root': {
            'handlers': ['console'],
            'level': level,
        },
        'loggers': {
            'django': { 'level': verbose_level or 'INFO', 'propagate': False },
            'daphne': { 'level': verbose_level or 'INFO', 'propagate': False },
            'asyncio': { 'level': verbose_level or 'INFO', 'propagate': False },            
            'PIL': { 'level': verbose_level or 'INFO', 'propagate': False },
            'celery.utils.functional': { 'level': verbose_level or 'INFO', 'propagate': False },
            'smbprotocol': { 'level': verbose_level or 'WARNING', 'propagate': False },
            'msal': { 'level': verbose_level or 'WARNING', 'propagate': False },
        },
    }

    if verbose_loggers:
        for name in verbose_loggers:
            config['loggers'][name] = { 'level': verbose_level or 'INFO', 'propagate': False }

    if not Color.NO_COLORS:
        config['formatters']['colored'] = {
            '()': ColoredFormatter.__module__ + '.' + ColoredFormatter.__qualname__,
            'format': '%(levelcolor)s%(levelname)s%(reset)s %(gray)s[%(name)s]%(reset)s %(messagecolor)s%(message)s%(reset)s' if name_info or level == 'DEBUG' else '%(levelcolor)s%(levelname)s%(reset)s: %(messagecolor)s%(message)s%(reset)s',
        }

        config['handlers']['console']['formatter'] = 'colored'

    file = os.environ.get('LOG_FILE')
    if file:
        if not isinstance(file_level, str):
            if isinstance(file_level, int):
                file_level = logging.getLevelName(file_level)
            else:
                file_level = os.environ.get('LOG_FILE_LEVEL', '').upper() or level

        log_dir = os.path.dirname(file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir)

        config['formatters']['file'] = {
            'format': '%(asctime)s %(levelname)s [%(name)s] %(message)s',
        }
        config['handlers']['file'] = {
            'class': 'logging.FileHandler',
            'level': file_level,
            'formatter': 'file',
            'filename': file,
            'encoding': 'utf-8',
        }

        config['root']['handlers'].append('file')
    
        file_intlevel = logging.getLevelName(file_level)
        intlevel = logging.getLevelName(level)
        if file_intlevel < intlevel:
            config['root']['level'] = file_level
    
    if counter or exit_handler:
        config['handlers']['counter'] = {
            'class': LogCounter.__module__ + '.' + LogCounter.__qualname__,
            'level': 'WARNING',
            'exit_handler': exit_handler,
        }

        config['root']['handlers'].append('counter')

    return config


class ColoredRecord:
    LEVELCOLORS = {
        logging.DEBUG:     Color.GRAY,
        logging.INFO:      Color.CYAN,
        logging.WARNING:   Color.YELLOW,
        logging.ERROR:     Color.RED,
        logging.CRITICAL:  Color.BG_RED,
    }

    MESSAGECOLORS = {
        logging.INFO:      '',
        logging.CRITICAL:  Color.RED,
    }

    def __init__(self, record: logging.LogRecord):
        # The internal dict is used by Python logging library when formatting the message.
        # (inspired from library "colorlog").
        self.__dict__.update(record.__dict__)
        
        self.levelcolor = self.LEVELCOLORS.get(record.levelno, '')
        self.messagecolor = self.MESSAGECOLORS.get(record.levelno, self.levelcolor)

        for attname, value in Color.__dict__.items():
            if attname == 'NO_COLORS' or attname.startswith('_'):
                continue
            setattr(self, attname.lower(), value)


class ColoredFormatter(logging.Formatter):
    def formatMessage(self, record: logging.LogRecord) -> str:
        """Format a message from a record object."""
        wrapper = ColoredRecord(record)
        message = super().formatMessage(wrapper) # type: ignore
        return message


class LogCounter(logging.Handler):
    """
    A logging handler that counts warnings and errors.
    
    If warnings and errors occured during the program execution, display counts at exit
    and set exit code (if it was not explicitely set with `sys.exit` function).
    """
    counts: dict[int, int]

    error_exit_code = 199
    warning_exit_code = 198

    
    _detected_exception: tuple[type[BaseException], BaseException, TracebackType|None]|None = None
    _detected_exit_code = 0
    _original_exit: Callable[[int]] = sys.exit
    _original_excepthook = sys.excepthook

    _registered = False
    _logger: logging.Logger

    def __init__(self, *, level = logging.WARNING, exit_handler = False):
        if not hasattr(self.__class__, 'counts'):
            self.__class__.counts = {}
        
        if exit_handler and not self.__class__._registered:
            sys.exit = self.__class__._exit
            sys.excepthook = self.__class__._excepthook
            atexit.register(self.__class__._exit_handler)
            self.__class__._logger = get_logger(f'{self.__class__.__module__}.{self.__class__.__qualname__}')
            self.__class__._registered = True
        
        super().__init__(level=level)

    def emit(self, record: logging.LogRecord):        
        if not record.levelno in self.__class__.counts:
            self.__class__.counts[record.levelno] = 1
        else:
            self.__class__.counts[record.levelno] += 1
    
    @classmethod
    def _exit(cls, code: int = 0):
        cls._detected_exit_code = code
        cls._original_exit(code)
    
    @classmethod
    def _excepthook(cls, exc_type: type[BaseException], exc_value: BaseException, exc_traceback: TracebackType|None):
        cls._detected_exception = exc_type, exc_value, exc_traceback
        cls._original_exit(1)

    @classmethod
    def _exit_handler(cls):
        if cls._detected_exception:
            exc_type, exc_value, exc_traceback = cls._detected_exception

            msg = 'An unhandled exception occured\n'
            msg += ''.join(format_exception(exc_type, exc_value, exc_traceback)).strip()
            cls._logger.critical(msg)

        else:
            error_count = 0
            warning_count = 0
            for level, count in cls.counts.items():
                if level >= logging.ERROR:
                    error_count += count
                elif level >= logging.WARNING:
                    warning_count += count
            
            msg = ''
            if error_count > 0:
                msg += (', ' if msg else 'Logged ') + f"{error_count:,} error{'s' if error_count > 1 else ''}"
            if warning_count > 0:
                msg += (', ' if msg else 'Logged ') + f"{warning_count:,} warning{'s' if warning_count > 1 else ''}"
            
            if msg:
                cls._logger.log(logging.ERROR if error_count > 0 else logging.WARNING, msg)                             
                # Change exit code if it was not originally set explicitely to another value using `sys.exit()`
                if cls._detected_exit_code == 0:
                    os._exit(cls.error_exit_code if error_count > 0 else cls.warning_exit_code)


@contextmanager
def log_warnings(*, ignore: str|re.Pattern|list[str|re.Pattern]|None = None, logger: logging.Logger|None = None):
    catch = catch_warnings(record=True)
    try:
        ctx = catch.__enter__()
        yield None
    
    finally:
        if not logger:
            logger = _logger
        if isinstance(ignore, (str,re.Pattern)):
            ignore = [ignore]

        for warning in ctx:
            ignored = False
            if ignore:
                message = str(warning.message)
                for spec in ignore:
                    if isinstance(spec, re.Pattern):
                        if spec.match(message):
                            ignored = True
                            break
                    elif spec == message:
                        ignored = True
                        break
            
            if not ignored:
                logger.warning("%s: %s", warning.category.__name__, warning.message)
        
        catch.__exit__(None, None, None)

#endregion


#region Graph

def topological_sort(source: dict[T,None|T|Iterable[T]]) -> list[T]:
    """
    Perform a topological sort.

    - `source`: dictionnary associating keys to list of dependencies
    - returns a list of keys, sorted with dependencies first
    """
    #See: https://stackoverflow.com/a/11564323
    pending = [(key, set() if deps is None else (set([deps]) if isinstance(deps, type(key)) else set(deps))) for key, deps in source.items()] # copy deps so we can modify set in-place  # type: ignore
    emitted = []
    result = []

    while pending:
        next_pending = []
        next_emitted = []

        for entry in pending:
            key, deps = entry
            deps.difference_update(emitted) # remove deps we emitted last pass
            if deps: # still has deps? recheck during next pass
                next_pending.append(entry) 
            else: # no more deps? time to emit
                result.append(key)
                emitted.append(key) # <-- not required, but helps preserve original ordering
                next_emitted.append(key) # remember what we emitted for difference_update() in next pass

        if not next_emitted: # all entries have unmet deps, one of two things is wrong...
            raise ValueError("Cyclic or missing dependency detected: %r" % (next_pending,))
        
        pending = next_pending
        emitted = next_emitted

    return result

#endregion


_logger = get_logger(__name__)
