{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heim and Kratzer partiality\n",
    "\n",
    "**The main question**: How can an analysis of presuppositions and presupposition projection be implemented?\n",
    " * There is very little literature on this, and only a couple of DRT implementations that I have been able to find.  The most comprehensive discussion is Johan Bos, *Implementing the Binding and Accommodation Theory for Anaphora Resolution and Presupposition Projection*, Computational Linguistics 29, 2003.\n",
    " http://cognet.mit.edu/journal/10.1162/089120103322145306\n",
    " * Other DRT work\n",
    " * Nothing that I've found for any other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: a non-partial compositional semantics\n",
    "\n",
    "The composition system in H&K chapter 3:\n",
    "\n",
    "1. *Terminal nodes* (TN)  \n",
    "If $\\alpha$ is a terminal node, $[[\\alpha]]$ is specified in the lexicon.\n",
    "\n",
    "2. *Non-branching nodes* (NN)  \n",
    "If $\\alpha$ is a non-branching node, and $\\beta$ is its daughter node, then $[[\\alpha]] = [[\\beta]]$.\n",
    "\n",
    "3. *Functional application* (FA)  \n",
    "If $\\alpha$ is a branching node, $\\{\\beta, \\gamma\\}$ is the set of $\\alpha$'s daughters, and $[[\\beta]]$ is a function whose domain contains $[[\\gamma]]$, then $[[\\alpha]] = [[\\beta]]([[\\gamma]])$\n",
    "\n",
    "All functions are assumed to be total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A partial semantics for presuppositions\n",
    "\n",
    "Strawson's intuition: sentences with presupposition failures are neither true nor false.\n",
    "\n",
    "    (1) The king of france is bald.\n",
    "\n",
    "How to implement trivalence?\n",
    "\n",
    "* Option A: a trivalent logic of some kind for the metalanguage.\n",
    "* Option B: reference failure in the interpretive process (somehow).\n",
    "\n",
    "The Heim & Kratzer solution is, on the surface, a version of option B: treat denotations as (potentially) partial functions, and adjust composition rules for partiality.\n",
    "\n",
    "1. *Functional application (partial)* (H&K p. 76)  \n",
    "If $\\alpha$ is a branching node and $\\{\\beta, \\gamma\\}$ is the set of $\\alpha$'s daughters, then $\\alpha$ is in the domain of $[[ ]]$ if both $\\beta$ and $\\gamma$ are, and $[[\\gamma]]$ is in the domain of $[[\\beta]]$.  In this case, $[[\\alpha]] = [[\\beta]]([[\\gamma]])$.\n",
    "\n",
    "2. Principle of interpretability  \n",
    "A constituent XP is interpretable only if XP is in the domain of $[[ ]]$.\n",
    "\n",
    "\n",
    "**Some questions**:\n",
    "* How exactly to track partiality conditions during composition?\n",
    "* Partiality appears to be mainly a property of denotations.  Can an implementation work by lifting the composition rules, and simply adding definedness conditions to functions?\n",
    "* How might this interact with the lack of a well-defined metalanguage in H&K?\n",
    "* How seamless is the integration with other machinery, e.g. quantifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metalanguage strategy 0: minimal modification of the metalanguage\n",
    "\n",
    "Suppose that we have the following two H&K-style entries:\n",
    "\n",
    "* $[[the]] = \\lambda f : f \\in D_{<e,t>} \\text{ and there is exactly one }x\\text{ such that }f(x)=1 . \\text{the unique }x\\text{ such that }f(x)=1$\n",
    "* $[[cat]] = \\lambda x : x \\in D_e . x\\text{ is a cat}$\n",
    "\n",
    "What we need to check is if $[[cat]]$ is in the domain of $[[the]]$.  This is quite straightforward -- the definedness condition tells us that it is just in case we can find exactly one cat.  This looks good for the minimal strategy which involves:\n",
    "\n",
    "1. When you apply FA on $f$ and $a$:\n",
    "  \n",
    "  1. Check if the argument satisfies the definedness condition of the function. If it doesn't, fail.\n",
    "  2. If it does, return the result of $f(a)$.\n",
    "  \n",
    "At first glance, it appears that this can be done without much change to the metalanguage implementation!\n",
    "\n",
    "### Complications\n",
    "  \n",
    "**Complication 1: arbitrary embedding of partiality.** However, in the general case, things may be more complicated.  What if we have an argument with a definedness condition as well?\n",
    "\n",
    "* $[[cat]] = \\lambda x : x \\in D_e \\text{ and } x\\text{ is alive} . x\\text{ is a cat}$  \n",
    "\n",
    "Or in more logical notation:\n",
    "\n",
    "* $[[cat]] = \\lambda x_e : \\text{Alive}(x) . \\text{Cat}(x)$  \n",
    "* $[[the]] = \\lambda f_{<e,t>} : \\exists ! x_e : f(x) . \\iota x_e : f(x)$\n",
    "\n",
    "Implementing step A above is no longer simple. First reduction step:\n",
    "\n",
    "$[[the]]([[cat]]) = \\exists ! x_e : [\\lambda x_e : \\text{Alive}(x) . \\text{Cat}(x)](x) . \\iota x_e : [\\lambda x_e : \\text{Alive}(x) . \\text{Cat}(x)](x)$\n",
    "\n",
    " * To check if this [[cat]] is in the domain of [[the]], you have to check if $x$ is in the domain of [[cat]].\n",
    " * $x$ is bound by a quantifier in the presupposition, so you have to have a story about how this interacts.\n",
    " * $x$ is also bound by an operator in the body.\n",
    " * Whatever happens to the partiality condition, it has to be carried by non-functions *in the metalanguage*.  At a minimum, operators will have to deal with it somehow.\n",
    " * But also, instances of application in the metalanguage that may involve partiality can be *arbitrarily embedded* in formulas. Using a non-logical metalanguage obscures this problem, but doesn't eliminate it.\n",
    "\n",
    "What even is the right result for the second and third reduction steps??\n",
    "\n",
    "**Complication 2: projecting partiality.**\n",
    "\n",
    "Suppose you have the following two expressions:\n",
    "\n",
    "1. $[[A]] = \\lambda f_{<e,t>} . \\lambda x_e . P(x) \\wedge f(x)$\n",
    "2. $[[B]] = \\lambda x_e : Q(x) . R(x)$\n",
    "\n",
    "The first reduction step should look something like:\n",
    "\n",
    "* $\\lambda x_e . P(x) \\wedge [\\lambda x_e : Q(x) . R(x)](x)$\n",
    "\n",
    "This instantiates the problem from above: partiality is happening at an embedded position in the formula.  But it also instantiates a new problem: when a human is working with H&K partial functions, they are actually doing a lot of conversion/simplification implicitly.  It's clear in this case what the result should be mathematically:\n",
    "\n",
    "* $\\lambda x_e : Q(x) . P(x) \\wedge R(x)$\n",
    "\n",
    "But how to get there?  Effectively what this requires is a precise algorithm for projecting partiality in metalanguage formulas.\n",
    "\n",
    "**Complication 3: binding into partiality**\n",
    "\n",
    "This is already instantiated by the above cases, but is useful to point out on its own terms. In general the metalanguage *cannot avoid* handling binding into partiality.  For example:\n",
    "\n",
    "1. $[[A]] = \\lambda f_{<e,t>} . \\exists x_e . P(x) \\wedge f(x)$\n",
    "2. $[[B]] = \\lambda x_e : Q(x) . R(x)$\n",
    "\n",
    "First reduction:\n",
    "\n",
    "* $\\exists x_e . P(x) \\wedge [\\lambda x_e : Q(x) . R(x)](x)$\n",
    "\n",
    "Whatever the result of this reduction is, $x$ to a first approximation is bound in both the definedness condition, and the regular part of an embedded formula.  In this case it *isn't* entirely obvious what the result should be, but something like:\n",
    "\n",
    "* $\\exists x_e . P(x) \\wedge R(x)$, defined iff $\\exists x_e . Q(x)$\n",
    "\n",
    "looks plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metalanguage strategy 1: multidimensional types\n",
    "\n",
    "See separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metalanguage strategy 2: a partiality operator outside the type system\n",
    "\n",
    "One fundamental element of the Heim & Kratzer treatment is that partiality doesn't really change types.\n",
    "* technical change: $D_{<\\alpha, \\beta>}$ is the set of all *partial* functions from $D_\\alpha$ to $D_\\beta$.\n",
    "\n",
    "But we have seen that *partiality is not just about functions*.  The lambda notebook implementation of functions rests in the `LFun` class, along with machinery for doing reduction.  But we need a more general way of implementing partiality.\n",
    "\n",
    "Basic reminder of how the lambda notebook implementation is structured.\n",
    "* Each type constructor corresponds to a python class that implements a representation and set of behaviors for that type constructor.\n",
    "* E.g. `LFun` stores a (typed) variable and a body, which is a typed expression, and defines reudction behaviors.  `TypedTerm` stores a term name and a type.  `Tuple` stores a sequence of typed expressions at arbitrary types.\n",
    "* The minimal implementation suggested that one could simply create a variant of `LFun` that has a third part to its representation -- a definedness condition.  However, the points above force the conclusion that this isn't general enough.\n",
    "\n",
    "*Basic idea*: add a new kind of typed expression, called `Partial`.  `Partial` objects:\n",
    " * Store a pair consisting of an arbitrary typed expression, and an object at type $t$.\n",
    " * Inherit their type from the first element in this pair.\n",
    "\n",
    "All typed expressions add:\n",
    " * an algorithm for calculating the projection of partiality.  This isn't intended to be a good presupposition projection algorithm, but rather, to be a good theory of partiality.\n",
    " \n",
    "**The algorithm `CP`**. For some formula $\\alpha$ with subformulas $\\beta_1...\\beta_n$ for $n \\geq 1$:\n",
    "1. If $\\alpha$ is a `Partial`, $CP(\\alpha)$ is $Partial(Partial.body \\wedge CP(Partial.body).body, Partial.condition \\wedge CP(Partial.body).condition \\wedge CP(Partial.condition).body \\wedge CP(Partial.condition).condition)$\n",
    "2. If $\\alpha$ is an `LFun` $\\lambda x . \\phi$, $CP(\\alpha)$ is $\\lambda x . CP(\\phi)$.\n",
    "3. If $\\alpha$ is a non-`LFun` binding expression, the details vary, though they all start by applying CP to the body.\n",
    "  1. $\\exists x : \\beta$: $Partial(\\exists x : CP(\\beta).body, \\exists x : CP(\\beta).condition)$\n",
    "  2. ...\n",
    "4. Otherwise, $CP(\\alpha)$ is $Partial(\\alpha, CP(\\beta_1) \\wedge ... \\wedge CP(\\beta_n))$\n",
    " \n",
    "**Reduction**: Reduction recurses into the body and the condition of a `Partial` just as if they were ordinary parts, after `CP`.  (In fact, I didn't need to change the reduction algorithm at all.)\n",
    "* This implies, for example, that where $\\beta$ does not contain $x$ free, $Partial(\\lambda x . \\alpha, \\beta)$  and $\\lambda x . Partial(\\alpha, \\beta)$ are equivalent under application with CP.\n",
    "\n",
    "\n",
    "See partiality documentation notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reference**\n",
    "\n",
    "Partial.calculate_partiality:\n",
    "    \n",
    "    def calculate_partiality(self):\n",
    "        new_body = self.body.calculate_partiality()\n",
    "        new_condition = self.condition.calculate_partiality()\n",
    "        if isinstance(new_condition, Partial):\n",
    "            new_condition = new_condition.body & new_condition.condition\n",
    "        if isinstance(new_body, Partial):\n",
    "            new_condition = new_condition & new_body.condition\n",
    "            new_body = new_body.body\n",
    "        new_condition = new_condition.simplify_all()\n",
    "        return Partial(new_body, new_condition)\n",
    "\n",
    "TypedExpr.calculate_partiality:\n",
    "\n",
    "    def calculate_partiality(self):\n",
    "        condition = true_term\n",
    "        new_parts = list()\n",
    "        for part in self:\n",
    "            if isinstance(part, TypedExpr):\n",
    "                part_i = part.calculate_partiality()\n",
    "                if isinstance(part_i, Partial):\n",
    "                    condition = condition & part_i.condition\n",
    "                    part_i = part_i.body\n",
    "            else:\n",
    "                part_i = part\n",
    "            new_parts.append(part_i)\n",
    "        new_self = self.copy_local(*new_parts)\n",
    "        condition = condition.simplify_all()\n",
    "        if condition is not true_term:\n",
    "            return Partial(new_self, condition)\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "LFun.calculate_partiality:\n",
    "\n",
    "    def calculate_partiality(self):\n",
    "        new_body = self.body.calculate_partiality()\n",
    "        return self.copy_local(self.op, self.var_instance, new_body)\n",
    "\n",
    "\n",
    "BindingOp.calculate_partiality is a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lambda Notebook (Python 3)",
   "language": "python",
   "name": "lambda-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
