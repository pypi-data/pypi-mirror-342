import base64
import json
import logging
import os
import datetime
from mcp.server.fastmcp import FastMCP, Context
from mcp.types import TextContent, ImageContent
from playwright.async_api import Page
from browse_mcp.browse_manager import BrowserManager
from pydantic import BaseModel, Field
from browse_mcp.tools.playwright_tools import PlaywrightTools
import pathlib
from browse_mcp.tools.file_tools import FileTools

# from autogen_ext.agents.web_surfer._types import InteractiveRegion
from autogen_ext.agents.web_surfer.playwright_controller import PlaywrightController
from typing import (
    Any,
    AsyncGenerator,
    Dict,
    List,
    Optional,
    Sequence,
)


class SearchResult(BaseModel):
    """ç™¾åº¦æœç´¢ç»“æœçš„æ•°æ®æ¨¡å‹"""
    title: str = Field(description="æœç´¢ç»“æœçš„æ ‡é¢˜")
    url: str = Field(description="æœç´¢ç»“æœçš„URLé“¾æ¥")
    snippet: str = Field(description="æœç´¢ç»“æœçš„ç®€ä»‹æ‘˜è¦")
    source: Optional[str] = Field(None, description="æ¥æºç½‘ç«™")
    time: Optional[str] = Field(None, description="å‘å¸ƒæ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰")
    
    class Config:
        # ç¡®ä¿JSONåºåˆ—åŒ–æ—¶ä¸å°†ä¸­æ–‡è½¬æ¢ä¸ºUnicodeç¼–ç 
        json_encoders = {
            str: lambda v: v
        }
        json_dumps_kwargs = {
            "ensure_ascii": False
        }
    
    def model_dump(self, **kwargs):
        """é‡å†™model_dumpæ–¹æ³•ï¼Œç¡®ä¿è¿”å›çš„JSONä¸ä½¿ç”¨Unicodeç¼–ç """
        # åˆå¹¶é»˜è®¤å‚æ•°å’Œä¼ å…¥çš„å‚æ•°
        dump_kwargs = {"exclude_none": True}
        dump_kwargs.update(kwargs)
        # è°ƒç”¨çˆ¶ç±»çš„model_dumpæ–¹æ³•
        result = super().model_dump(**dump_kwargs)
        return result

class BrowserNavigationServer(FastMCP):
    def __init__(self, server_name="browser-operation-server"):
        super().__init__(server_name)
        self.mcp = self
        self.browser_manager = BrowserManager()
        # self.llm_config = get_default_llm_config()
        # self.llm_client = LLMClient(self.llm_config)

        self.search_results_cache = {}
        self.current_page = 1
        self.current_query = ""
        self.total_pages = 1
        
        self.screenshots = dict()
        self._setup_logger()
        self.register_tools()
        self.register_resources()
        self.register_prompts()
        self.file_tools = FileTools()
        self.logger.info("BrowserNavigationServer åˆå§‹åŒ–å®Œæˆ")
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = "/Users/lixiaohao/Downloads/llm-logs"
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»ºå¸¦æœ‰æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"browse_operation_{timestamp}.log")
        
        # é…ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger("BrowserNavigationServer")
        self.logger.setLevel(logging.DEBUG)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°è®°å½•å™¨
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {log_file}")

    def register_tools(self):
        @self.mcp.tool(description="Navigate to a URL and get makrdown content")
        async def playwright_navigate(url: str):
            """Navigate to a URL and return the page content in markdown format."""
            self.logger.info(f"å¼€å§‹å¯¼èˆªåˆ°URL: {url}")
            try:
                page: Page = await self.browser_manager.ensure_browser()
                self.logger.debug(f"æµè§ˆå™¨å·²å‡†å¤‡å°±ç»ªï¼Œå¼€å§‹åŠ è½½é¡µé¢: {url}")
                await page.goto(url=url, wait_until="load", timeout=30000)
                
                # è·å–é¡µé¢æ ‡é¢˜
                page_title = await page.title()
                self.logger.info(f"é¡µé¢åŠ è½½å®Œæˆï¼Œæ ‡é¢˜: {page_title}")
                
                # ä½¿ç”¨PlaywrightControllerè·å–é¡µé¢å†…å®¹çš„markdownæ ¼å¼
                playwright_controller = PlaywrightController()
                page_markdown = await playwright_controller.get_page_markdown(page)
                
                self.logger.debug(f"å·²æå–é¡µé¢Markdownå†…å®¹ï¼Œé•¿åº¦: {len(page_markdown)}å­—ç¬¦,å†…å®¹: {page_markdown}")
                
                
                result = {
                    "title": page_title,
                    "markdown_content_url": url,
                    "markdown_content": page_markdown
                }
                self.logger.info(f"é¡µé¢å¯¼èˆªå®Œæˆ: {url}")
                return result
            except Exception as e:
                error_msg = f"Navigation failed: {e}"
                self.logger.error(error_msg, exc_info=True)
                raise ValueError(error_msg)
                
        @self.mcp.tool(description="åœ¨ç™¾åº¦ä¸Šæœç´¢å¹¶è·å–ç»“æœ")
        async def baidu_search(query: str, page: int = 1):
            """åœ¨ç™¾åº¦ä¸Šæ‰§è¡Œæœç´¢æŸ¥è¯¢å¹¶è¿”å›ç»“æœ
            
            ä¸ºäº†é¿å…è§¦å‘ç™¾åº¦éªŒè¯ç ï¼Œæœ¬æ–¹æ³•ä¼š:
            1. è®¾ç½®åˆç†çš„User-Agent
            2. æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
            3. æ§åˆ¶è¯·æ±‚é¢‘ç‡
            4. åœ¨éœ€è¦æ—¶å¤„ç†éªŒè¯ç 
            5. é€šè¿‡ç‚¹å‡»åˆ†é¡µæŒ‰é’®è·å–æ¯é¡µç»“æœ
            
            å‚æ•°:
                query: æœç´¢å…³é”®è¯
                page: æ£€ç´¢çš„é¡µæ•°
            """
            self.logger.info(f"å¼€å§‹ç™¾åº¦æœç´¢ï¼Œå…³é”®è¯: '{query}'ï¼Œé¡µç : {page}")
            try:
                self.current_query = query
                self.current_page = page
                
                # å¯¼èˆªåˆ°ç™¾åº¦é¦–é¡µ
                search_url = f"https://www.baidu.com"
                self.logger.debug(f"å‡†å¤‡å¯¼èˆªåˆ°ç™¾åº¦é¦–é¡µ: {search_url}")
                
                # å¯¼èˆªåˆ°æœç´¢é¡µé¢
                browser_page: Page = await self.browser_manager.ensure_browser()

                # è®¾ç½®åˆç†çš„User-Agentå’Œå…¶ä»–è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
                # ä½¿ç”¨å¸¸è§çš„ç°ä»£æµè§ˆå™¨User-Agent
                user_agents = [
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36 Edg/92.0.902.55",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0"
                ]
                import random
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªUser-Agent
                selected_user_agent = random.choice(user_agents)
                
                # è®¾ç½®é¢å¤–çš„è¯·æ±‚å¤´
                extra_headers = {
                    "User-Agent": selected_user_agent,
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Cache-Control": "max-age=0"
                }
                
                # è®¾ç½®è¯·æ±‚å¤´å¹¶å¯¼èˆªåˆ°æœç´¢é¡µé¢
                await browser_page.set_extra_http_headers(extra_headers)
                await browser_page.goto(url=search_url, wait_until="domcontentloaded", timeout=30000)
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                await browser_page.wait_for_load_state("networkidle")
                
                # å®šä½æœç´¢æ¡†å¹¶è¾“å…¥å…³é”®è¯
                search_input_selector = "#kw"
                await browser_page.wait_for_selector(search_input_selector, timeout=5000)
                await browser_page.fill(search_input_selector, query)
                # ç‚¹å‡»æœç´¢æŒ‰é’®
                search_button_selector = "#su"
                await browser_page.wait_for_selector(search_button_selector, timeout=5000)
                await browser_page.click(search_button_selector)
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                await browser_page.wait_for_load_state("networkidle")
                await browser_page.wait_for_selector(".result", timeout=5000)
                self.logger.info("æœç´¢ç»“æœé¡µé¢åŠ è½½å®Œæˆ")
                
                # æå–åˆ†é¡µä¿¡æ¯
                total_pages = await self._extract_pagination_info(browser_page)
                self.total_pages = total_pages
                self.logger.debug(f"æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}")
                
                # æå–æœç´¢ç»“æœ
                self.logger.debug("å¼€å§‹æå–ç¬¬ä¸€é¡µæœç´¢ç»“æœ")
                page_results = await self._extract_search_results(browser_page)
                if total_pages > 1 and page > 1:
                    self.current_page = 2
                    # ä»å½“å‰é¡µå¼€å§‹ï¼Œè·å–åç»­æ‰€æœ‰é¡µé¢çš„ç»“æœ
                    while self.current_page <= page and self.current_page <= self.total_pages:
                        # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’® - å°è¯•å¤šç§é€‰æ‹©å™¨
                        next_page_selector = "a.n:has-text('ä¸‹ä¸€é¡µ')"
                        clicked = False
                        
                        # ç­‰å¾…ä¸‹ä¸€é¡µæŒ‰é’®å‡ºç°
                        next_button = await browser_page.wait_for_selector(next_page_selector, timeout=15000)
                        if next_button:
                            # ç¡®ä¿æŒ‰é’®å¯è§ä¸”å¯ç‚¹å‡»
                            await next_button.scroll_into_view_if_needed()
                            await browser_page.click(next_page_selector)
                            clicked = True
                            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                            await browser_page.wait_for_load_state("networkidle")
                            await browser_page.wait_for_selector(".result", timeout=15000)
            
                        
                        if not clicked:
                            print("æ— æ³•æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                            break
                        
                        # æ›´æ–°å½“å‰é¡µç 
                        self.current_page += 1
                        
                        # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è§¦å‘éªŒè¯ç 
                        delay_time = random.uniform(1.0, 2.0)
                        await browser_page.wait_for_timeout(delay_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                        current_page_results = await self._extract_search_results(browser_page)
                        # åˆå¹¶ç»“æœ
                        page_results.extend(current_page_results)
                # ç¼“å­˜ç»“æœ
                cache_key = f"{query}_{page}"
                self.search_results_cache[cache_key] = page_results
                self.logger.debug(f"å·²ç¼“å­˜æœç´¢ç»“æœï¼Œé”®å€¼: {cache_key}")
                
                # æ„å»ºè¿”å›ä¿¡æ¯
                # ä½¿ç”¨json.dumpsç¡®ä¿ä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«è½¬æ¢ä¸ºUnicodeç¼–ç 
                result = {
                    "query": query,
                    "page": page,
                    "total_pages": total_pages,
                    "results": json.loads(json.dumps([result.model_dump(exclude_none=True) for result in page_results], ensure_ascii=False)),
                    "result_count": len(page_results)
                }
                self.logger.info(f"ç™¾åº¦æœç´¢å®Œæˆï¼Œå…±è·å– {len(page_results)} æ¡ç»“æœ")
                return result
            except Exception as e:
                error_msg = f"æœç´¢å¤±è´¥: {e}"
                self.logger.error(error_msg, exc_info=True)
                raise ValueError(error_msg)
    async def _extract_pagination_info(self, page: Page) -> int:
        """æå–åˆ†é¡µä¿¡æ¯"""
        self.logger.debug("å¼€å§‹æå–åˆ†é¡µä¿¡æ¯")
        try:
            # å°è¯•è·å–æ€»é¡µæ•°
            total_pages = await page.evaluate("""
                () => {
                    const pageInfo = document.querySelector('.page-inner');
                    if (pageInfo) {
                        const lastPage = pageInfo.querySelector('a:last-of-type');
                        if (lastPage && lastPage.textContent) {
                            const pageNum = parseInt(lastPage.textContent.trim());
                            return isNaN(pageNum) ? 1 : pageNum;
                        }
                    }
                    return 1; // é»˜è®¤ä¸º1é¡µ
                }
            """)
            self.logger.debug(f"é¡µé¢è¯„ä¼°è¿”å›çš„é¡µæ•°: {total_pages}ï¼Œä½¿ç”¨å›ºå®šå€¼10")
            return 10
        except Exception as e:
            self.logger.warning(f"æå–åˆ†é¡µä¿¡æ¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼1")
            return 1  # å¦‚æœæ— æ³•æå–ï¼Œé»˜è®¤ä¸º1é¡µ
    async def _simulate_human_behavior(self, page: Page):
        """æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼Œéšæœºæ»šåŠ¨å’Œæš‚åœ"""
        import random
        import asyncio
        
        self.logger.debug("å¼€å§‹æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º")
        # éšæœºæ»šåŠ¨é¡µé¢
        scroll_count = random.randint(2, 5)
        self.logger.debug(f"å°†æ‰§è¡Œ {scroll_count} æ¬¡éšæœºæ»šåŠ¨")
        
        for i in range(scroll_count):
            # éšæœºæ»šåŠ¨è·ç¦»
            scroll_distance = random.randint(300, 800)
            self.logger.debug(f"æ»šåŠ¨è·ç¦»: {scroll_distance}px (ç¬¬ {i+1}/{scroll_count} æ¬¡)")
            await page.evaluate(f"window.scrollBy(0, {scroll_distance})")
            
            # éšæœºæš‚åœæ—¶é—´
            pause_time = random.uniform(0.5, 2.0)
            self.logger.debug(f"æš‚åœæ—¶é—´: {pause_time:.2f}ç§’")
            await asyncio.sleep(pause_time)
        
        # æœ‰æ—¶å€™æ»šå›é¡¶éƒ¨
        if random.random() > 0.7:
            self.logger.debug("éšæœºå†³å®šæ»šå›é¡µé¢é¡¶éƒ¨")
            await page.evaluate("window.scrollTo(0, 0)")
            wait_time = random.uniform(0.3, 1.0)
            self.logger.debug(f"æ»šå›é¡¶éƒ¨åç­‰å¾…: {wait_time:.2f}ç§’")
            await asyncio.sleep(wait_time)
        
        self.logger.debug("æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºå®Œæˆ")
            
    async def _check_and_handle_captcha(self, page: Page) -> bool:
        """æ£€æŸ¥æ˜¯å¦å‡ºç°éªŒè¯ç å¹¶å°è¯•å¤„ç†
        
        è¿”å›å€¼:
            bool: å¦‚æœæ£€æµ‹åˆ°å¹¶å¤„ç†äº†éªŒè¯ç è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.debug("æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ")
        # æ£€æŸ¥å¸¸è§çš„ç™¾åº¦éªŒè¯ç å…ƒç´ 
        captcha_selectors = [
            "#verify_img",  # å›¾ç‰‡éªŒè¯ç 
            ".vcode-spin",  # æ—‹è½¬éªŒè¯ç 
            ".vcode-slide",  # æ»‘åŠ¨éªŒè¯ç 
            "#seccodeImage",  # å®‰å…¨éªŒè¯ç å›¾ç‰‡
            ".vcode-body"  # éªŒè¯ç å®¹å™¨
        ]
        
        for selector in captcha_selectors:
            if await page.query_selector(selector) is not None:
                self.logger.warning(f"æ£€æµ‹åˆ°ç™¾åº¦éªŒè¯ç  ({selector})ï¼Œéœ€è¦äººå·¥å¤„ç†")
                
                # åœ¨æ§åˆ¶å°è®°å½•éªŒè¯ç å‡ºç°
                if hasattr(self, 'console_logs'):
                    self.console_logs.append("[è­¦å‘Š] æ£€æµ‹åˆ°ç™¾åº¦éªŒè¯ç ï¼Œè¯·æ‰‹åŠ¨å¤„ç†")
                
                # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†éªŒè¯ç 
                # è¿™é‡Œæˆ‘ä»¬ç­‰å¾…30ç§’ï¼Œå‡è®¾ç”¨æˆ·ä¼šåœ¨è¿™æ®µæ—¶é—´å†…è§£å†³éªŒè¯ç 
                # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„æœºåˆ¶æ¥é€šçŸ¥ç”¨æˆ·å¹¶ç­‰å¾…éªŒè¯ç è§£å†³
                self.logger.info("ç­‰å¾…30ç§’ï¼Œè®©ç”¨æˆ·å¤„ç†éªŒè¯ç ")
                await page.wait_for_timeout(30000)  # ç­‰å¾…30ç§’
                
                # æ£€æŸ¥éªŒè¯ç æ˜¯å¦å·²è§£å†³
                if await page.query_selector(selector) is None:
                    self.logger.info("éªŒè¯ç å·²è§£å†³")
                    return True  # éªŒè¯ç å·²è§£å†³
                
                # å¦‚æœéªŒè¯ç ä»ç„¶å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´æˆ–å…¶ä»–å¤„ç†æ–¹å¼
                # è¿™é‡Œç®€å•åœ°å†ç­‰å¾…30ç§’
                self.logger.warning("éªŒè¯ç ä»æœªè§£å†³ï¼Œå†ç­‰å¾…30ç§’")
                await page.wait_for_timeout(30000)  # å†ç­‰å¾…30ç§’
                return True  # æ— è®ºéªŒè¯ç æ˜¯å¦è§£å†³ï¼Œæˆ‘ä»¬éƒ½è¿”å›Trueè¡¨ç¤ºå·²å°è¯•å¤„ç†
        
        self.logger.debug("æœªæ£€æµ‹åˆ°éªŒè¯ç ")
        return False  # æ²¡æœ‰æ£€æµ‹åˆ°éªŒè¯ç 
    
    async def _extract_search_results(self, page: Page) -> List[SearchResult]:
        """ä»ç™¾åº¦æœç´¢é¡µé¢æå–æœç´¢ç»“æœ"""
        self.logger.debug("å¼€å§‹ä»é¡µé¢æå–æœç´¢ç»“æœ")
        results = []
        
        # ä½¿ç”¨JavaScriptæå–æœç´¢ç»“æœ
        self.logger.debug("æ‰§è¡Œé¡µé¢JavaScriptä»¥æå–æœç´¢ç»“æœ")
        try:
            raw_results = await page.evaluate("""
                () => {
                    const resultElements = document.querySelectorAll('.result');
                    return Array.from(resultElements).map(el => {
                        // æå–æ ‡é¢˜å’ŒURL
                        const titleEl = el.querySelector('.t a, .c-title a');
                        const title = titleEl ? titleEl.textContent.trim() : '';
                        const url = titleEl ? titleEl.href : '';
                        
                        // æå–æ‘˜è¦
                        const snippetEl = el.querySelector('.c-abstract, .content-abstract');
                        const snippet = snippetEl ? snippetEl.textContent.trim() : '';
                        
                        // æå–æ¥æºå’Œæ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
                        const sourceEl = el.querySelector('.c-author, .c-color-gray');
                        const source = sourceEl ? sourceEl.textContent.trim() : null;
                        
                        // æå–æ—¶é—´ï¼ˆé€šå¸¸åŒ…å«åœ¨æ¥æºä¿¡æ¯ä¸­ï¼‰
                        let time = null;
                        if (source) {
                            const timeMatch = source.match(/\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|\d{1,2}å¤©å‰|\d{1,2}å°æ—¶å‰/);
                            time = timeMatch ? timeMatch[0] : null;
                        }
                        
                        return { title, url, snippet, source, time };
                    });
                }
            """)
            self.logger.debug(f"JavaScriptæ‰§è¡ŒæˆåŠŸï¼Œè·å–åˆ° {len(raw_results)} ä¸ªåŸå§‹ç»“æœ")
        except Exception as e:
            self.logger.error(f"JavaScriptæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []
        
        # å°†åŸå§‹ç»“æœè½¬æ¢ä¸ºSearchResultå¯¹è±¡
        self.logger.debug("å¼€å§‹è½¬æ¢åŸå§‹ç»“æœä¸ºSearchResultå¯¹è±¡")
        for i, item in enumerate(raw_results):
            if item['title'] and item['url']:
                try:
                    result = SearchResult(
                        title=item['title'],
                        url=item['url'],
                        snippet=item['snippet'] if item['snippet'] else "æ— æ‘˜è¦",
                        source=item['source'],
                        time=item['time']
                    )
                    results.append(result)
                    self.logger.debug(f"=========== æŸ¥è¯¢ç»“æœç¬¬{i+1}æ¡ ===========")
                    self.logger.debug(f"æ ‡é¢˜={item['title']}")
                    self.logger.debug(f"åœ°å€={item['url']}")
                except Exception as e:
                    self.logger.warning(f"è½¬æ¢ç»“æœ #{i+1} å¤±è´¥: {e}")
        
        self.logger.info(f"æå–åˆ° {len(results)} æ¡æœç´¢ç»“æœ")
        return results

        



    def register_resources(self):
        self.logger.info("æ³¨å†Œèµ„æºå¤„ç†å™¨")
        @self.mcp.resource("console://logs")
        async def get_console_logs() -> str:
            """Get a personalized greeting"""
            self.logger.debug("è·å–æ§åˆ¶å°æ—¥å¿—")
            return TextContent(
                type="text", text="\n".join(self.browser_manager.console_logs)
            )

        @self.mcp.resource("screenshot://{name}")
        async def get_screenshot(name: str) -> str:
            """Get a screenshot by name"""
            self.logger.debug(f"è·å–æˆªå›¾: {name}")
            screenshot_base64 = self.screenshots.get(name)
            if screenshot_base64:
                self.logger.debug(f"æ‰¾åˆ°æˆªå›¾: {name}")
                return ImageContent(
                    type="image",
                    data=screenshot_base64,
                    mimeType="image/png",
                    uri=f"screenshot://{name}",
                )
            else:
                self.logger.warning(f"æˆªå›¾æœªæ‰¾åˆ°: {name}")
                raise ValueError(f"Screenshot {name} not found")

    def register_prompts(self):
        self.logger.info("æ³¨å†Œæç¤ºå¤„ç†å™¨")
        @self.mcp.prompt()
        async def hello_world(code: str) -> str:
            self.logger.debug(f"å¤„ç†hello_worldæç¤ºï¼Œä»£ç é•¿åº¦: {len(code)}å­—ç¬¦")
            return f"Hello world:\n\n{code}"


""" 
When executing the MCP Inspector in a terminal, use the following command:

```bash
cmd> fastmcp dev ./server/browser_navigator_server.py:app
```

app = BrowserNavigationServer()

- `server/browser_navigator_server.py` specifies the file path.
- `app` refers to the server object created by `BrowserNavigationServer`.

After running the command, the following message will be displayed:

```
> Starting MCP Inspector...
> ğŸ” MCP Inspector is up and running at http://localhost:5173 ğŸš€
```

**Important:** Do not use `__main__` to launch the MCP Inspector. This will result in the following error:

    No server object found in **.py. Please either:
    1. Use a standard variable name (mcp, server, or app)
    2. Specify the object name with file:object syntax
"""

app = BrowserNavigationServer()
def main():
    app.run()

print("BrowserNavigationServer is running...")
# print all attributes of the mcp
# print(dir(app))


# if __name__ == "__main__":
#     app = BrowserNavigationServer()
#     app.run()
#     print("BrowserNavigationServer is running...")

