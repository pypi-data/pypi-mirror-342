{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# Workflow Interface 405: Federated Evaluation\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/openfl/blob/develop/openfl-tutorials/experimental/workflow/405_MNIST_FederatedEvaluation.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd059520",
   "metadata": {},
   "source": [
    "Welcome to the first OpenFL Federated Evaluation Workflow Interface tutorial! This notebook demonstrates OpenFL capability of running your first horizontal federated evaluation workflow. This work has the following goals:\n",
    "\n",
    "- Template for federated evaluation exposing key metrics post evaluation run (e.g model accuracy)\n",
    "- Build on top of first example of learning via workflow API (refer [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) ) using MNIST dataset and perform fedeval (federated evaluation) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d98490",
   "metadata": {},
   "source": [
    "First we start by installing the necessary dependencies for the workflow interface as per [installation guide](https://openfl.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this if running in Google Colab and set USERNAME if running in docker container.\n",
    "#!pip install -r https://raw.githubusercontent.com/intel/openfl/develop/openfl-tutorials/experimental/workflow/workflow_interface_requirements.txt\n",
    "#import os\n",
    "#os.environ[\"USERNAME\"] = \"colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac0fac",
   "metadata": {},
   "source": [
    "One foundational pre-requisite for evaluation is to have a pre-trained model available and that's exactly what this notebook expects as a pre-requisite:\n",
    "- A pre-trained model that can be loaded for evaluation\n",
    "\n",
    "For this tutorial, let's use the final output model of [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) run, a sample of same is saved at [Pre-trained model](../pretrainedmodels/cnn_mnist.pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682720b",
   "metadata": {},
   "source": [
    "Sample of the output of training run model that was saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd108b5a",
   "metadata": {
    "vscode": {
     "languageId": "code-text-binary"
    }
   },
   "outputs": [],
   "source": [
    "Sample of the final model weights: tensor([[[ 0.1221, -0.0846, -0.0635,  0.0590, -0.2059],\n",
    "         [ 0.1558, -0.0202,  0.1005,  0.0272, -0.0148],\n",
    "         [ 0.1034,  0.0560,  0.1089, -0.0367,  0.0182],\n",
    "         [ 0.0086,  0.0602,  0.0315,  0.2058,  0.0909],\n",
    "         [-0.0778, -0.1234, -0.0414, -0.0904, -0.0548]]])\n",
    "\n",
    "Final aggregated model accuracy for 2 rounds of training: 0.8463999927043915"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7237eac4",
   "metadata": {},
   "source": [
    "Let's first define our dataloaders, model, optimizer, and some helper functions like we would for any other deep learning experiment, however \n",
    "notice the difference in this setup compared to a typical training/learning setup as detailed in [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) :\n",
    "- There is no need to download the training set as we will do only evaluation\n",
    "- No optimizer settings needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"./files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "def inference(network,test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, `LocalRuntime`, and placement decorators.\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `Runtime` – Defines where the flow runs, infrastructure for task transitions (how information gets sent). The `LocalRuntime` runs the flow on a single node.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e406db6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we come to the flow definition. The OpenFL Workflow Interface adopts the conventions set by Metaflow, that every workflow begins with `start` and concludes with the `end` task. The aggregator begins with an optionally passed in model and optimizer. The aggregator begins the flow with the `start` task, where the list of collaborators is extracted from the runtime (`self.collaborators = self.runtime.collaborators`) and is then used as the list of participants to run the task listed in `self.next`, `evaluate`. The model, optimizer, and anything that is not explicitly excluded from the next function will be passed from the `start` function on the aggregator to the `evaluate` task on the collaborator. Where the tasks run is determined by the placement decorator that precedes each task definition (`@aggregator` or `@collaborator`). Once each of the collaborators (defined in the runtime) complete the `evaluate` task, they finally `join` at the aggregator doing just model evaluation/validation without any training. It is in `join` that an accuracy of model per collaborator is shown.\n",
    "\n",
    "![FedEval.png](../../../docs/images/FedEval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "class FederatedEvaluationFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, rounds=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            \n",
    "        self.rounds = rounds\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(self.evaluate, foreach='collaborators', exclude=['private'])\n",
    "\n",
    "    @collaborator\n",
    "    def evaluate(self):\n",
    "        print(f'Performing model evaluation for collaborator {self.input}')\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        print(f'Average aggregated model accuracy values = {self.aggregated_model_accuracy}')\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(self.evaluate,\n",
    "                      foreach='collaborators', exclude=['private'])\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'This is the end of the flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ab527",
   "metadata": {},
   "source": [
    "Now let's setup the participants in similar fashion as basic learning/training tutorial but notice the difference in the setup below since we are doing only evaluation there is no need to configure training related data, targets and data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_test, shuffle=True)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "278ad46b",
   "metadata": {},
   "source": [
    "Now that we have our evaluation flow and runtime defined, let's run the experiment! Since its evaluation we need to run it only for one round of validation and for that first we will load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/securefederatedai/openfl/raw/refs/heads/develop/openfl-tutorials/experimental/workflow/pretrainedmodels/cnn_mnist.pth cnn_mnist.pth\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('cnn_mnist.pth'))\n",
    "best_model = model\n",
    "flflow = FederatedEvaluationFlow(model, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cc8f7",
   "metadata": {},
   "source": [
    "Now that the flow has completed, let's get the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863761fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nFinal model accuracy for {flflow.rounds} rounds of evaluation: {flflow.aggregated_model_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec521c90",
   "metadata": {},
   "source": [
    "It should ideally report +-0.05 as per the pre-trained models' accuracy that is used in this experiment which, as detailed above, was ~0.846"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a61a876d",
   "metadata": {},
   "source": [
    "Now that the flow is complete, let's dig into some of the information captured along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = flflow._run_id\n",
    "from metaflow import Metaflow, Flow, Task, Step\n",
    "m = Metaflow()\n",
    "s = Step(f'FederatedEvaluationFlow/{run_id}/evaluate')\n",
    "list(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb1866b7",
   "metadata": {},
   "source": [
    "Now we see **4** steps: **4** collaborators each performed **1** rounds of model evaluation\n",
    "Let's look at one of those data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Task(f'FederatedEvaluationFlow/{run_id}/evaluate/2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef877a50",
   "metadata": {},
   "source": [
    "Now let's look at the data artifacts this task generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data.input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9826c45f",
   "metadata": {},
   "source": [
    "Now let's look at its log output (stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235d3d8",
   "metadata": {},
   "source": [
    "For more details on checkpointing and using Metaflow to dig into more details of a federation run please refer to previous tutorials on learning like [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "426f2395",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You've successfully completed your first Federated Evaluation workflow interface quickstart notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
