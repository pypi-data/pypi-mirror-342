{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPipe client initialized\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from openpipe.client import OpenPipe\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "op_client = OpenPipe()\n",
    "print(\"OpenPipe client initialized\")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "import openai\n",
    "import time\n",
    "from litellm import acompletion\n",
    "\n",
    "from .utils import generate_game, get_opponent_move, render_board, check_winner, apply_agent_move, get_trajectory_messages, AgentMove\n",
    "\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError))\n",
    "async def rollout(\n",
    "    model: str, iteration: int, is_validation: bool\n",
    ") -> art.Trajectory:\n",
    "\n",
    "    game = generate_game()\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return the move in the format 'A1', 'B2', 'C3', etc. You are the {game['agent_symbol']} symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        reward=0,\n",
    "        metrics={\"test\": 5},\n",
    "    )\n",
    "\n",
    "    if game[\"agent_symbol\"] == \"o\":\n",
    "        starting_opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
    "            \"opponent_symbol\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    while check_winner(game[\"board\"]) is None:\n",
    "\n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = get_trajectory_messages(trajectory)\n",
    "\n",
    "        async def get_completion():\n",
    "            return await acompletion(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                response_format=AgentMove\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            op_client.report(\n",
    "                requested_at=requested_at,\n",
    "                received_at=int(time.time() * 1000),\n",
    "                req_payload={\n",
    "                    \"model\": model,\n",
    "                    \"messages\": messages,\n",
    "                    \"metadata\": {\n",
    "                        \"notebook-id\": \"tic-tac-toe\",\n",
    "                        \"iteration\": str(iteration),\n",
    "                        \"validation\": str(is_validation),\n",
    "                        \"move_number\": str(len(trajectory.messages_and_choices) - 1),\n",
    "                    },\n",
    "                },\n",
    "                resp_payload=chat_completion,\n",
    "                status_code=200,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "        except ValueError:\n",
    "            trajectory.reward = -1\n",
    "            break\n",
    "\n",
    "        if check_winner(game[\"board\"]) is not None:\n",
    "            break\n",
    "\n",
    "        opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
    "\n",
    "    winner = check_winner(game[\"board\"])\n",
    "\n",
    "    if winner == game[\"agent_symbol\"]:\n",
    "        trajectory.reward = 1\n",
    "    elif winner == game[\"opponent_symbol\"]:\n",
    "        trajectory.reward = 0\n",
    "    elif winner == \"draw\":\n",
    "        trajectory.reward = 0.5\n",
    "\n",
    "    try:\n",
    "        op_client.update_log_metadata(\n",
    "            filters=[\n",
    "                {\n",
    "                    \"field\": \"completionId\",\n",
    "                    \"equals\": last_completion.id,\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"reward\": str(trajectory.reward),\n",
    "                \"reward_assigned\": \"true\",\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d4077a05fe4af8a3c1d9a6a1524762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for gpt-4o-mini: 0.285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from art.utils.benchmark_rollout import benchmark_rollout\n",
    "\n",
    "await benchmark_rollout(\n",
    "    \"gpt-4o-mini\",\n",
    "    100,\n",
    "    rollout,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed10ab9b72404b45be73050f7ebd4cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for gpt-4o-2024-11-20: 0.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await benchmark_rollout(\n",
    "    \"gpt-4o-2024-11-20\",\n",
    "    100,\n",
    "    rollout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f74be8707b846be819f9b9d517a2c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for o3-mini-2025-01-31: 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await benchmark_rollout(\n",
    "    \"o3-mini-2025-01-31\",\n",
    "    100,\n",
    "    rollout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be81c36dc20e46cbacec9ce36c608982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for gpt-4.5-preview-2025-02-27: 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await benchmark_rollout(\n",
    "    \"gpt-4.5-preview-2025-02-27\",\n",
    "    10,\n",
    "    rollout,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
