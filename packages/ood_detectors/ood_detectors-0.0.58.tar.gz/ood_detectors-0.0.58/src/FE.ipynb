{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_sliding_window_variance(ax, x, y, xlabel, ylabel, title, window_size=5):\n",
    "    # Ensure x, y are numpy arrays for easier manipulation\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Sort x and y by x to ensure correct plotting\n",
    "    sort_idx = np.argsort(x)\n",
    "    x, y = x[sort_idx], y[sort_idx]\n",
    "    if len(x) < 5:\n",
    "        ax.legend(['models'])\n",
    "        return\n",
    "    # Linear fit\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    fit_line = np.polyval([slope, intercept], x)\n",
    "    ax.plot(x, fit_line, color='red')\n",
    "    if len(x) > 5:\n",
    "        # Calculate variance with a sliding window\n",
    "        variance = np.array([np.var(y[max(0, i-window_size//2):min(len(y), i+window_size//2)]) for i in range(len(y))])\n",
    "        sigma = np.sqrt(variance)\n",
    "        \n",
    "        # Smooth the variance to avoid overly noisy bands\n",
    "        sigma_smoothed = savgol_filter(sigma, window_length=min(100, len(x) - 1 if len(x) % 2 == 0 else len(x)), polyorder=3)\n",
    "        \n",
    "        # Plot sliding window variance (confidence interval) around the line\n",
    "        ax.fill_between(x, fit_line - sigma_smoothed, fit_line + sigma_smoothed, color='red', alpha=0.2)\n",
    "\n",
    "        ax.legend(['models', 'trend', 'variance'])\n",
    "    else:\n",
    "        ax.legend(['models', 'trend'])\n",
    "\n",
    "def bar_encoder(ax, encoders, encoder_positions, *args, skip_labels=False):\n",
    "    n_bars = len(args)\n",
    "    n_groups = len(encoders)\n",
    "    width = 0.8 / n_bars  # Calculate the width for each bar\n",
    "\n",
    "    # Iterate through each metric provided\n",
    "    for i, (metric, label) in enumerate(args):\n",
    "        averages = []  # Store average metric values\n",
    "        error = []  # Store min and max relative to the mean for error bars\n",
    "        max_vals = []\n",
    "\n",
    "        # Calculate statistics for each encoder\n",
    "        for encoder in encoders:\n",
    "            mask = encoder_positions == encoder\n",
    "            filtered_metric = metric[mask]\n",
    "            mean_val = np.mean(filtered_metric)\n",
    "            averages.append(mean_val)\n",
    "            # Calculate errors as distances from the mean to the min and max values\n",
    "            min_val = np.min(filtered_metric)\n",
    "            max_val = np.max(filtered_metric)\n",
    "            error.append([mean_val - min_val, max_val - mean_val])\n",
    "            max_vals.append(max_val)\n",
    "        \n",
    "        # Calculate positions for the current set of bars\n",
    "        positions = np.arange(len(encoders)) + i * width\n",
    "        \n",
    "        # Plot the bars with modified error bars representing the min to max range\n",
    "        bars = ax.bar(positions, averages, width, label=label, alpha=0.7)\n",
    "        # Add error bars for min and max values\n",
    "        error = np.array(error).T  # Transpose to match the expected shape for error bars\n",
    "        ax.errorbar(positions, averages, yerr=error, fmt='none', ecolor='black', capsize=5, alpha=0.7, label='min to max')\n",
    "        # Annotate each bar with min and max values\n",
    "        for bar, avg, maximum in zip(bars, averages, max_vals):\n",
    "            if avg == maximum:\n",
    "                ax.annotate(f'{maximum:.2f}',\n",
    "                            xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom', fontsize=8)\n",
    "            else:\n",
    "                ax.annotate(f'Max: {maximum:.2f}\\nAvg: {avg:.2f}',\n",
    "                            xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    ax.set_xticks(np.arange(n_groups) + width * (n_bars - 1) / 2)\n",
    "    # ax.set_xticklabels([f'{encoder}\\nsamples: {np.sum([encoder_positions == encoder])}' for encoder in encoders], rotation=15)\n",
    "    if not skip_labels:\n",
    "        ax.set_xticklabels([e.replace(\"_\", \"_\\n\").replace(\".\", \".\\n\") for e in encoders])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subVPSDE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_name = \"sub\"\n",
    "db = \"sqlite:///optuna_v3.db\"\n",
    "storage = optuna.storages.RDBStorage(url=db)\n",
    "study_names = list(map(lambda x: x.study_name, storage.get_all_studies()))\n",
    "study_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 50 trials\n",
      "50 50\n"
     ]
    }
   ],
   "source": [
    "trials = []\n",
    "for sn  in study_names:\n",
    "    if sn.startswith(study_name):\n",
    "        study = optuna.load_study(study_name=sn, storage=storage)\n",
    "        trials += study.trials\n",
    "\n",
    "print(f\"loaded {len(trials)} trials\")\n",
    "\n",
    "\n",
    "# values = list([t.user_attrs | t.params |{\"auc\": (t.values[1] + t.values[2])/2} for t in trials if t.values is not None])\n",
    "values = list([t.user_attrs for t in trials])# if t.values is not None])\n",
    "\n",
    "print(len(values), len(trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder swin has 35 values\n",
      "encoder deit has 39 values\n",
      "encoder repvgg has 37 values\n",
      "encoder resnet50d has 36 values\n",
      "encoder bit has 37 values\n",
      "swin      >  37: (auc=90.85 fpr=45.12)   33: (auc=90.79 fpr=44.83)    3: (auc=90.71 fpr=46.31)   24: (auc=90.55 fpr=46.19)    4: (auc=90.43 fpr=46.65)  \n",
      "deit      >  36: (auc=82.88 fpr=80.52)   24: (auc=82.47 fpr=79.93)    3: (auc=82.35 fpr=80.30)   26: (auc=82.16 fpr=80.60)    4: (auc=82.13 fpr=78.50)  \n",
      "repvgg    >  36: (auc=83.40 fpr=64.92)    3: (auc=82.78 fpr=66.05)   10: (auc=82.73 fpr=66.51)    8: (auc=82.58 fpr=65.74)   30: (auc=82.50 fpr=65.79)  \n",
      "resnet50d >  24: (auc=85.87 fpr=61.54)   10: (auc=85.87 fpr=61.34)    3: (auc=85.59 fpr=62.12)   26: (auc=85.31 fpr=61.63)    4: (auc=84.93 fpr=63.46)  \n",
      "bit       >  24: (auc=95.30 fpr=22.05)   36: (auc=95.23 fpr=22.11)   10: (auc=95.03 fpr=22.74)   30: (auc=94.97 fpr=22.82)    3: (auc=94.96 fpr=22.88)  \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "scores = defaultdict(list)\n",
    "for encoder in ['swin', 'deit', 'repvgg', 'resnet50d', 'bit']:\n",
    "\n",
    "    dataset = 'imagenet'\n",
    "\n",
    "    intrests = [\n",
    "                f'{encoder}_{dataset}_nearood_imagenet-o_AUC',\n",
    "                f'{encoder}_{dataset}_nearood_imagenet-o_FPR_95',\n",
    "\n",
    "                f'{encoder}_{dataset}_farood_inaturalist_AUC',\n",
    "                f'{encoder}_{dataset}_farood_inaturalist_FPR_95',\n",
    "\n",
    "                f'{encoder}_{dataset}_farood_openimageo_AUC',\n",
    "                f'{encoder}_{dataset}_farood_openimageo_FPR_95',\n",
    "\n",
    "                f'{encoder}_{dataset}_farood_textures_AUC',\n",
    "                f'{encoder}_{dataset}_farood_textures_FPR_95',\n",
    "                ]\n",
    "    \n",
    "    full_values_auc = []\n",
    "    full_values_fpr = []\n",
    "    for i, v in enumerate(values):\n",
    "        missing = False\n",
    "        for k in intrests:\n",
    "            if k not in v:\n",
    "                # print(f\"missing {k}\")\n",
    "                missing = True\n",
    "        if not missing:\n",
    "            full_values_auc.append((i, [v[k] for k in intrests if k.endswith(\"AUC\")]))\n",
    "            full_values_fpr.append((i, [v[k] for k in intrests if k.endswith(\"FPR_95\")]))\n",
    "\n",
    "    print(f\"encoder {encoder} has {len(full_values_auc)} values\")\n",
    "    if len(full_values_auc) == 0:\n",
    "        continue\n",
    "\n",
    "    for (i, fpr), (j, auc) in zip(full_values_fpr, full_values_auc):\n",
    "        if i != j:\n",
    "            print(\"ERROR\", i, j)\n",
    "            continue\n",
    "        fpr = np.array(fpr).mean()\n",
    "        auc = np.array(auc).mean()\n",
    "        scores[encoder].append((i, fpr, auc))\n",
    "\n",
    "for k, v in scores.items():\n",
    "    sorted_v = sorted(v, key=lambda x: x[2], reverse=True)\n",
    "    print(f\"{k:10}>\", end=\" \")\n",
    "    for i, fpr, auc in sorted_v[:5]:\n",
    "        fpr *= 100\n",
    "        auc *= 100\n",
    "        print(f\" {i:2}: ({auc=:.2f} {fpr=:.2f}) \", end=\" \")\n",
    "    print()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit: 91.2%\n",
      "    n_epochs: 366.667, [500, 300, 300]\n",
      "    bottleneck_channels: 853.333, [256, 1792, 512]\n",
      "    num_res_blocks: 6.000, [9, 6, 3]\n",
      "    time_embed_dim: 341.333, [512, 256, 256]\n",
      "    dropout: 0.222, [0.20507666213092263, 0.24317537435908987, 0.21856901360140613]\n",
      "    lr: 0.004, [1.2184911425255087e-06, 0.003413265727580557, 0.009921410750025034]\n",
      "    likelihood_weighting: 1.000, [True, True, True]\n",
      "    reduce_mean: 0.333, [False, True, False]\n",
      "    beta_min: 0.453, [0.7707111985158176, 0.04509669122298443, 0.5434724083688577]\n",
      "    beta_max: 15.000, [15, 20, 10]\n",
      "dinov2: 86.6%\n",
      "    n_epochs: 200.000, [100, 300, 200]\n",
      "    bottleneck_channels: 1109.333, [512, 1792, 1024]\n",
      "    num_res_blocks: 7.667, [3, 11, 9]\n",
      "    time_embed_dim: 512.000, [256, 1024, 256]\n",
      "    dropout: 0.289, [0.19047268055467803, 0.323222360975637, 0.35343530173117726]\n",
      "    lr: 0.003, [0.00827849803876892, 6.139565305858912e-05, 0.00036565887340390897]\n",
      "    likelihood_weighting: 0.667, [True, False, True]\n",
      "    reduce_mean: 0.333, [True, False, False]\n",
      "    beta_min: 0.402, [0.22930562999592197, 0.5198903348650605, 0.4580014481720224]\n",
      "    beta_max: 18.333, [25, 15, 15]\n",
      "swin: 85.4%\n",
      "    n_epochs: 333.333, [200, 300, 500]\n",
      "    bottleneck_channels: 768.000, [768, 1024, 512]\n",
      "    num_res_blocks: 10.000, [10, 9, 11]\n",
      "    time_embed_dim: 341.333, [512, 256, 256]\n",
      "    dropout: 0.261, [0.37047573368312936, 0.37245867801156296, 0.04078120832350568]\n",
      "    lr: 0.000, [2.57831430959927e-05, 6.139565305858912e-05, 1.6398830668205752e-06]\n",
      "    likelihood_weighting: 0.667, [False, True, True]\n",
      "    reduce_mean: 0.667, [True, False, True]\n",
      "    beta_min: 0.152, [0.04509669122298443, 0.14669878527715852, 0.2649559299031501]\n",
      "    beta_max: 23.333, [30, 15, 25]\n",
      "dino: 82.8%\n",
      "    n_epochs: 300.000, [100, 400, 400]\n",
      "    bottleneck_channels: 853.333, [512, 512, 1536]\n",
      "    num_res_blocks: 8.667, [3, 12, 11]\n",
      "    time_embed_dim: 512.000, [256, 512, 768]\n",
      "    dropout: 0.127, [0.19047268055467803, 0.13488343852112938, 0.05595264897012242]\n",
      "    lr: 0.003, [0.00827849803876892, 1.0975759525713911e-05, 6.139565305858912e-05]\n",
      "    likelihood_weighting: 0.667, [True, False, True]\n",
      "    reduce_mean: 0.667, [True, False, True]\n",
      "    beta_min: 0.385, [0.22930562999592197, 0.7707111985158176, 0.15430036108021294]\n",
      "    beta_max: 18.333, [25, 10, 20]\n",
      "resnet50d: 82.7%\n",
      "    n_epochs: 366.667, [300, 400, 400]\n",
      "    bottleneck_channels: 1621.333, [1792, 1024, 2048]\n",
      "    num_res_blocks: 6.333, [11, 4, 4]\n",
      "    time_embed_dim: 597.333, [768, 768, 256]\n",
      "    dropout: 0.263, [0.3175387811801977, 0.28118180340986615, 0.18904405481291364]\n",
      "    lr: 0.001, [0.0006770360941712382, 1.156489398176711e-06, 0.0014359207141287824]\n",
      "    likelihood_weighting: 0.667, [True, True, False]\n",
      "    reduce_mean: 0.667, [True, True, False]\n",
      "    beta_min: 0.467, [0.2229691576235453, 0.7214737982838372, 0.4580014481720224]\n",
      "    beta_max: 21.667, [30, 25, 10]\n",
      "repvgg: 78.6%\n",
      "    n_epochs: 366.667, [400, 400, 300]\n",
      "    bottleneck_channels: 1706.667, [2048, 1024, 2048]\n",
      "    num_res_blocks: 8.667, [13, 4, 9]\n",
      "    time_embed_dim: 853.333, [1024, 768, 768]\n",
      "    dropout: 0.255, [0.46234570962516164, 0.28118180340986615, 0.022529892303661203]\n",
      "    lr: 0.000, [2.331831485153692e-05, 1.156489398176711e-06, 3.963100219418096e-06]\n",
      "    likelihood_weighting: 0.667, [True, True, False]\n",
      "    reduce_mean: 0.333, [False, True, False]\n",
      "    beta_min: 0.630, [0.35764183511484704, 0.7214737982838372, 0.8110505546879924]\n",
      "    beta_max: 20.000, [15, 25, 20]\n",
      "deit: 75.9%\n",
      "    n_epochs: 100.000, [100, 100, 100]\n",
      "    bottleneck_channels: 768.000, [1536, 512, 256]\n",
      "    num_res_blocks: 9.000, [5, 14, 8]\n",
      "    time_embed_dim: 512.000, [256, 768, 512]\n",
      "    dropout: 0.337, [0.46539396790525817, 0.4605887367979303, 0.08612596375556619]\n",
      "    lr: 0.003, [0.007454094375692583, 0.0013015362544716662, 0.00017446957378507193]\n",
      "    likelihood_weighting: 0.333, [True, False, False]\n",
      "    reduce_mean: 0.333, [False, True, False]\n",
      "    beta_min: 0.646, [0.9119283081750147, 0.2623256831612669, 0.7623790978271663]\n",
      "    beta_max: 20.000, [15, 15, 30]\n"
     ]
    }
   ],
   "source": [
    "for encoder, (auc, conf) in sorted(encoder_best_auc.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{encoder}: {auc:.1%}\")\n",
    "    for k, v in conf.items():\n",
    "        print(f\"    {k}: {np.mean(v):.3f}, {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
